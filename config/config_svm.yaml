data:
  dataset_folder: data
  train_dataset: train.csv
  val_dataset: dev.csv
  test_dataset: test.csv
  use_id: False  # If True, use ID information; if False, ignore ID

svm:
  gamma: 0.01  # Optimized for rbf, poly, sigmoid kernels (lower = smoother)
  degree: 3    # Optimized for polynomial kernel 
  r: 0.5       # Optimized for poly, sigmoid kernels
  kernel_type: rbf  # RBF usually performs better than linear for NLI

tokenizer:
  padding: max_length
  max_length: 512
  truncation: True
  return_attention_mask: True

text_embedding:
  type: pretrained  # Options: pretrained, tf_idf, count_vec
  add_new_token: False
  text_encoder: MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7  # Better base model for NLI
  freeze: False
  use_lora: False   # Enable LoRA for better fine-tuning
  # LoRA configs (only used when use_lora: True)
  lora_target_modules: ["query_proj", "value_proj"]  # DeBERTa specific
  lora_r: 16       # Higher rank for better performance
  lora_alpha: 32   # 2x lora_r
  lora_dropout: 0.1
  d_features: 768  # DeBERTa-base hidden size
  d_model: 512     # Project to this dimension
  dropout: 0.1     # Lower dropout

model:
  type_model: svm
  intermediate_dims: 512
  dropout: 0.2

train:
  output_dir: checkpoint
  seed: 12345
  num_train_epochs: 50    # Reduced epochs for SVM
  patience: 8            # More patience for SVM convergence
  learning_rate: 1.0e-4  # Higher LR for SVM kernel learning
  weight_decay: 1.0e-4   # Small weight decay for regularization
  metric_for_best_model: accuracy
  per_device_train_batch_size: 8  # Smaller batch for stable gradient
  per_device_valid_batch_size: 16
  gradient_accumulation_steps: 2   # Effective batch = 8*2 = 16
  dataloader_drop_last: True      # Drop last incomplete batch

inference:
  test_dataset: data/test.csv  # Should match data.test_dataset path
  batch_size: 64