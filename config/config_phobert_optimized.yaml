data:
  dataset_folder: data
  train_dataset: train.csv
  val_dataset: dev.csv
  test_dataset: test.csv
  use_id: False

tokenizer:
  padding: max_length
  max_length: 256      # PhoBERT works well with shorter sequences
  truncation: True
  return_attention_mask: True

text_embedding:
  type: pretrained
  add_new_token: False
  text_encoder: vinai/phobert-base  # Vietnamese optimized
  freeze: False
  use_lora: True
  lora_target_modules: ["query", "key", "value"]
  lora_r: 32           # Higher rank cho better performance
  lora_alpha: 64
  lora_dropout: 0.05
  d_features: 768
  d_model: 768
  dropout: 0.05        # Lower dropout

model:
  type_model: pretrained_nli
  dropout: 0.05

train:
  output_dir: checkpoint
  seed: 42
  num_train_epochs: 25  # More epochs cho Vietnamese model
  patience: 8
  learning_rate: 3.0e-5 # Higher LR cho PhoBERT
  weight_decay: 0.01
  metric_for_best_model: f1
  per_device_train_batch_size: 32  # Larger batch cho shorter sequences
  per_device_valid_batch_size: 32
  gradient_accumulation_steps: 1   # No accumulation needed

inference:
  test_dataset: data/test.csv
  batch_size: 64
