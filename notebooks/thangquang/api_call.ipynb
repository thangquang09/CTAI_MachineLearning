{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdd932fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from bert_logistic import read_texts_from_dir\n",
    "import os\n",
    "def read_texts_from_dir(dir_path):\n",
    "    \"\"\"\n",
    "    Reads the texts from a given directory and saves them in the pd.DataFrame with columns ['id', 'file_1', 'file_2'].\n",
    "\n",
    "    Params:\n",
    "      dir_path (str): path to the directory with data\n",
    "    \"\"\"\n",
    "    # Count number of directories in the provided path\n",
    "    dir_count = sum(\n",
    "        os.path.isdir(os.path.join(root, d))\n",
    "        for root, dirs, _ in os.walk(dir_path)\n",
    "        for d in dirs\n",
    "    )\n",
    "    data = [0 for _ in range(dir_count)]\n",
    "    print(f\"Number of directories: {dir_count}\")\n",
    "\n",
    "    # For each directory, read both file_1.txt and file_2.txt and save results to the list\n",
    "    i = 0\n",
    "    for folder_name in sorted(os.listdir(dir_path)):\n",
    "        folder_path = os.path.join(dir_path, folder_name)\n",
    "        if os.path.isdir(folder_path):\n",
    "            try:\n",
    "                with open(\n",
    "                    os.path.join(folder_path, \"file_1.txt\"), \"r\", encoding=\"utf-8\"\n",
    "                ) as f1:\n",
    "                    text1 = f1.read().strip()\n",
    "                with open(\n",
    "                    os.path.join(folder_path, \"file_2.txt\"), \"r\", encoding=\"utf-8\"\n",
    "                ) as f2:\n",
    "                    text2 = f2.read().strip()\n",
    "                index = int(folder_name[-4:])\n",
    "                data[i] = (index, text1, text2)\n",
    "                i += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading directory {folder_name}: {e}\")\n",
    "\n",
    "    # Change list with results into pandas DataFrame\n",
    "    df = pd.DataFrame(data, columns=[\"id\", \"file_1\", \"file_2\"]).set_index(\"id\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95db2fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Number of directories: 95\n",
      "Number of directories: 1068\n"
     ]
    }
   ],
   "source": [
    "train_path = \"/home/thangquang09/CODE/CTAI_MachineLearning/data/fake-or-real-the-impostor-hunt/data/train\"\n",
    "test_path = \"/home/thangquang09/CODE/CTAI_MachineLearning/data/fake-or-real-the-impostor-hunt/data/test\"\n",
    "gt_path = \"/home/thangquang09/CODE/CTAI_MachineLearning/data/fake-or-real-the-impostor-hunt/data/train.csv\"\n",
    "print(\"Loading data...\")\n",
    "df_train = read_texts_from_dir(train_path)\n",
    "df_test = read_texts_from_dir(test_path)\n",
    "df_train_gt = pd.read_csv(gt_path)\n",
    "y_train = df_train_gt[\"real_text_id\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47424885",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"\"\"You are an expert AI analyst specializing in detecting textual artifacts and style inconsistencies. Your task is to analyze a pair of texts, text_0 and text_1. One of these texts is the \"REAL\" text, which is closely based on an original scientific article. The other is the \"FAKE\" text, which has been deliberately altered by an AI to be different from the original.\n",
    "\n",
    "Your goal is to identify which text is the REAL one.\n",
    "\n",
    "Here are the criteria to guide your analysis:\n",
    "\n",
    "Characteristics of a REAL Text:\n",
    "\n",
    "    Maintains a formal, objective, and academic tone throughout.\n",
    "\n",
    "    Uses consistent, topic-specific terminology.\n",
    "\n",
    "    The content is focused and coherent, presenting scientific information.\n",
    "\n",
    "Common Patterns in a FAKE Text:\n",
    "\n",
    "    Text Corruption: The text may contain nonsensical characters, random strings of text from multiple languages, emojis, or malformed code snippets, often appearing abruptly after a few coherent sentences.\n",
    "\n",
    "    Fantastical Content: The text introduces bizarre, fantastical, or absurd elements that are completely unrelated to the scientific topic (e.g., Santa Claus living on the moon, rainbow unicorns, interstellar wars).\n",
    "\n",
    "    Inappropriate Tone Shift: The text shifts from a scientific tone to an overly informal, conversational, or narrative style, resembling a blog post, a story, or marketing copy. It may use exclamation points excessively or ask rhetorical questions.\n",
    "\n",
    "    Plausible Falsification: The text might maintain a scientific tone but replace key entities (like the names of telescopes, projects, or locations) with fabricated but plausible-sounding names.\n",
    "\n",
    "Your Analysis Process:\n",
    "\n",
    "    Read both text_0 and text_1 carefully.\n",
    "\n",
    "    For each text, check for any of the FAKE text patterns listed above.\n",
    "\n",
    "    Compare the tone, style, and content of the two texts.\n",
    "\n",
    "    Based on your analysis, decide which text is REAL and which is FAKE.\n",
    "\n",
    "Input Texts:\n",
    "\n",
    "<text_0>\n",
    "{text_0}\n",
    "</text_0>\n",
    "\n",
    "<text_1>\n",
    "{text_1}\n",
    "</text_1>\n",
    "\n",
    "Output Format:\n",
    "Provide your response as a single JSON object. Do not write any text outside of the JSON block. The JSON object should have the following structure:\n",
    "{{\n",
    "\"label\": 0\n",
    "}}\n",
    "\n",
    "If you determine text_0 is REAL, the label should be 0. If text_1 is REAL, the label should be 1.\n",
    "\n",
    "Now, analyze the provided texts and return the JSON output.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9aca55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "class ResponseFormatter(BaseModel):\n",
    "    \"\"\"Always use this tool to structure your response to the user.\"\"\"\n",
    "    label: str = Field(description=\"If you determine text_0 is REAL, the label should be 0. If text_1 is REAL, the label should be 1.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da554f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-pro\",\n",
    "    temperature=0.0,\n",
    "    api_key=os.getenv(\"GEMINI_API_KEY\")\n",
    ")\n",
    "\n",
    "llm_structured_output = llm.with_structured_output(ResponseFormatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "582cfcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"o1\",\n",
    "    # temperature=0.0,\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "llm_structured_output = llm.with_structured_output(ResponseFormatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aba55485",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_1 = df_train.loc[0][\"file_1\"]\n",
    "text_2 = df_train.loc[0][\"file_2\"]\n",
    "\n",
    "test_prompt = PROMPT.format(\n",
    "    text_0=text_1,\n",
    "    text_1=text_2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "744cc896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResponseFormatter(label='0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm_structured_output.invoke(test_prompt)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "287dd9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run this code you need to install the following dependencies:\n",
    "# pip install google-genai\n",
    "\n",
    "import base64\n",
    "import json\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "# from google import genai\n",
    "# from google.genai import types\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "def generate(prompt):\n",
    "    response = llm_structured_output.invoke(prompt)\n",
    "    return int(response.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aaa723e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_label = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d494addb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:   0%|          | 0/1068 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  10%|█         | 108/1068 [09:11<1:40:06,  6.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for row 109, waiting 60 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  10%|█         | 109/1068 [09:16<1:36:25,  6.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for row 110, waiting 60 seconds...\n",
      "Rate limit hit for row 109, waiting 120 seconds...\n",
      "Rate limit hit for row 110, waiting 120 seconds...\n",
      "Rate limit hit for row 109, waiting 240 seconds...\n",
      "Rate limit hit for row 110, waiting 240 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  10%|█         | 110/1068 [16:19<33:45:13, 126.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to process row 109\n",
      "Rate limit hit for row 111, waiting 60 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  10%|█         | 111/1068 [16:24<24:14:34, 91.20s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to process row 110\n",
      "Rate limit hit for row 112, waiting 60 seconds...\n",
      "Rate limit hit for row 111, waiting 120 seconds...\n",
      "Rate limit hit for row 112, waiting 120 seconds...\n",
      "Rate limit hit for row 111, waiting 240 seconds...\n",
      "Rate limit hit for row 112, waiting 240 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  10%|█         | 112/1068 [23:27<50:10:05, 188.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to process row 111\n",
      "Rate limit hit for row 113, waiting 60 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  11%|█         | 113/1068 [23:31<35:36:14, 134.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to process row 112\n",
      "Rate limit hit for row 114, waiting 60 seconds...\n",
      "Rate limit hit for row 113, waiting 120 seconds...\n",
      "Rate limit hit for row 114, waiting 120 seconds...\n",
      "Rate limit hit for row 113, waiting 240 seconds...\n",
      "Rate limit hit for row 114, waiting 240 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  11%|█         | 114/1068 [30:35<58:21:47, 220.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to process row 113\n",
      "Rate limit hit for row 115, waiting 60 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  11%|█         | 115/1068 [30:39<41:13:48, 155.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to process row 114\n",
      "Rate limit hit for row 116, waiting 60 seconds...\n",
      "Rate limit hit for row 115, waiting 120 seconds...\n",
      "Rate limit hit for row 116, waiting 120 seconds...\n",
      "Rate limit hit for row 115, waiting 240 seconds...\n",
      "Rate limit hit for row 116, waiting 240 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  11%|█         | 116/1068 [37:43<62:24:05, 235.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to process row 115\n",
      "Rate limit hit for row 117, waiting 60 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  11%|█         | 117/1068 [37:46<43:55:00, 166.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to process row 116\n",
      "Rate limit hit for row 118, waiting 60 seconds...\n",
      "Rate limit hit for row 117, waiting 120 seconds...\n",
      "Rate limit hit for row 118, waiting 120 seconds...\n",
      "Rate limit hit for row 117, waiting 240 seconds...\n",
      "Rate limit hit for row 118, waiting 240 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  11%|█         | 118/1068 [44:50<64:17:12, 243.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to process row 117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  11%|█         | 119/1068 [44:53<45:10:27, 171.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to process row 118\n",
      "Rate limit hit for row 119, waiting 60 seconds...\n",
      "Rate limit hit for row 120, waiting 60 seconds...\n",
      "Rate limit hit for row 119, waiting 120 seconds...\n",
      "Rate limit hit for row 120, waiting 120 seconds...\n",
      "Rate limit hit for row 119, waiting 240 seconds...\n",
      "Rate limit hit for row 120, waiting 240 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  11%|█         | 120/1068 [51:58<65:11:24, 247.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to process row 119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  11%|█▏        | 121/1068 [52:00<45:43:58, 173.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to process row 120\n",
      "Rate limit hit for row 121, waiting 60 seconds...\n",
      "Rate limit hit for row 122, waiting 60 seconds...\n",
      "Rate limit hit for row 121, waiting 120 seconds...\n",
      "Rate limit hit for row 122, waiting 120 seconds...\n",
      "Rate limit hit for row 121, waiting 240 seconds...\n",
      "Rate limit hit for row 122, waiting 240 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  11%|█▏        | 122/1068 [59:07<65:35:51, 249.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to process row 121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  12%|█▏        | 123/1068 [59:08<46:00:12, 175.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to process row 122\n",
      "Rate limit hit for row 123, waiting 60 seconds...\n",
      "Rate limit hit for row 124, waiting 60 seconds...\n",
      "Rate limit hit for row 123, waiting 120 seconds...\n",
      "Rate limit hit for row 124, waiting 120 seconds...\n",
      "Rate limit hit for row 123, waiting 240 seconds...\n",
      "Rate limit hit for row 124, waiting 240 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  12%|█▏        | 124/1068 [1:06:14<65:41:38, 250.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to process row 123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  12%|█▏        | 125/1068 [1:06:16<46:02:00, 175.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to process row 124\n",
      "Rate limit hit for row 125, waiting 60 seconds...\n",
      "Rate limit hit for row 126, waiting 60 seconds...\n",
      "Rate limit hit for row 125, waiting 120 seconds...\n",
      "Rate limit hit for row 126, waiting 120 seconds...\n",
      "Rate limit hit for row 125, waiting 240 seconds...\n",
      "Rate limit hit for row 126, waiting 240 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  12%|█▏        | 126/1068 [1:13:22<65:41:35, 251.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to process row 125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  12%|█▏        | 127/1068 [1:13:24<46:02:14, 176.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to process row 126\n",
      "Rate limit hit for row 127, waiting 60 seconds...\n",
      "Rate limit hit for row 128, waiting 60 seconds...\n",
      "Rate limit hit for row 127, waiting 120 seconds...\n",
      "Rate limit hit for row 128, waiting 120 seconds...\n",
      "Rate limit hit for row 127, waiting 240 seconds...\n",
      "Rate limit hit for row 128, waiting 240 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  12%|█▏        | 128/1068 [1:20:30<65:36:31, 251.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to process row 127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  12%|█▏        | 129/1068 [1:20:31<45:57:27, 176.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to process row 128\n",
      "Rate limit hit for row 129, waiting 60 seconds...\n",
      "Rate limit hit for row 130, waiting 60 seconds...\n",
      "Rate limit hit for row 129, waiting 120 seconds...\n",
      "Rate limit hit for row 130, waiting 120 seconds...\n",
      "Rate limit hit for row 129, waiting 240 seconds...\n",
      "Rate limit hit for row 130, waiting 240 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  12%|█▏        | 129/1068 [1:24:59<10:18:42, 39.53s/it] \n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Reset predicted_label since the previous run failed\n",
    "predicted_label = []\n",
    "\n",
    "# Function to process a single row with retry logic\n",
    "def process_single_row(row_data):\n",
    "    file_1, file_2, row_id = row_data\n",
    "    max_retries = 3\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            prompt = PROMPT.format(text_0=file_1, text_1=file_2)\n",
    "            response = generate(prompt)\n",
    "            return row_id, response\n",
    "        except Exception as e:\n",
    "            if \"rate_limit\" in str(e).lower() or \"quota\" in str(e).lower():\n",
    "                wait_time = (2 ** attempt) * 60  # Exponential backoff: 1min, 2min, 4min\n",
    "                print(f\"Rate limit hit for row {row_id}, waiting {wait_time} seconds...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                print(f\"Error processing row {row_id}: {e}\")\n",
    "                break\n",
    "    \n",
    "    return row_id, None  # Return None if all retries failed\n",
    "\n",
    "# Prepare data for processing\n",
    "row_data_list = [(row.file_1, row.file_2, row.Index) for row in df_test.itertuples()]\n",
    "\n",
    "# Use ThreadPoolExecutor with limited workers to avoid overwhelming the API\n",
    "results = {}\n",
    "with ThreadPoolExecutor(max_workers=2) as executor:  # Reduced workers to avoid rate limits\n",
    "    # Submit all tasks\n",
    "    future_to_row = {executor.submit(process_single_row, row_data): row_data[2] \n",
    "                     for row_data in row_data_list}\n",
    "    \n",
    "    # Process completed tasks with progress bar\n",
    "    for future in tqdm(as_completed(future_to_row), total=len(future_to_row), desc=\"Processing rows\"):\n",
    "        try:\n",
    "            row_id, result = future.result()\n",
    "            if result is not None:\n",
    "                results[row_id] = result\n",
    "            else:\n",
    "                print(f\"Failed to process row {row_id}\")\n",
    "        except Exception as e:\n",
    "            row_id = future_to_row[future]\n",
    "            print(f\"Exception for row {row_id}: {e}\")\n",
    "\n",
    "# Convert results to list in correct order\n",
    "predicted_label = [results.get(row_id, 0) for row_id in df_test.index]\n",
    "\n",
    "print(f\"Successfully processed {len([r for r in predicted_label if r != 0])}/{len(df_test)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169ae4df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1068"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2697e676",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_label = np.array(predicted_label) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ae421a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting on test ...\n",
      "✅ Submission saved to /home/thangquang09/CODE/CTAI_MachineLearning/notebooks/submission_api_call_full_openai.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "print(\"Predicting on test ...\")\n",
    "test_pred = predicted_label\n",
    "\n",
    "# --- Build submission -------------------------------------------------\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": df_test.index,\n",
    "    \"real_text_id\": test_pred.astype(int)\n",
    "}).sort_values(\"id\")\n",
    "\n",
    "save_path = Path(\"submission_api_call_full_openai_o1.csv\")\n",
    "submission.to_csv(save_path, index=False)\n",
    "print(f\"✅ Submission saved to {save_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ac13c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ctai-machinelearning (3.10.18)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
