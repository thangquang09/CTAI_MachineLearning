{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdd932fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from bert_logistic import read_texts_from_dir\n",
    "import os\n",
    "def read_texts_from_dir(dir_path):\n",
    "    \"\"\"\n",
    "    Reads the texts from a given directory and saves them in the pd.DataFrame with columns ['id', 'file_1', 'file_2'].\n",
    "\n",
    "    Params:\n",
    "      dir_path (str): path to the directory with data\n",
    "    \"\"\"\n",
    "    # Count number of directories in the provided path\n",
    "    dir_count = sum(\n",
    "        os.path.isdir(os.path.join(root, d))\n",
    "        for root, dirs, _ in os.walk(dir_path)\n",
    "        for d in dirs\n",
    "    )\n",
    "    data = [0 for _ in range(dir_count)]\n",
    "    print(f\"Number of directories: {dir_count}\")\n",
    "\n",
    "    # For each directory, read both file_1.txt and file_2.txt and save results to the list\n",
    "    i = 0\n",
    "    for folder_name in sorted(os.listdir(dir_path)):\n",
    "        folder_path = os.path.join(dir_path, folder_name)\n",
    "        if os.path.isdir(folder_path):\n",
    "            try:\n",
    "                with open(\n",
    "                    os.path.join(folder_path, \"file_1.txt\"), \"r\", encoding=\"utf-8\"\n",
    "                ) as f1:\n",
    "                    text1 = f1.read().strip()\n",
    "                with open(\n",
    "                    os.path.join(folder_path, \"file_2.txt\"), \"r\", encoding=\"utf-8\"\n",
    "                ) as f2:\n",
    "                    text2 = f2.read().strip()\n",
    "                index = int(folder_name[-4:])\n",
    "                data[i] = (index, text1, text2)\n",
    "                i += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading directory {folder_name}: {e}\")\n",
    "\n",
    "    # Change list with results into pandas DataFrame\n",
    "    df = pd.DataFrame(data, columns=[\"id\", \"file_1\", \"file_2\"]).set_index(\"id\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95db2fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Number of directories: 95\n",
      "Number of directories: 1068\n"
     ]
    }
   ],
   "source": [
    "train_path = \"/home/thangquang09/CODE/CTAI_MachineLearning/data/fake-or-real-the-impostor-hunt/data/train\"\n",
    "test_path = \"/home/thangquang09/CODE/CTAI_MachineLearning/data/fake-or-real-the-impostor-hunt/data/test\"\n",
    "gt_path = \"/home/thangquang09/CODE/CTAI_MachineLearning/data/fake-or-real-the-impostor-hunt/data/train.csv\"\n",
    "print(\"Loading data...\")\n",
    "df_train = read_texts_from_dir(train_path)\n",
    "df_test = read_texts_from_dir(test_path)\n",
    "df_train_gt = pd.read_csv(gt_path)\n",
    "y_train = df_train_gt[\"real_text_id\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47424885",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"\"\"You are an expert AI analyst specializing in detecting textual artifacts and style inconsistencies. Your task is to analyze a pair of texts, text_0 and text_1. One of these texts is the \"REAL\" text, which is closely based on an original scientific article. The other is the \"FAKE\" text, which has been deliberately altered by an AI to be different from the original.\n",
    "\n",
    "Your goal is to identify which text is the REAL one.\n",
    "\n",
    "Here are the criteria to guide your analysis:\n",
    "\n",
    "Characteristics of a REAL Text:\n",
    "\n",
    "    Maintains a formal, objective, and academic tone throughout.\n",
    "\n",
    "    Uses consistent, topic-specific terminology.\n",
    "\n",
    "    The content is focused and coherent, presenting scientific information.\n",
    "\n",
    "Common Patterns in a FAKE Text:\n",
    "\n",
    "    Text Corruption: The text may contain nonsensical characters, random strings of text from multiple languages, emojis, or malformed code snippets, often appearing abruptly after a few coherent sentences.\n",
    "\n",
    "    Fantastical Content: The text introduces bizarre, fantastical, or absurd elements that are completely unrelated to the scientific topic (e.g., Santa Claus living on the moon, rainbow unicorns, interstellar wars).\n",
    "\n",
    "    Inappropriate Tone Shift: The text shifts from a scientific tone to an overly informal, conversational, or narrative style, resembling a blog post, a story, or marketing copy. It may use exclamation points excessively or ask rhetorical questions.\n",
    "\n",
    "    Plausible Falsification: The text might maintain a scientific tone but replace key entities (like the names of telescopes, projects, or locations) with fabricated but plausible-sounding names.\n",
    "\n",
    "Your Analysis Process:\n",
    "\n",
    "    Read both text_0 and text_1 carefully.\n",
    "\n",
    "    For each text, check for any of the FAKE text patterns listed above.\n",
    "\n",
    "    Compare the tone, style, and content of the two texts.\n",
    "\n",
    "    Based on your analysis, decide which text is REAL and which is FAKE.\n",
    "\n",
    "Input Texts:\n",
    "\n",
    "<text_0>\n",
    "{text_0}\n",
    "</text_0>\n",
    "\n",
    "<text_1>\n",
    "{text_1}\n",
    "</text_1>\n",
    "\n",
    "Output Format:\n",
    "Provide your response as a single JSON object. Do not write any text outside of the JSON block. The JSON object should have the following structure:\n",
    "{{\n",
    "\"label\": 0\n",
    "}}\n",
    "\n",
    "If you determine text_0 is REAL, the label should be 0. If text_1 is REAL, the label should be 1.\n",
    "\n",
    "Now, analyze the provided texts and return the JSON output.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9aca55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "class ResponseFormatter(BaseModel):\n",
    "    \"\"\"Always use this tool to structure your response to the user.\"\"\"\n",
    "    label: str = Field(description=\"If you determine text_0 is REAL, the label should be 0. If text_1 is REAL, the label should be 1.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da554f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-pro\",\n",
    "    temperature=0.0,\n",
    "    api_key=os.getenv(\"GEMINI_API_KEY\")\n",
    ")\n",
    "\n",
    "llm_structured_output = llm.with_structured_output(ResponseFormatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "582cfcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"o1\",\n",
    "    # temperature=0.0,\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "llm_structured_output = llm.with_structured_output(ResponseFormatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aba55485",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_1 = df_train.loc[0][\"file_1\"]\n",
    "text_2 = df_train.loc[0][\"file_2\"]\n",
    "\n",
    "test_prompt = PROMPT.format(\n",
    "    text_0=text_1,\n",
    "    text_1=text_2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "744cc896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResponseFormatter(label='0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm_structured_output.invoke(test_prompt)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "287dd9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run this code you need to install the following dependencies:\n",
    "# pip install google-genai\n",
    "\n",
    "import base64\n",
    "import json\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "# from google import genai\n",
    "# from google.genai import types\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "def generate(prompt):\n",
    "    response = llm_structured_output.invoke(prompt)\n",
    "    return int(response.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aaa723e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_label = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d494addb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:   0%|          | 0/1068 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:   2%|▏         | 25/1068 [02:16<1:58:17,  6.81s/it]"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Reset predicted_label since the previous run failed\n",
    "predicted_label = []\n",
    "\n",
    "# Function to process a single row with retry logic\n",
    "def process_single_row(row_data):\n",
    "    file_1, file_2, row_id = row_data\n",
    "    max_retries = 3\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            prompt = PROMPT.format(text_0=file_1, text_1=file_2)\n",
    "            response = generate(prompt)\n",
    "            return row_id, response\n",
    "        except Exception as e:\n",
    "            if \"rate_limit\" in str(e).lower() or \"quota\" in str(e).lower():\n",
    "                wait_time = (2 ** attempt) * 60  # Exponential backoff: 1min, 2min, 4min\n",
    "                print(f\"Rate limit hit for row {row_id}, waiting {wait_time} seconds...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                print(f\"Error processing row {row_id}: {e}\")\n",
    "                break\n",
    "    \n",
    "    return row_id, None  # Return None if all retries failed\n",
    "\n",
    "# Prepare data for processing\n",
    "row_data_list = [(row.file_1, row.file_2, row.Index) for row in df_test.itertuples()]\n",
    "\n",
    "# Use ThreadPoolExecutor with limited workers to avoid overwhelming the API\n",
    "results = {}\n",
    "with ThreadPoolExecutor(max_workers=2) as executor:  # Reduced workers to avoid rate limits\n",
    "    # Submit all tasks\n",
    "    future_to_row = {executor.submit(process_single_row, row_data): row_data[2] \n",
    "                     for row_data in row_data_list}\n",
    "    \n",
    "    # Process completed tasks with progress bar\n",
    "    for future in tqdm(as_completed(future_to_row), total=len(future_to_row), desc=\"Processing rows\"):\n",
    "        try:\n",
    "            row_id, result = future.result()\n",
    "            if result is not None:\n",
    "                results[row_id] = result\n",
    "            else:\n",
    "                print(f\"Failed to process row {row_id}\")\n",
    "        except Exception as e:\n",
    "            row_id = future_to_row[future]\n",
    "            print(f\"Exception for row {row_id}: {e}\")\n",
    "\n",
    "# Convert results to list in correct order\n",
    "predicted_label = [results.get(row_id, 0) for row_id in df_test.index]\n",
    "\n",
    "print(f\"Successfully processed {len([r for r in predicted_label if r != 0])}/{len(df_test)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169ae4df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1068"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2697e676",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_label = np.array(predicted_label) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ae421a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting on test ...\n",
      "✅ Submission saved to /home/thangquang09/CODE/CTAI_MachineLearning/notebooks/submission_api_call_full_openai.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "print(\"Predicting on test ...\")\n",
    "test_pred = predicted_label\n",
    "\n",
    "# --- Build submission -------------------------------------------------\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": df_test.index,\n",
    "    \"real_text_id\": test_pred.astype(int)\n",
    "}).sort_values(\"id\")\n",
    "\n",
    "save_path = Path(\"submission_api_call_full_openai_o1.csv\")\n",
    "submission.to_csv(save_path, index=False)\n",
    "print(f\"✅ Submission saved to {save_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ac13c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ctai-machinelearning (3.10.18)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
