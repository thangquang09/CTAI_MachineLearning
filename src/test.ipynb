{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00d7f781",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thangquang09/CODE/CTAI_MachineLearning/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/thangquang09/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/thangquang09/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kích thước từ vựng: 10000\n"
     ]
    }
   ],
   "source": [
    "from build_dataset_dataloader import get_dataset\n",
    "\n",
    "train_dataset, val_dataset, vocabulary = get_dataset(case=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ece56f3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    59\n",
       "2    41\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"thangquang09/fake-new-imposter-hunt-in-texts\") \n",
    "\n",
    "val = dataset['case2_validation'].to_pandas()\n",
    "val['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63f2ae92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from CONFIG import *\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ee1525a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextClassificationLSTM(\n",
       "  (embedding): Embedding(10000, 512, padding_idx=0)\n",
       "  (lstm): LSTM(512, 512, num_layers=3, batch_first=True, dropout=0.3, bidirectional=True)\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.3, inplace=False)\n",
       "    (6): Linear(in_features=256, out_features=1, bias=True)\n",
       "  )\n",
       "  (embedding_dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from LSTM import TextClassificationLSTM\n",
    "from CONFIG import *\n",
    "import torch\n",
    "\n",
    "model_path = \"/home/thangquang09/CODE/CTAI_MachineLearning/models/TextClassificationLSTM_case2_best_pair_acc_20250824_185657.pth\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TextClassificationLSTM(\n",
    "    vocab_size=len(vocabulary),\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    output_dim=OUTPUT_DIM,\n",
    ")\n",
    "\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a41db87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def predict_pair_confidence(model, seq1, seq2):\n",
    "    \"\"\"\n",
    "    Predict which text is REAL using confidence-based approach\n",
    "    Returns: prediction (0 or 1), confidence score, explanation\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Get predictions for both texts\n",
    "        logit1 = model(seq1)\n",
    "        logit2 = model(seq2)\n",
    "        \n",
    "        # Convert to probabilities\n",
    "        prob1 = torch.sigmoid(logit1)\n",
    "        prob2 = torch.sigmoid(logit2)\n",
    "        \n",
    "        # Determine prediction based on confidence\n",
    "        predictions = []\n",
    "        explanations = []\n",
    "        \n",
    "        for i in range(len(prob1)):\n",
    "            p1, p2 = prob1[i].item(), prob2[i].item()\n",
    "            \n",
    "            if p1 > 0.5 and p2 > 0.5:\n",
    "                # Both predicted as REAL -> choose higher confidence\n",
    "                if p1 > p2:\n",
    "                    pred = 0  # text1 is REAL (label=1 in original format)\n",
    "                    exp = f\"Both REAL, text1 more confident ({p1:.4f} vs {p2:.4f})\"\n",
    "                else:\n",
    "                    pred = 1  # text2 is REAL (label=2 in original format)\n",
    "                    exp = f\"Both REAL, text2 more confident ({p2:.4f} vs {p1:.4f})\"\n",
    "            elif p1 < 0.5 and p2 < 0.5:\n",
    "                # Both predicted as FAKE -> choose less fake (higher prob)\n",
    "                if p1 > p2:\n",
    "                    pred = 0  # text1 less fake\n",
    "                    exp = f\"Both FAKE, text1 less fake ({p1:.4f} vs {p2:.4f})\"\n",
    "                else:\n",
    "                    pred = 1  # text2 less fake\n",
    "                    exp = f\"Both FAKE, text2 less fake ({p2:.4f} vs {p1:.4f})\"\n",
    "            else:\n",
    "                # Normal case: one REAL, one FAKE\n",
    "                if p1 > 0.5:\n",
    "                    pred = 0  # text1 is REAL\n",
    "                    exp = f\"Text1 REAL ({p1:.4f}), Text2 FAKE ({p2:.4f})\"\n",
    "                else:\n",
    "                    pred = 1  # text2 is REAL\n",
    "                    exp = f\"Text2 REAL ({p2:.4f}), Text1 FAKE ({p1:.4f})\"\n",
    "            \n",
    "            predictions.append(pred)\n",
    "            explanations.append(exp)\n",
    "        \n",
    "        return torch.tensor(predictions, device=seq1.device), explanations\n",
    "\n",
    "\n",
    "def evaluate_individual(model, valid_dataloader, criterion):\n",
    "    \"\"\"Evaluate on individual text classification\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    running_correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for seq, labels in valid_dataloader:\n",
    "            seq, labels = seq.to(device), labels.to(device).float()\n",
    "            outputs = model(seq)\n",
    "            loss = criterion(outputs.squeeze(1), labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            predicted = (torch.sigmoid(outputs.squeeze(1)) > 0.5).float()\n",
    "            running_correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    accuracy = 100 * running_correct / total\n",
    "    total_loss = total_loss / len(valid_dataloader)\n",
    "    return total_loss, accuracy\n",
    "\n",
    "\n",
    "def evaluate_pairs(model, pair_dataloader):\n",
    "    \"\"\"Evaluate on original pair comparison task\"\"\"\n",
    "    model.eval()\n",
    "    running_correct = 0\n",
    "    total = 0\n",
    "    total_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for seq1, seq2, labels in pair_dataloader:\n",
    "            seq1, seq2, labels = seq1.to(device), seq2.to(device), labels.to(device)\n",
    "            \n",
    "            # Get pair predictions using confidence approach\n",
    "            predictions, _ = predict_pair_confidence(model, seq1, seq2)\n",
    "            total_predictions.extend(predictions.cpu().numpy())\n",
    "            # Convert original labels (0,1) to predictions format\n",
    "            # Original: 0 means text1 is REAL, 1 means text2 is REAL\n",
    "            running_correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    accuracy = 100 * running_correct / total\n",
    "    return accuracy, total_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7d18553",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, preds = evaluate_pairs(model, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10996081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UniqueCountsResult(values=array([0, 1]), counts=array([52, 48]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.unique_counts(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d20883e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TextComparisonDataset.__init__() missing 1 required positional argument: 'vocab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mbuild_dataset_dataloader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TextComparisonDataset\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mLSTM\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TextClassificationLSTM\n\u001b[0;32m----> 3\u001b[0m val_dataset, vocabulary \u001b[38;5;241m=\u001b[39m \u001b[43mTextComparisonDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mCONFIG\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m TextClassificationLSTM(\n\u001b[1;32m      7\u001b[0m     vocab_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(vocabulary),\n\u001b[1;32m      8\u001b[0m     embedding_dim\u001b[38;5;241m=\u001b[39mEMBEDDING_DIM,\n\u001b[1;32m      9\u001b[0m     hidden_dim\u001b[38;5;241m=\u001b[39mHIDDEN_DIM,\n\u001b[1;32m     10\u001b[0m     output_dim\u001b[38;5;241m=\u001b[39mOUTPUT_DIM,\n\u001b[1;32m     11\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: TextComparisonDataset.__init__() missing 1 required positional argument: 'vocab'"
     ]
    }
   ],
   "source": [
    "from build_dataset_dataloader import TextComparisonDataset\n",
    "from LSTM import TextClassificationLSTM\n",
    "val_dataset, vocabulary = TextComparisonDataset(val)\n",
    "from CONFIG import *\n",
    "\n",
    "model = TextClassificationLSTM(\n",
    "    vocab_size=len(vocabulary),\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    output_dim=OUTPUT_DIM,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c000d919",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/thangquang09/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/thangquang09/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kích thước từ vựng: 9125\n",
      "Index của '<pad>': 0\n",
      "Index của một từ ngẫu nhiên 'hello': 2\n",
      "Index của một từ không có trong từ điển: 2\n"
     ]
    }
   ],
   "source": [
    "from build_dataset_dataloader import get_dataset, text_to_sequence, TextComparisonDataset\n",
    "\n",
    "train_dataset, val_dataset, vocabulary = get_dataset(case=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12b1ea34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9125"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c0e2ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3264f2ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[2620,    8, 2396,  ...,    0,    0,    0],\n",
       "         [  26,  329,    4,  ...,    0,    0,    0],\n",
       "         [  10,   14,    7,  ...,    0,    0,    0],\n",
       "         ...,\n",
       "         [   7,  308,   16,  ...,    0,    0,    0],\n",
       "         [ 421,   81,   48,  ...,    0,    0,    0],\n",
       "         [   3,   96,  116,  ...,    0,    0,    0]]),\n",
       " tensor([[   8, 2396,    4,  ...,    0,    0,    0],\n",
       "         [  26,  329,    4,  ...,  571,  572,  573],\n",
       "         [   7, 1285, 2626,  ...,  630, 1669,  807],\n",
       "         ...,\n",
       "         [6510,  467,  147,  ...,    0,    0,    0],\n",
       "         [   7,  156, 1941,  ...,    0,    0,    0],\n",
       "         [   0,    0,    0,  ...,    0,    0,    0]]),\n",
       " tensor([0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.])]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d26b99c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocabulary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m NUM_EPOCHS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n\u001b[1;32m      9\u001b[0m LEARNING_RATE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m\n\u001b[0;32m---> 10\u001b[0m VOCAB_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mvocabulary\u001b[49m)\n\u001b[1;32m     11\u001b[0m SEQ_LENGTH \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m600\u001b[39m\n\u001b[1;32m     13\u001b[0m model \u001b[38;5;241m=\u001b[39m SiameseLSTM(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vocabulary' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from LSTM import SiameseLSTM\n",
    "\n",
    "EMBEDDING_DIM = 64\n",
    "HIDDEN_DIM = 128\n",
    "OUTPUT_DIM = 1 # Output 1 giá trị logit cho binary classification\n",
    "NUM_EPOCHS = 20\n",
    "LEARNING_RATE = 0.001\n",
    "VOCAB_SIZE = len(vocabulary)\n",
    "SEQ_LENGTH = 600\n",
    "\n",
    "model = SiameseLSTM(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acdfb406",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ac4afd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b671010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, valid_dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    running_correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for seq1, seq2, labels in valid_dataloader:\n",
    "            seq1, seq2, labels = seq1.to(device), seq2.to(device), labels.to(device).float()\n",
    "            outputs = model(seq1, seq2)\n",
    "            loss = criterion(outputs.squeeze(1), labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            predicted = (torch.sigmoid(outputs.squeeze(1)) > 0.5).float()\n",
    "            running_correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    accuracy = 100 * running_correct / total\n",
    "    total_loss = total_loss / total\n",
    "    return total_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "553da1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(model, max_epoch, train_dataloader, valid_dataloader, criterion, optimizer, device):\n",
    "    model.to(device)\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    test_losses = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    # callbacks (save best params)\n",
    "    best_weights = None\n",
    "    best_test_acc, best_test_loss = -1, float('inf')\n",
    "\n",
    "\n",
    "    for epoch in range(max_epoch):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_correct = 0   # to track number of correct predictions\n",
    "        total = 0   \n",
    "        \n",
    "        for i, (seq1, seq2, labels) in enumerate(train_dataloader):\n",
    "            epoch_start_time = time.time()\n",
    "            \n",
    "            seq1, seq2, labels = seq1.to(device), seq2.to(device), labels.to(device).float()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward\n",
    "            outputs = model(seq1, seq2)\n",
    "            loss = criterion(outputs.squeeze(1), labels)\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            predicted = (torch.sigmoid(outputs.squeeze(1)) > 0.5).float()\n",
    "            total += labels.size(0)\n",
    "            running_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Backward\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        epoch_accuracy = 100 * running_correct / total\n",
    "        epoch_loss = running_loss / total\n",
    "\n",
    "        test_loss, test_accuracy = evaluate(model, valid_dataloader, criterion)\n",
    "\n",
    "        if test_loss < best_test_loss:\n",
    "            best_test_loss = test_loss\n",
    "            # best_test_acc = test_accuracy\n",
    "            best_weights = model.state_dict()\n",
    "            \n",
    "        print(\n",
    "            \"| Epoch {:3d} | Time: {:5.2f}s | Train Accuracy {:8.3f}% | Train Loss {:8.3f} \"\n",
    "            \"| Valid Accuracy {:8.3f}% | Valid Loss {:8.3f} \".format(\n",
    "                epoch+1, time.time() - epoch_start_time, epoch_accuracy, epoch_loss, test_accuracy, test_loss\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # save for plot\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracies.append(epoch_accuracy)\n",
    "        test_losses.append(test_loss)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "        \n",
    "    values_dict = {\n",
    "        \"train_losses\": train_losses,\n",
    "        \"train_accuracies\": train_accuracies,\n",
    "        \"test_losses\": test_losses,\n",
    "        \"test_accuracies\": test_accuracies\n",
    "    }\n",
    "\n",
    "    return values_dict, best_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d99de564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch   1 | Time:  0.32s | Train Accuracy   58.068% | Train Loss    0.042 | Valid Accuracy   66.000% | Valid Loss    0.043 \n",
      "| Epoch   2 | Time:  0.31s | Train Accuracy   60.895% | Train Loss    0.040 | Valid Accuracy   67.000% | Valid Loss    0.042 \n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "\n",
    "history, best_weights = train(model, 2, train_dataloader, val_dataloader, criterion, optimizer, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de57284e",
   "metadata": {},
   "source": [
    "## Make Submisison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36e52a72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from LSTM import SiameseLSTM\n",
    "from CONFIG import  *\n",
    "\n",
    "# load model from /home/thangquang09/CODE/CTAI_MachineLearning/models/Siamese_LSTM_case2.pth\n",
    "model = SiameseLSTM(\n",
    "    vocab_size=len(vocabulary),\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    output_dim=OUTPUT_DIM,\n",
    ")\n",
    "\n",
    "model.load_state_dict(torch.load('/home/thangquang09/CODE/CTAI_MachineLearning/models/Siamese_LSTM_case2.pth', map_location='cpu'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5180f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of directories: 1068\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from load_data import read_texts_from_dir\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "df_test = read_texts_from_dir('/home/thangquang09/CODE/CTAI_MachineLearning/data/fake-or-real-the-impostor-hunt/data/test')\n",
    "df_test['label'] = 3\n",
    "\n",
    "test_dataset = TextComparisonDataset(df_test, vocabulary)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17064002",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "full_predicted = []\n",
    "\n",
    "for seq1, seq2, labels in test_dataloader:\n",
    "    seq1, seq2 = seq1.to(device), seq2.to(device)\n",
    "    outputs = model(seq1, seq2)\n",
    "    predicted = (torch.sigmoid(outputs.squeeze(1)) > 0.5).float()\n",
    "    full_predicted.append(predicted)\n",
    "\n",
    "# Concatenate all predictions\n",
    "full_predicted = torch.cat(full_predicted, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23a57915",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_predicted = full_predicted.cpu().numpy()\n",
    "full_predicted = full_predicted + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5be2636b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "\n",
    "def make_submission(y_pred, file_name):\n",
    "    submission = pd.DataFrame({\n",
    "        \"id\": df_test.index,\n",
    "        \"real_text_id\": y_pred.astype(int)\n",
    "    }).sort_values(\"id\")\n",
    "\n",
    "    save_path = Path(file_name)\n",
    "    submission.to_csv(save_path, index=False)\n",
    "    print(f\"✅ Submission saved to {save_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b51a11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Submission saved to /home/thangquang09/CODE/CTAI_MachineLearning/src/submission_case2_LSTM.csv\n"
     ]
    }
   ],
   "source": [
    "make_submission(full_predicted, 'submission_case2_LSTM.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac78d9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ctai-machinelearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
