{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12829168,"sourceType":"datasetVersion","datasetId":8113409}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"3dc33282-c32a-4c7f-a466-8628bda9eee8","cell_type":"code","source":"!pip install --q numpy pandas scipy tqdm nltk textstat torch transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T16:24:36.950830Z","iopub.execute_input":"2025-08-21T16:24:36.951313Z","iopub.status.idle":"2025-08-21T16:24:41.526342Z","shell.execute_reply.started":"2025-08-21T16:24:36.951275Z","shell.execute_reply":"2025-08-21T16:24:41.525072Z"},"_kg_hide-output":true},"outputs":[],"execution_count":8},{"id":"60837225-21bf-46a5-9f19-5fcfc62ae0bb","cell_type":"code","source":"# ----------------------------- IMPORTS ------------------------------------\nimport math\nimport os\nimport random\nimport re\nimport warnings\nfrom collections import Counter\n# from pathlib import Path\nfrom multiprocessing import Pool, cpu_count\n# from functools import partial\n\n# import joblib\n# import lightgbm as lgb\n\n# NLP\nimport nltk\nimport numpy as np\n# import optuna\nimport pandas as pd\nimport textstat\n\n# Deep Learning\nimport torch\n# from langdetect import DetectorFactory, detect\n# from langdetect.lang_detect_exception import LangDetectException\n# from nltk.corpus import stopwords\n# from nltk.stem import WordNetLemmatizer\n# from nltk.tokenize import sent_tokenize, word_tokenize\nfrom scipy.sparse import csr_matrix, hstack\n# from sklearn.feature_selection import SelectKBest, f_classif\n# from sklearn.linear_model import LogisticRegression\n# from sklearn.metrics import accuracy_score\n\n# ML & utils\n# from sklearn.model_selection import GridSearchCV, StratifiedKFold\n# from sklearn.pipeline import Pipeline\nfrom tqdm.auto import tqdm\nfrom transformers import AutoModel, AutoTokenizer\n\nwarnings.filterwarnings(\"ignore\")\nnltk.download(\"punkt\", quiet=True)\nnltk.download(\"wordnet\", quiet=True)\nnltk.download(\"stopwords\", quiet=True)\n\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\n# DetectorFactory.seed = SEED\n\n#----------------------------- BASE FUNCTION ------------------------------\n\ndef read_texts_from_dir(dir_path):\n    \"\"\"\n    Reads the texts from a given directory and saves them in the pd.DataFrame with columns ['id', 'file_1', 'file_2'].\n\n    Params:\n      dir_path (str): path to the directory with data\n    \"\"\"\n    # Count number of directories in the provided path\n    dir_count = sum(\n        os.path.isdir(os.path.join(root, d))\n        for root, dirs, _ in os.walk(dir_path)\n        for d in dirs\n    )\n    data = [0 for _ in range(dir_count)]\n    print(f\"Number of directories: {dir_count}\")\n\n    # For each directory, read both file_1.txt and file_2.txt and save results to the list\n    i = 0\n    for folder_name in sorted(os.listdir(dir_path)):\n        folder_path = os.path.join(dir_path, folder_name)\n        if os.path.isdir(folder_path):\n            try:\n                with open(\n                    os.path.join(folder_path, \"file_1.txt\"), \"r\", encoding=\"utf-8\"\n                ) as f1:\n                    text1 = f1.read().strip()\n                with open(\n                    os.path.join(folder_path, \"file_2.txt\"), \"r\", encoding=\"utf-8\"\n                ) as f2:\n                    text2 = f2.read().strip()\n                index = int(folder_name[-4:])\n                data[i] = (index, text1, text2)\n                i += 1\n            except Exception as e:\n                print(f\"Error reading directory {folder_name}: {e}\")\n\n    # Change list with results into pandas DataFrame\n    df = pd.DataFrame(data, columns=[\"id\", \"file_1\", \"file_2\"]).set_index(\"id\")\n    return df\n\ndef clean_text(text: str) -> str:\n    if not isinstance(text, str):\n        return \"\"\n    text = text.lower()\n    # 2. Xóa các ký tự không mong muốn nhưng giữ lại ' và - nếu ở trong từ\n    #   - Cho phép: chữ, số, khoảng trắng, ', -\n    text = re.sub(r\"[^a-z0-9\\s'\\-]\", \" \", text)\n    # 3. Chuẩn hoá khoảng trắng\n    text = re.sub(r\"\\s+\", \" \", text).strip()\n    return text\n\n# ----------------------------- FEATURE ENGINEERING ------------------------------\n\ndef compute_advanced_features(text: str) -> dict:\n    \"\"\"\n    Tính toán các features đếm và tỉ lệ, ... cho real or fake detection.\n    \"\"\"\n    if not isinstance(text, str) or not text.strip():\n        return {f: 0 for f in [\n            'unique_word_count_ratio', 'latin_ratio', 'digit_count', \n            'flesch_reading_ease', 'dale_chall_readability',\n            'coleman_liau_index', 'short_word_count_ratio', 'uppercase_ratio',\n            'english_ratio', 'perplexity_score', 'sentence_count',\n            'word_count', 'avg_word_length'\n        ]}\n    \n    # Basic text stats\n    cleaned_text = clean_text(text)\n    words = cleaned_text.split()\n    word_count = len(words)\n    unique_words = len(set(words))\n    char_count = len(text)\n\n    # 1. Unique word count ratio (top feature!)\n    unique_word_count_ratio = unique_words / max(word_count, 1)\n    \n    # 2. Latin ratio (character-based)\n    latin_chars = len(re.findall(r'[a-zA-Z]', text))\n    latin_ratio = latin_chars / max(char_count, 1)\n    \n    # 3. Digit count\n    digit_count = len(re.findall(r'\\d', text))\n    \n    # 4. Readability scores using textstat\n    flesch_reading_ease = textstat.flesch_reading_ease(text)\n    dale_chall_readability = textstat.dale_chall_readability_score(text)\n    coleman_liau_index = textstat.coleman_liau_index(text)\n\n    # 5. Short word ratio\n    short_words = [w for w in words if len(w) <= 3]\n    short_word_count_ratio = len(short_words) / max(word_count, 1)\n\n    # 6. Uppercase ratio\n    uppercase_chars = len(re.findall(r'[A-Z]', text))\n    uppercase_ratio = uppercase_chars / max(char_count, 1)\n\n    # 7. English ratio (approximate using common English patterns)\n    english_words = len(re.findall(r'\\b[a-zA-Z]+\\b', cleaned_text))\n    english_ratio = english_words / max(word_count, 1)\n\n    # 8. Simple perplexity approximation (entropy-based)\n    def calculate_perplexity(words):\n        if not words:\n            return 0\n        \n        word_freq = Counter(words)\n        total_words = len(words)  # Dùng total occurrences thay vì unique\n        \n        # Calculate entropy\n        entropy = 0\n        for freq in word_freq.values():\n            prob = freq / total_words\n            entropy -= prob * math.log2(prob)\n        \n        return 2 ** entropy if entropy > 0 else 1\n    perplexity_score = calculate_perplexity(words)\n\n\n    # 9. Sentence count\n    sentences = re.split(r'[.!?]+', text)\n    sentence_count = len([s for s in sentences if s.strip()])\n\n    # 10. Average word length\n    avg_word_length = sum(len(w) for w in words) / max(word_count, 1)\n    \n    return {\n        'unique_word_count_ratio': unique_word_count_ratio,\n        'latin_ratio': latin_ratio,\n        'digit_count': digit_count,\n        'flesch_reading_ease': flesch_reading_ease,\n        'dale_chall_readability': dale_chall_readability,\n        'coleman_liau_index': coleman_liau_index,\n        'short_word_count_ratio': short_word_count_ratio,\n        'uppercase_ratio': uppercase_ratio,\n        'english_ratio': english_ratio,\n        'perplexity_score': perplexity_score,\n        'sentence_count': sentence_count,\n        'word_count': word_count,\n        'avg_word_length': avg_word_length\n    }\n\ndef process_row_top_features(row_data):\n    \"\"\"Process single row for top features extraction (for multiprocessing).\"\"\"\n    text1, text2 = row_data\n    \n    # Get features for both texts\n    f1 = compute_advanced_features(text1)\n    f2 = compute_advanced_features(text2)\n    \n    # Create difference and ratio features (theo pattern từ biểu đồ)\n    feature_row = []\n    \n    # 1. unique_word_count_ratio (tỷ lệ giữa file1 và file2)\n    unique_ratio = (f1['unique_word_count_ratio'] + 1e-8) / (f2['unique_word_count_ratio'] + 1e-8)\n    unique_ratio = np.clip(unique_ratio, 0.1, 10.0)\n    feature_row.append(unique_ratio)\n    \n    # 2. latin_ratio_diff (signed difference)\n    latin_ratio_diff = f1['latin_ratio'] - f2['latin_ratio']\n    feature_row.append(latin_ratio_diff)\n    \n    # 3. digit_count_diff (signed difference)\n    digit_count_diff = f1['digit_count'] - f2['digit_count']\n    feature_row.append(digit_count_diff)\n    \n    # 4. semantic_similarity \n    def cosine_similarity(text1, text2):\n        words1 = Counter(text1.lower().split())\n        words2 = Counter(text2.lower().split())\n        # Get common words\n        common_words = set(words1.keys()) & set(words2.keys())\n        if not common_words:\n            return 0.0\n        # Calculate dot product and norms\n        dot_product = sum(words1[word] * words2[word] for word in common_words)\n        norm1 = math.sqrt(sum(count**2 for count in words1.values()))\n        norm2 = math.sqrt(sum(count**2 for count in words2.values()))\n        \n        return dot_product / (norm1 * norm2) if norm1 * norm2 > 0 else 0.0\n    semantic_similarity = cosine_similarity(text1, text2)\n    feature_row.append(semantic_similarity)\n    \n    \n    # 5. perplexity_diff (signed difference)\n    perplexity_diff = f1['perplexity_score'] - f2['perplexity_score']\n    feature_row.append(perplexity_diff)\n\n    # 6. flesch_reading_ease_ratio\n    flesch_ratio = (f1['flesch_reading_ease'] + 100) / (f2['flesch_reading_ease'] + 100)\n    flesch_ratio = np.clip(flesch_ratio, 0.1, 10.0)\n    feature_row.append(flesch_ratio)\n\n    # 7. short_word_count_ratio\n    short_ratio = (f1['short_word_count_ratio'] + 1e-8) / (f2['short_word_count_ratio'] + 1e-8)\n    short_ratio = np.clip(short_ratio, 0.1, 10.0)\n    feature_row.append(short_ratio)\n\n    # 8. readability_avg_ratio\n    readability_avg_1 = (f1['flesch_reading_ease'] + f1['dale_chall_readability']) / 2\n    readability_avg_2 = (f2['flesch_reading_ease'] + f2['dale_chall_readability']) / 2\n    readability_avg_ratio = (readability_avg_1 + 50) / (readability_avg_2 + 50)\n    readability_avg_ratio = np.clip(readability_avg_ratio, 0.1, 10.0)\n    feature_row.append(readability_avg_ratio)\n\n    # 9. dale_chall_readability_score_diff (signed difference)\n    dale_chall_diff = f1['dale_chall_readability'] - f2['dale_chall_readability']\n    feature_row.append(dale_chall_diff)\n\n    # 10. sentence_count_diff (signed difference)\n    sentence_count_diff = f1['sentence_count'] - f2['sentence_count']\n    feature_row.append(sentence_count_diff)\n\n    # 11. perplexity_ratio\n    perplexity_ratio = (f1['perplexity_score'] + 1e-8) / (f2['perplexity_score'] + 1e-8)\n    perplexity_ratio = np.clip(perplexity_ratio, 0.1, 10.0)\n    feature_row.append(perplexity_ratio)\n\n    # 12. coleman_liau_index_diff (signed difference)\n    coleman_diff = f1['coleman_liau_index'] - f2['coleman_liau_index']\n    feature_row.append(coleman_diff)\n\n    # 13. english_ratio_diff (signed difference)\n    english_ratio_diff = f1['english_ratio'] - f2['english_ratio']\n    feature_row.append(english_ratio_diff)\n\n    # 14. word_count_diff (signed difference)\n    word_count_diff = f1['word_count'] - f2['word_count']\n    feature_row.append(word_count_diff)\n\n    # 15. uppercase_ratio_diff (signed difference)\n    uppercase_ratio_diff = f1['uppercase_ratio'] - f2['uppercase_ratio']\n    feature_row.append(uppercase_ratio_diff)\n\n    # 16. latin_ratio_ratio\n    latin_ratio_ratio = (f1['latin_ratio'] + 1e-8) / (f2['latin_ratio'] + 1e-8)\n    latin_ratio_ratio = np.clip(latin_ratio_ratio, 0.1, 10.0)\n    feature_row.append(latin_ratio_ratio)\n\n    # 17. english_ratio_ratio\n    english_ratio_ratio = (f1['english_ratio'] + 1e-8) / (f2['english_ratio'] + 1e-8)\n    english_ratio_ratio = np.clip(english_ratio_ratio, 0.1, 10.0)\n    feature_row.append(english_ratio_ratio)\n\n    # Add individual features as well\n    feature_row.extend([\n        f1['unique_word_count_ratio'], f2['unique_word_count_ratio'],\n        f1['latin_ratio'], f2['latin_ratio'],\n        f1['flesch_reading_ease'], f2['flesch_reading_ease'],\n        f1['perplexity_score'], f2['perplexity_score']\n    ])\n    \n    return feature_row\n\ndef extract_top_features(df: pd.DataFrame, n_jobs: int = None) -> np.ndarray:\n    \"\"\"Extract top features theo biểu đồ importance with multiprocessing.\"\"\"\n    if n_jobs is None:\n        n_jobs = min(cpu_count(), 8)  # Limit to 8 cores max to avoid memory issues\n    \n    # Prepare data for multiprocessing\n    row_data = [(row['file_1'], row['file_2']) for _, row in df.iterrows()]\n    \n    print(f\"Using {n_jobs} cores for top features extraction...\")\n    \n    if len(row_data) < 100 or n_jobs == 1:\n        # For small datasets, use single process to avoid overhead\n        features = []\n        for data in tqdm(row_data, desc=\"Extracting top features (single-threaded)\"):\n            features.append(process_row_top_features(data))\n    else:\n        # Use multiprocessing for larger datasets\n        with Pool(n_jobs) as pool:\n            features = list(tqdm(\n                pool.imap(process_row_top_features, row_data),\n                total=len(row_data),\n                desc=\"Extracting top features (multi-threaded)\"\n            ))\n    \n    return np.array(features).astype(np.float32)\n\n\n# ----------------------- Extracting rule-based features ---------------------------\ndef compute_rule_based_features(text: str) -> dict:\n    \"\"\"Tính toán các đặc trưng rule-based cho một văn bản.\"\"\"\n    if not isinstance(text, str):\n        text = \"\"\n\n    # Existing features...\n    cleaned_text = clean_text(text)\n    word_count = len(cleaned_text.split())\n\n    # === NEW FEATURES FOR FAKE DETECTION ===\n\n    # 1. Multi-script detection (Garbage text pattern)\n    cyrillic_count = len(re.findall(r\"[\\u0400-\\u04FF]\", text))  # Russian\n    arabic_count = len(re.findall(r\"[\\u0600-\\u06FF]\", text))  # Arabic\n    chinese_count = len(re.findall(r\"[\\u4e00-\\u9fff]\", text))  # Chinese\n    emoji_count = len(\n        re.findall(\n            r\"[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF]\", text\n        )\n    )\n\n    # Mixed script score\n    script_diversity = sum(\n        1 for count in [cyrillic_count, arabic_count, chinese_count] if count > 0\n    )\n\n    # 2. Storytelling pattern detection\n    exclamation_ratio = text.count(\"!\") / max(len(text), 1)\n    question_ratio = text.count(\"?\") / max(len(text), 1)\n\n    # Informal language indicators\n    informal_words = [\"forget\", \"wow\", \"amazing\", \"incredible\", \"magic\", \"unicorn\"]\n    informal_count = sum(text.lower().count(word) for word in informal_words)\n\n    # Bold/emphasis markers (markdown style)\n    bold_count = text.count(\"**\") + text.count(\"__\")\n\n    # 3. Tone analysis\n    first_person_count = len(re.findall(r\"\\b(I|we|our|my|mine)\\b\", text, re.I))\n    second_person_count = len(re.findall(r\"\\b(you|your|yours)\\b\", text, re.I))\n\n    # Scientific vs casual tone\n    scientific_terms = [\n        \"observation\",\n        \"analysis\",\n        \"telescope\",\n        \"data\",\n        \"measurement\",\n        \"survey\",\n    ]\n    scientific_count = sum(text.lower().count(term) for term in scientific_terms)\n\n    # 4. Inconsistency detection\n    # Sudden language change (character encoding issues)\n    unicode_control_chars = len(re.findall(r\"[\\u0000-\\u001F\\u007F-\\u009F]\", text))\n\n    # Suspicious name patterns (China relay network, etc.)\n    suspicious_entities = [\"china relay\", \"rainbow unicorn\", \"santa\", \"north pole\"]\n    suspicious_count = sum(text.lower().count(entity) for entity in suspicious_entities)\n\n\n    return {\n        # Multi-script features\n        \"cyrillic_count\": cyrillic_count,\n        \"arabic_count\": arabic_count,\n        \"chinese_count\": chinese_count,\n        \"emoji_count\": emoji_count,\n        \"script_diversity\": script_diversity,\n        # Storytelling features\n        \"exclamation_ratio\": exclamation_ratio,\n        \"question_ratio\": question_ratio,\n        \"informal_count\": informal_count,\n        \"bold_count\": bold_count,\n        # Tone features\n        \"first_person_count\": first_person_count,\n        \"second_person_count\": second_person_count,\n        \"scientific_count\": scientific_count,\n        # Inconsistency features\n        \"unicode_control_chars\": unicode_control_chars,\n        \"suspicious_count\": suspicious_count,\n        # Existing features\n        \"number_count\": len(re.findall(r\"\\d+\", text)),\n        \"unit_count\": len(\n            re.findall(\n                r\"\\b(?:km|cm|m|s|kg|g|Hz|K|A|deg|arcsec|dex|A|petabytes|terabytes)\\b\",\n                text,\n                re.I,\n            )\n        ),\n        \"acronym_count\": len(re.findall(r\"\\b[A-Z]{2,}\\b\", text)),\n        \"uppercase_word_count\": len(re.findall(r\"\\b[A-Z][A-Z]+\\b\", text)),\n        \"exclamation_count\": text.count(\"!\"),\n        \"repetition_score\": sum(\n            [\n                count\n                for word, count in Counter(cleaned_text.lower().split()).items()\n                if count > 3\n            ]\n        )\n        / max(word_count, 1),\n    }\n\n\ndef process_row_rule_based(row_data):\n    \"\"\"Process single row for rule-based features extraction (for multiprocessing).\"\"\"\n    text1, text2 = row_data\n    f1 = compute_rule_based_features(text1)\n    f2 = compute_rule_based_features(text2)\n    \n    # Tạo diff features\n    diff = {k: f1[k] - f2[k] for k in f1}\n    \n    # Kết hợp f1, f2, diff thành một vector\n    feature_vector = list(f1.values()) + list(f2.values()) + list(diff.values())\n    return feature_vector\n\ndef extract_rule_based_features(df: pd.DataFrame, n_jobs: int = None) -> np.ndarray:\n    \"\"\"Tạo ma trận đặc trưng rule-based với multiprocessing.\"\"\"\n    if n_jobs is None:\n        n_jobs = min(cpu_count(), 8)\n    \n    # Prepare data for multiprocessing\n    row_data = [(row['file_1'], row['file_2']) for _, row in df.iterrows()]\n    \n    print(f\"Using {n_jobs} cores for rule-based features extraction...\")\n    \n    if len(row_data) < 100 or n_jobs == 1:\n        # For small datasets, use single process\n        features = []\n        for data in tqdm(row_data, desc=\"Extracting rule-based features (single-threaded)\"):\n            features.append(process_row_rule_based(data))\n    else:\n        # Use multiprocessing for larger datasets\n        with Pool(n_jobs) as pool:\n            features = list(tqdm(\n                pool.imap(process_row_rule_based, row_data),\n                total=len(row_data),\n                desc=\"Extracting rule-based features (multi-threaded)\"\n            ))\n\n    return np.array(features).astype(np.float32)\n\n# ----------------------- PREPARE DATA FOR MODEL ---------------------------\nclass EmbeddingExtractor:\n    def __init__(self, model_name: str, max_length: int = 512):\n        self.model_name = model_name\n        self.max_length = max_length\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n        self.model = AutoModel.from_pretrained(model_name, trust_remote_code=True).to(self.device)\n\n    def get_embedding(self, text : str ) -> np.ndarray:\n        inputs = self.tokenizer(\n            text,\n            return_tensors=\"pt\",\n            padding=True,\n            truncation=True,\n            max_length=self.max_length\n        )\n        with torch.no_grad():\n            embeddings = self.model(inputs['input_ids'].to(self.device), inputs['attention_mask'].to(self.device)).last_hidden_state[:, 0, :].cpu().numpy()\n        return embeddings\n\n\ndef extract_embedding_features(df: pd.DataFrame, embedding_extractor ) -> np.ndarray:\n    \"\"\"Extract embedding features from the text data using the embedding extractor.\"\"\"\n    embeddings = []\n    for text in tqdm(df['text'], desc=\"Extracting embeddings\"):\n        embedding = embedding_extractor.get_embedding(text )\n        embeddings.append(embedding)\n    return np.vstack(embeddings)\n\n# ----------------------- PREPARE DATA FOR MODEL ---------------------------\ndef prepare_data_for_model(\n    df: pd.DataFrame,\n    embedding_extractor: EmbeddingExtractor = None,\n    model_name: str = \"bert-base-uncased\",\n    n_jobs: int = None,\n):\n    \"\"\"\n    Chuẩn bị dữ liệu cho model với focus vào top features theo importance chart.\n\n    Args:\n        df: DataFrame chứa dữ liệu thô\n        embedding_extractor: Universal embedding extractor, nếu None sẽ tạo mới\n        model_name: Tên model embedding để sử dụng\n        n_jobs: Số lượng CPU cores để sử dụng cho multiprocessing\n\n    Returns:\n        feature_matrix: Ma trận features đã kết hợp\n        embedding_extractor: Embedding extractor (để dùng cho test set)\n    \"\"\"\n    if n_jobs is None:\n        n_jobs = min(cpu_count(), 8)  # Default to 8 cores max\n    \n    print(f\"Using {n_jobs} CPU cores for feature extraction...\")\n    \n    # 0. clean text\n    df['cleaned_file_1'] = df['file_1'].apply(clean_text)\n    df['cleaned_file_2'] = df['file_2'].apply(clean_text)\n    df['text'] = '[CLS] ' + df['cleaned_file_1'] + \" [SEP] \" + df['cleaned_file_2']\n\n    # 1. Extract top features (most important) - with multiprocessing\n    print(\"Step 1: Extracting top importance features...\")\n    top_features = extract_top_features(df, n_jobs=n_jobs)\n    \n    # 2. Extract rule-based features (existing) - with multiprocessing\n    print(\"Step 2: Extracting rule-based features...\")\n    rule_features = extract_rule_based_features(df, n_jobs=n_jobs)\n    \n    # # 4. Extract embedding features (lighter approach) - handled by embedding extractor\n    # print(\"Step 4: Extracting embedding features...\")\n    # if embedding_extractor is None:\n    #     embedding_extractor = EmbeddingExtractor(\n    #         model_name=model_name,\n    #         max_length=512,\n    #     )\n    \n    # embedding_features = extract_embedding_features(df, embedding_extractor)\n    \n    # 6. Combine all features with priority on top features\n    print(\"Step 6: Combining features...\")  \n    # Priority: top features first, then others\n    # feature_matrix = np.hstack([top_features, rule_features, embedding_features])\n    feature_matrix = np.hstack([top_features, rule_features])\n\n    print(f\"Final feature matrix shape: {feature_matrix.shape}\")\n    # print(f\"Top features: {top_features.shape[1]}, Rule: {rule_features.shape[1]}, Embedding: {embedding_features.shape[1]}\")\n\n    return feature_matrix","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T16:24:41.528935Z","iopub.execute_input":"2025-08-21T16:24:41.529274Z","iopub.status.idle":"2025-08-21T16:24:41.597724Z","shell.execute_reply.started":"2025-08-21T16:24:41.529247Z","shell.execute_reply":"2025-08-21T16:24:41.596700Z"}},"outputs":[],"execution_count":9},{"id":"3261ee8c","cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\n\ndef read_texts_from_dir(dir_path):\n    \"\"\"\n    Reads the texts from a given directory and saves them in the pd.DataFrame with columns ['id', 'file_1', 'file_2'].\n\n    Params:\n      dir_path (str): path to the directory with data\n    \"\"\"\n    # Count number of directories in the provided path\n    dir_count = sum(\n        os.path.isdir(os.path.join(root, d))\n        for root, dirs, _ in os.walk(dir_path)\n        for d in dirs\n    )\n    data = [0 for _ in range(dir_count)]\n    print(f\"Number of directories: {dir_count}\")\n\n    # For each directory, read both file_1.txt and file_2.txt and save results to the list\n    i = 0\n    for folder_name in sorted(os.listdir(dir_path)):\n        folder_path = os.path.join(dir_path, folder_name)\n        if os.path.isdir(folder_path):\n            try:\n                with open(\n                    os.path.join(folder_path, \"file_1.txt\"), \"r\", encoding=\"utf-8\"\n                ) as f1:\n                    text1 = f1.read().strip()\n                with open(\n                    os.path.join(folder_path, \"file_2.txt\"), \"r\", encoding=\"utf-8\"\n                ) as f2:\n                    text2 = f2.read().strip()\n                index = int(folder_name[-4:])\n                data[i] = (index, text1, text2)\n                i += 1\n            except Exception as e:\n                print(f\"Error reading directory {folder_name}: {e}\")\n\n    # Change list with results into pandas DataFrame\n    df = pd.DataFrame(data, columns=[\"id\", \"file_1\", \"file_2\"]).set_index(\"id\")\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T16:24:41.598633Z","iopub.execute_input":"2025-08-21T16:24:41.598881Z","iopub.status.idle":"2025-08-21T16:24:41.652249Z","shell.execute_reply.started":"2025-08-21T16:24:41.598862Z","shell.execute_reply":"2025-08-21T16:24:41.651236Z"}},"outputs":[],"execution_count":10},{"id":"3c40d538","cell_type":"code","source":"df_train = read_texts_from_dir(\"/kaggle/input/real-or-fake/data/train\")\ndf_test = read_texts_from_dir(\"/kaggle/input/real-or-fake/data/test\")\ndf_train_gt = pd.read_csv(\"/kaggle/input/real-or-fake/data/train.csv\")\ny_train = df_train_gt[\"real_text_id\"].values\ndf_train['label'] = df_train_gt[\"real_text_id\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T16:24:41.654264Z","iopub.execute_input":"2025-08-21T16:24:41.654642Z","iopub.status.idle":"2025-08-21T16:24:44.120862Z","shell.execute_reply.started":"2025-08-21T16:24:41.654620Z","shell.execute_reply":"2025-08-21T16:24:44.120066Z"}},"outputs":[{"name":"stdout","text":"Number of directories: 95\nNumber of directories: 1068\n","output_type":"stream"}],"execution_count":11},{"id":"a6dd7e0b","cell_type":"code","source":"df_train.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T16:24:44.121600Z","iopub.execute_input":"2025-08-21T16:24:44.121816Z","iopub.status.idle":"2025-08-21T16:24:44.132364Z","shell.execute_reply.started":"2025-08-21T16:24:44.121799Z","shell.execute_reply":"2025-08-21T16:24:44.131538Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"                                               file_1  \\\nid                                                      \n0   The VIRSA (Visible Infrared Survey Telescope A...   \n1   China\\nThe goal of this project involves achie...   \n2   Scientists can learn about how galaxies form a...   \n3   China\\nThe study suggests that multiple star s...   \n4   Dinosaur Rex was excited about his new toy set...   \n\n                                               file_2  label  \nid                                                            \n0   The China relay network has released a signifi...      1  \n1   The project aims to achieve an accuracy level ...      2  \n2   Dinosaur eggshells offer clues about what dino...      1  \n3   The importance for understanding how stars evo...      2  \n4   Analyzing how fast stars rotate within a galax...      2  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>file_1</th>\n      <th>file_2</th>\n      <th>label</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>The VIRSA (Visible Infrared Survey Telescope A...</td>\n      <td>The China relay network has released a signifi...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>China\\nThe goal of this project involves achie...</td>\n      <td>The project aims to achieve an accuracy level ...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Scientists can learn about how galaxies form a...</td>\n      <td>Dinosaur eggshells offer clues about what dino...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>China\\nThe study suggests that multiple star s...</td>\n      <td>The importance for understanding how stars evo...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Dinosaur Rex was excited about his new toy set...</td>\n      <td>Analyzing how fast stars rotate within a galax...</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":12},{"id":"6641a6a6-c271-49c9-9f05-87a049d42a21","cell_type":"markdown","source":"## Finetune Encoder for Data","metadata":{}},{"id":"d80bc984-de10-4924-9eb6-74788bbad692","cell_type":"code","source":"\ndf_finetune = df_train.copy()\ndf_finetune['file_1'] = df_finetune['file_1'].apply(clean_text)\ndf_finetune['file_2'] = df_finetune['file_2'].apply(clean_text)\ndf_finetune['label'] = df_finetune['label'] - 1\ndf_finetune['text'] = '[CLS] ' + df_finetune['file_1'] + \" [SEP] \" + df_finetune['file_2']\ndf_finetune.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T16:24:44.133376Z","iopub.execute_input":"2025-08-21T16:24:44.133693Z","iopub.status.idle":"2025-08-21T16:24:44.210012Z","shell.execute_reply.started":"2025-08-21T16:24:44.133667Z","shell.execute_reply":"2025-08-21T16:24:44.209094Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"                                               file_1  \\\nid                                                      \n0   the virsa visible infrared survey telescope ar...   \n1   china the goal of this project involves achiev...   \n2   scientists can learn about how galaxies form a...   \n3   china the study suggests that multiple star sy...   \n4   dinosaur rex was excited about his new toy set...   \n\n                                               file_2  label  \\\nid                                                             \n0   the china relay network has released a signifi...      0   \n1   the project aims to achieve an accuracy level ...      1   \n2   dinosaur eggshells offer clues about what dino...      0   \n3   the importance for understanding how stars evo...      1   \n4   analyzing how fast stars rotate within a galax...      1   \n\n                                                 text  \nid                                                     \n0   [CLS] the virsa visible infrared survey telesc...  \n1   [CLS] china the goal of this project involves ...  \n2   [CLS] scientists can learn about how galaxies ...  \n3   [CLS] china the study suggests that multiple s...  \n4   [CLS] dinosaur rex was excited about his new t...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>file_1</th>\n      <th>file_2</th>\n      <th>label</th>\n      <th>text</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>the virsa visible infrared survey telescope ar...</td>\n      <td>the china relay network has released a signifi...</td>\n      <td>0</td>\n      <td>[CLS] the virsa visible infrared survey telesc...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>china the goal of this project involves achiev...</td>\n      <td>the project aims to achieve an accuracy level ...</td>\n      <td>1</td>\n      <td>[CLS] china the goal of this project involves ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>scientists can learn about how galaxies form a...</td>\n      <td>dinosaur eggshells offer clues about what dino...</td>\n      <td>0</td>\n      <td>[CLS] scientists can learn about how galaxies ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>china the study suggests that multiple star sy...</td>\n      <td>the importance for understanding how stars evo...</td>\n      <td>1</td>\n      <td>[CLS] china the study suggests that multiple s...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>dinosaur rex was excited about his new toy set...</td>\n      <td>analyzing how fast stars rotate within a galax...</td>\n      <td>1</td>\n      <td>[CLS] dinosaur rex was excited about his new t...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":13},{"id":"632647db-481f-443f-824d-2b3b08533cf1","cell_type":"code","source":"from datasets import Dataset\nfinetune_dataset = Dataset.from_pandas(df_finetune[['text', 'label']])\nsplit_datasets = finetune_dataset.train_test_split(test_size=0.2)\ntrain_dataset = split_datasets['train']\nval_dataset = split_datasets['test']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T16:00:23.299916Z","iopub.execute_input":"2025-08-21T16:00:23.300106Z","iopub.status.idle":"2025-08-21T16:00:24.286372Z","shell.execute_reply.started":"2025-08-21T16:00:23.300091Z","shell.execute_reply":"2025-08-21T16:00:24.285774Z"}},"outputs":[],"execution_count":7},{"id":"ad6cc3e8-f8ad-4bc6-89a3-f3176ef5d33f","cell_type":"code","source":"import torch\nfrom transformers import AutoModel, AutoTokenizer\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Sử dụng thiết bị:\", device)\nmodel_name = 'google-bert/bert-base-uncased'\n\nd_model = 1024\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.add_special_tokens({'cls_token': '[CLS]'})\ntokenizer.add_special_tokens({'sep_token': '[SEP]'})\n# encoder = AutoModel.from_pretrained(model_name, trust_remote_code=True).to(device)\n\n\nmax_length = 512\ndef tokenizer_fn(example):\n    tokenized_inputs = tokenizer(example['text'], padding= 'max_length', truncation= True, return_tensors= 'pt', max_length= max_length).to(device)\n    if 'label' in example.keys():\n        tokenized_inputs['labels'] = torch.Tensor(example['label']).to(device)\n    return tokenized_inputs\n\ntrain_dataset = train_dataset.map(tokenizer_fn, batched=True)\nval_dataset = val_dataset.map(tokenizer_fn, batched=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T16:00:24.286992Z","iopub.execute_input":"2025-08-21T16:00:24.287305Z","iopub.status.idle":"2025-08-21T16:00:25.754219Z","shell.execute_reply.started":"2025-08-21T16:00:24.287289Z","shell.execute_reply":"2025-08-21T16:00:25.753381Z"}},"outputs":[{"name":"stdout","text":"Sử dụng thiết bị: cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59e3cd4ba87d41759d3edcdd8e1c3b90"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f70333f36704492a855843465b17973"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"125d405f28624f058f430db113d53baa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0f82aab572e41618c7da21713b47f9a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/76 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09cb84113e4a4eef8942b5a03a7d758d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/19 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f1b243f859d498a885420eff0b512b8"}},"metadata":{}}],"execution_count":8},{"id":"9eebfc5f-562d-4f24-88c4-e1eec69363f1","cell_type":"code","source":"train_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T16:00:25.755219Z","iopub.execute_input":"2025-08-21T16:00:25.755483Z","iopub.status.idle":"2025-08-21T16:00:25.761309Z","shell.execute_reply.started":"2025-08-21T16:00:25.755464Z","shell.execute_reply":"2025-08-21T16:00:25.760482Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['text', 'label', 'id', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n    num_rows: 76\n})"},"metadata":{}}],"execution_count":9},{"id":"31c36000-fcb9-475f-b477-ba1f1bab1738","cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch\nfrom transformers import AutoModel, AutoTokenizer\n\n\nclass Encoder(nn.Module):\n    def __init__(self, model_name, d_model, max_length = 512, num_label = 2):\n        super().__init__()\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        print('sử dụng', self.device)\n        self.encoder = AutoModel.from_pretrained(model_name, trust_remote_code=True).to(self.device)\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n        self.tokenizer.add_special_tokens({'cls_token': '[CLS]'})\n        self.tokenizer.add_special_tokens({'sep_token': '[SEP]'})\n        self.encoder.resize_token_embeddings(len(tokenizer))\n        self.d_model = d_model\n        self.max_length= max_length\n        self.fc = nn.Linear(d_model, num_label)\n\n        # setup loss function\n        self.loss_fn = nn.CrossEntropyLoss()\n\n    def forward(self, input_ids, attention_mask, labels = None ):\n        embeddings = self.encoder(input_ids= input_ids, attention_mask= attention_mask).last_hidden_state[:, 0, :]\n        logits = self.fc(embeddings)\n        loss = None\n        if labels is not None:\n            loss = self.loss_fn(logits, labels)\n            return {\"loss\": loss, \"logits\": logits}\n        return {\"logits\": logits}\n    def get_embedding(self, text):\n        cleaned_text = clean_text(text)\n        inputs = self.tokenizer(cleaned_text, return_tensors=\"pt\", padding='max_length', max_length= self.max_length, truncation=True).to(self.device)\n        with torch.no_grad():\n            embeddings = self.encoder(**inputs).last_hidden_state[:, 0, :].cpu().numpy()\n        return embeddings","metadata":{"trusted":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2025-08-21T16:00:25.765120Z","iopub.execute_input":"2025-08-21T16:00:25.765446Z","iopub.status.idle":"2025-08-21T16:00:26.542138Z","shell.execute_reply.started":"2025-08-21T16:00:25.765428Z","shell.execute_reply":"2025-08-21T16:00:26.541394Z"}},"outputs":[],"execution_count":10},{"id":"38f63795-f428-4524-adf2-1da66de76b55","cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel_name = 'google-bert/bert-base-uncased'\nencoder = Encoder(model_name = model_name, d_model = 768, max_length = 512).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T16:00:26.543004Z","iopub.execute_input":"2025-08-21T16:00:26.543323Z","iopub.status.idle":"2025-08-21T16:00:48.460040Z","shell.execute_reply.started":"2025-08-21T16:00:26.543300Z","shell.execute_reply":"2025-08-21T16:00:48.459440Z"}},"outputs":[{"name":"stdout","text":"sử dụng cuda\n","output_type":"stream"},{"name":"stderr","text":"2025-08-21 16:00:36.103320: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1755792036.252694      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1755792036.296092      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aab371bcc93949098f15eebd062e09f5"}},"metadata":{}}],"execution_count":11},{"id":"5834ba17-a0aa-4c92-8776-8ea723213c44","cell_type":"code","source":"from transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"./PairwiseClassification\",\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    per_device_train_batch_size=4,   # giảm xuống 1\n    per_device_eval_batch_size=4,\n    gradient_accumulation_steps=8,   # giữ effective batch size = 4\n    learning_rate=3e-5,\n    num_train_epochs=5,\n    warmup_ratio=0.1,\n    lr_scheduler_type=\"linear\",\n    logging_strategy=\"epoch\",\n    fp16=True,                       # nếu lỗi thì thử bf16 hoặc tắt\n    optim=\"adamw_torch\",\n    load_best_model_at_end=True,\n    weight_decay=0.03,\n    metric_for_best_model=\"f1\"\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T16:00:48.460735Z","iopub.execute_input":"2025-08-21T16:00:48.461432Z","iopub.status.idle":"2025-08-21T16:00:49.674124Z","shell.execute_reply.started":"2025-08-21T16:00:48.461404Z","shell.execute_reply":"2025-08-21T16:00:49.673552Z"}},"outputs":[],"execution_count":12},{"id":"8637126c-0234-4611-9d05-bbccdf2f389a","cell_type":"code","source":"from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score\ndef compute_metrics(eval_pred):\n    y_pred, y_true = np.argmax(eval_pred.predictions, -1), eval_pred.label_ids\n    return {'accuracy': accuracy_score(y_true, y_pred),\n            'precision': precision_score(y_true, y_pred),\n            'recall': recall_score(y_true, y_pred),\n            'f1': f1_score(y_true, y_pred)}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T16:00:49.674776Z","iopub.execute_input":"2025-08-21T16:00:49.675050Z","iopub.status.idle":"2025-08-21T16:00:49.680546Z","shell.execute_reply.started":"2025-08-21T16:00:49.675026Z","shell.execute_reply":"2025-08-21T16:00:49.679662Z"}},"outputs":[],"execution_count":13},{"id":"64fa3b7b-b278-498e-999a-a65485a06a56","cell_type":"code","source":"!pip --q install wandb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T16:00:49.681512Z","iopub.execute_input":"2025-08-21T16:00:49.681793Z","iopub.status.idle":"2025-08-21T16:00:52.864102Z","shell.execute_reply.started":"2025-08-21T16:00:49.681769Z","shell.execute_reply":"2025-08-21T16:00:52.863301Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":14},{"id":"2bbf6fa7-c347-4d49-999d-57a1001bd598","cell_type":"code","source":"import wandb\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\nwandb.login(key= user_secrets.get_secret(\"wandb_key\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T16:00:52.865108Z","iopub.execute_input":"2025-08-21T16:00:52.865326Z","iopub.status.idle":"2025-08-21T16:01:00.742517Z","shell.execute_reply.started":"2025-08-21T16:00:52.865296Z","shell.execute_reply":"2025-08-21T16:01:00.741609Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtrongbg2692004\u001b[0m (\u001b[33mtrongbg2692004-post-and-telecommunications-institute-of-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":15},{"id":"b59008ce-2861-47fb-9258-d5839d7b131e","cell_type":"code","source":"from transformers import Trainer\n\n# Hàm preprocess để convert dict của dataset thành tensor đúng định dạng cho mô hình\ndef collate_fn(batch):\n    input_ids = torch.tensor([item[\"input_ids\"] for item in batch])\n    attention_mask = torch.tensor([item[\"attention_mask\"] for item in batch])\n    # Chuyển đổi nhãn sang kiểu long\n    labels = torch.tensor([item[\"labels\"] for item in batch], dtype=torch.long)\n    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n\ntrainer = Trainer(\n    model=encoder,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset= val_dataset,\n    data_collator=collate_fn,\n    compute_metrics=compute_metrics\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T16:01:00.744553Z","iopub.execute_input":"2025-08-21T16:01:00.745366Z","iopub.status.idle":"2025-08-21T16:01:01.884417Z","shell.execute_reply.started":"2025-08-21T16:01:00.745337Z","shell.execute_reply":"2025-08-21T16:01:01.883851Z"}},"outputs":[],"execution_count":16},{"id":"fbff69f0-1a0a-4dbb-b950-0a63e4961018","cell_type":"code","source":"num_params = sum(p.numel() for p in encoder.parameters())\nprint(\"Tổng số tham số của mô hình:\", num_params)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T16:01:01.885157Z","iopub.execute_input":"2025-08-21T16:01:01.885424Z","iopub.status.idle":"2025-08-21T16:01:01.890518Z","shell.execute_reply.started":"2025-08-21T16:01:01.885406Z","shell.execute_reply":"2025-08-21T16:01:01.889690Z"}},"outputs":[{"name":"stdout","text":"Tổng số tham số của mô hình: 109483778\n","output_type":"stream"}],"execution_count":17},{"id":"4dd2e782-f7f2-451f-9797-1be4bc5f26f6","cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T16:01:01.891279Z","iopub.execute_input":"2025-08-21T16:01:01.891945Z","iopub.status.idle":"2025-08-21T16:01:46.933195Z","shell.execute_reply.started":"2025-08-21T16:01:01.891916Z","shell.execute_reply":"2025-08-21T16:01:46.932465Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.20.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250821_160102-fnbbg33a</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/trongbg2692004-post-and-telecommunications-institute-of-/huggingface/runs/fnbbg33a' target=\"_blank\">./PairwiseClassification</a></strong> to <a href='https://wandb.ai/trongbg2692004-post-and-telecommunications-institute-of-/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/trongbg2692004-post-and-telecommunications-institute-of-/huggingface' target=\"_blank\">https://wandb.ai/trongbg2692004-post-and-telecommunications-institute-of-/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/trongbg2692004-post-and-telecommunications-institute-of-/huggingface/runs/fnbbg33a' target=\"_blank\">https://wandb.ai/trongbg2692004-post-and-telecommunications-institute-of-/huggingface/runs/fnbbg33a</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [10/10 00:32, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.439400</td>\n      <td>0.677759</td>\n      <td>0.526316</td>\n      <td>0.500000</td>\n      <td>0.888889</td>\n      <td>0.640000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.412200</td>\n      <td>0.600469</td>\n      <td>0.578947</td>\n      <td>0.538462</td>\n      <td>0.777778</td>\n      <td>0.636364</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.360500</td>\n      <td>0.513360</td>\n      <td>0.736842</td>\n      <td>0.833333</td>\n      <td>0.555556</td>\n      <td>0.666667</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.331600</td>\n      <td>0.512176</td>\n      <td>0.736842</td>\n      <td>0.700000</td>\n      <td>0.777778</td>\n      <td>0.736842</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.301000</td>\n      <td>0.536611</td>\n      <td>0.684211</td>\n      <td>0.636364</td>\n      <td>0.777778</td>\n      <td>0.700000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=10, training_loss=0.36893380284309385, metrics={'train_runtime': 44.381, 'train_samples_per_second': 8.562, 'train_steps_per_second': 0.225, 'total_flos': 0.0, 'train_loss': 0.36893380284309385, 'epoch': 5.0})"},"metadata":{}}],"execution_count":18},{"id":"772ba460-10a8-45f6-996d-b89589411c89","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"d3f93b3e-dd3a-4e07-92bb-298124dd6865","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"25360573-72f4-4355-9fae-45328727ba1e","cell_type":"markdown","source":"## Prepare Data for classification","metadata":{}},{"id":"c881c62d","cell_type":"code","source":"# embedding_extractor = EmbeddingExtractor(model_name='Qwen/Qwen3-Embedding-0.6B', max_length = 1024)\nfeature_matrix = prepare_data_for_model(df_train, embedding_extractor=None)\nfeature_matrix.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T16:24:48.559067Z","iopub.execute_input":"2025-08-21T16:24:48.559699Z","iopub.status.idle":"2025-08-21T16:24:49.675275Z","shell.execute_reply.started":"2025-08-21T16:24:48.559671Z","shell.execute_reply":"2025-08-21T16:24:49.674376Z"}},"outputs":[{"name":"stdout","text":"Using 4 CPU cores for feature extraction...\nStep 1: Extracting top importance features...\nUsing 4 cores for top features extraction...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting top features (single-threaded):   0%|          | 0/95 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6ab8e078d3d4a04ad4881c8fe6e8a66"}},"metadata":{}},{"name":"stdout","text":"Step 2: Extracting rule-based features...\nUsing 4 cores for rule-based features extraction...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting rule-based features (single-threaded):   0%|          | 0/95 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ab5a6131ab6442f85df98f47b85c011"}},"metadata":{}},{"name":"stdout","text":"Step 6: Combining features...\nFinal feature matrix shape: (95, 85)\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"(95, 85)"},"metadata":{}}],"execution_count":14},{"id":"456cce67","cell_type":"code","source":"feature_matrix_test = prepare_data_for_model(df_test, embedding_extractor=None)\nfeature_matrix_test.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T16:25:03.075747Z","iopub.execute_input":"2025-08-21T16:25:03.076058Z","iopub.status.idle":"2025-08-21T16:25:10.077387Z","shell.execute_reply.started":"2025-08-21T16:25:03.076034Z","shell.execute_reply":"2025-08-21T16:25:10.076340Z"}},"outputs":[{"name":"stdout","text":"Using 4 CPU cores for feature extraction...\nStep 1: Extracting top importance features...\nUsing 4 cores for top features extraction...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting top features (multi-threaded):   0%|          | 0/1068 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5b1020d060b4c5da0903de53f245330"}},"metadata":{}},{"name":"stdout","text":"Step 2: Extracting rule-based features...\nUsing 4 cores for rule-based features extraction...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting rule-based features (multi-threaded):   0%|          | 0/1068 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"227435fb5b4c46ff9300dc5997eaa253"}},"metadata":{}},{"name":"stdout","text":"Step 6: Combining features...\nFinal feature matrix shape: (1068, 85)\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"(1068, 85)"},"metadata":{}}],"execution_count":16},{"id":"a4a09cfd","cell_type":"code","source":"X_test = feature_matrix_test.copy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T16:25:28.044732Z","iopub.execute_input":"2025-08-21T16:25:28.045423Z","iopub.status.idle":"2025-08-21T16:25:28.049814Z","shell.execute_reply.started":"2025-08-21T16:25:28.045389Z","shell.execute_reply":"2025-08-21T16:25:28.048893Z"}},"outputs":[],"execution_count":17},{"id":"785533de","cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(feature_matrix, y_train, test_size=0.2, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T16:26:01.839314Z","iopub.execute_input":"2025-08-21T16:26:01.839652Z","iopub.status.idle":"2025-08-21T16:26:01.846943Z","shell.execute_reply.started":"2025-08-21T16:26:01.839629Z","shell.execute_reply":"2025-08-21T16:26:01.846033Z"}},"outputs":[],"execution_count":18},{"id":"be5ac816","cell_type":"code","source":"X_train.shape, X_val.shape, y_train.shape, y_val.shape, X_test.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T16:26:01.848205Z","iopub.execute_input":"2025-08-21T16:26:01.848493Z","iopub.status.idle":"2025-08-21T16:26:01.864723Z","shell.execute_reply.started":"2025-08-21T16:26:01.848473Z","shell.execute_reply":"2025-08-21T16:26:01.863776Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"((76, 85), (19, 85), (76,), (19,), (1068, 85))"},"metadata":{}}],"execution_count":19},{"id":"5097929f","cell_type":"markdown","source":"### Lựa chọn mô hình phân loại với dữ liệu nhiều thuộc tính (853 features)\n- **Tree-based models** như Random Forest, Gradient Boosting (XGBoost, LightGBM, CatBoost) rất phù hợp với dữ liệu nhiều chiều, không cần chuẩn hóa đặc trưng, tự động chọn thuộc tính quan trọng và chống overfitting tốt.\n- **Logistic Regression** với regularization (L1/L2) cũng có thể thử, nhưng hiệu quả thường kém hơn tree-based khi dữ liệu phi tuyến tính và nhiều thuộc tính không quan trọng.\n- **SVM** (Support Vector Machine) có thể dùng, nhưng với số chiều lớn sẽ tốn nhiều tài nguyên và thời gian.\n- **Neural Network** (MLP) chỉ nên dùng nếu dữ liệu rất lớn và đã chuẩn hóa tốt.\n\n**Khuyến nghị:**\n- Ưu tiên thử Random Forest hoặc LightGBM/XGBoost đầu tiên.\n- Có thể dùng Logistic Regression để baseline và kiểm tra feature importance.\n- Nên dùng cross-validation để chọn mô hình tối ưu.","metadata":{}},{"id":"21151aa1","cell_type":"markdown","source":"## RandomForestClassifier","metadata":{}},{"id":"4ce73b1e","cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\nrdc = RandomForestClassifier()\nrdc.fit(X_train, y_train)\ny_pred = rdc.predict(X_val)\n\nprint(classification_report(y_val, y_pred))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T16:26:01.865559Z","iopub.execute_input":"2025-08-21T16:26:01.865848Z","iopub.status.idle":"2025-08-21T16:26:02.310723Z","shell.execute_reply.started":"2025-08-21T16:26:01.865824Z","shell.execute_reply":"2025-08-21T16:26:02.309925Z"}},"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           1       1.00      0.75      0.86        12\n           2       0.70      1.00      0.82         7\n\n    accuracy                           0.84        19\n   macro avg       0.85      0.88      0.84        19\nweighted avg       0.89      0.84      0.84        19\n\n","output_type":"stream"}],"execution_count":20},{"id":"bf64df58","cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n    'n_estimators': [i for i in range(100, 201, 20)],\n    'max_depth': [i for i in range(1, 21)],\n}\nrdc = RandomForestClassifier()\n\ngrid_search = GridSearchCV(estimator=rdc, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\ngrid_search.fit(X_train, y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T16:26:02.312241Z","iopub.execute_input":"2025-08-21T16:26:02.312514Z","iopub.status.idle":"2025-08-21T16:26:56.879093Z","shell.execute_reply.started":"2025-08-21T16:26:02.312483Z","shell.execute_reply":"2025-08-21T16:26:56.878338Z"}},"outputs":[{"name":"stdout","text":"Fitting 5 folds for each of 120 candidates, totalling 600 fits\n[CV] END ......................max_depth=1, n_estimators=100; total time=   0.2s\n[CV] END ......................max_depth=1, n_estimators=120; total time=   0.3s\n[CV] END ......................max_depth=1, n_estimators=140; total time=   0.3s\n[CV] END ......................max_depth=1, n_estimators=160; total time=   0.4s\n[CV] END ......................max_depth=1, n_estimators=160; total time=   0.4s\n[CV] END ......................max_depth=1, n_estimators=180; total time=   0.5s\n[CV] END ......................max_depth=1, n_estimators=200; total time=   0.4s\n[CV] END ......................max_depth=2, n_estimators=100; total time=   0.2s\n[CV] END ......................max_depth=2, n_estimators=100; total time=   0.2s\n[CV] END ......................max_depth=2, n_estimators=120; total time=   0.3s\n[CV] END ......................max_depth=2, n_estimators=140; total time=   0.3s\n[CV] END ......................max_depth=2, n_estimators=160; total time=   0.4s\n[CV] END ......................max_depth=2, n_estimators=160; total time=   0.4s\n[CV] END ......................max_depth=2, n_estimators=180; total time=   0.4s\n[CV] END ......................max_depth=2, n_estimators=200; total time=   0.5s\n[CV] END ......................max_depth=3, n_estimators=100; total time=   0.2s\n[CV] END ......................max_depth=3, n_estimators=100; total time=   0.2s\n[CV] END ......................max_depth=3, n_estimators=120; total time=   0.3s\n[CV] END ......................max_depth=3, n_estimators=140; total time=   0.4s\n[CV] END ......................max_depth=3, n_estimators=160; total time=   0.4s\n[CV] END ......................max_depth=3, n_estimators=180; total time=   0.5s\n[CV] END ......................max_depth=3, n_estimators=180; total time=   0.4s\n[CV] END ......................max_depth=3, n_estimators=200; total time=   0.5s\n[CV] END ......................max_depth=4, n_estimators=100; total time=   0.2s\n[CV] END ......................max_depth=4, n_estimators=120; total time=   0.3s\n[CV] END ......................max_depth=4, n_estimators=140; total time=   0.3s\n[CV] END ......................max_depth=4, n_estimators=140; total time=   0.3s\n[CV] END ......................max_depth=4, n_estimators=160; total time=   0.4s\n[CV] END ......................max_depth=4, n_estimators=180; total time=   0.4s\n[CV] END ......................max_depth=4, n_estimators=200; total time=   0.5s\n[CV] END ......................max_depth=5, n_estimators=100; total time=   0.2s\n[CV] END ......................max_depth=5, n_estimators=100; total time=   0.2s\n[CV] END ......................max_depth=5, n_estimators=120; total time=   0.3s\n[CV] END ......................max_depth=5, n_estimators=140; total time=   0.3s\n[CV] END ......................max_depth=5, n_estimators=160; total time=   0.4s\n[CV] END ......................max_depth=5, n_estimators=160; total time=   0.4s\n[CV] END ......................max_depth=5, n_estimators=180; total time=   0.4s\n[CV] END ......................max_depth=5, n_estimators=200; total time=   0.5s\n[CV] END ......................max_depth=6, n_estimators=100; total time=   0.2s\n[CV] END ......................max_depth=6, n_estimators=120; total time=   0.3s\n[CV] END ......................max_depth=6, n_estimators=120; total time=   0.3s\n[CV] END ......................max_depth=6, n_estimators=140; total time=   0.3s\n[CV] END ......................max_depth=6, n_estimators=160; total time=   0.4s\n[CV] END ......................max_depth=6, n_estimators=180; total time=   0.4s\n[CV] END ......................max_depth=6, n_estimators=200; total time=   0.5s\n[CV] END ......................max_depth=6, n_estimators=200; total time=   0.5s\n[CV] END ......................max_depth=7, n_estimators=100; total time=   0.2s\n[CV] END ......................max_depth=7, n_estimators=120; total time=   0.3s\n[CV] END ......................max_depth=7, n_estimators=140; total time=   0.3s\n[CV] END ......................max_depth=7, n_estimators=160; total time=   0.4s\n[CV] END ......................max_depth=7, n_estimators=180; total time=   0.4s\n[CV] END ......................max_depth=7, n_estimators=180; total time=   0.4s\n[CV] END ......................max_depth=7, n_estimators=200; total time=   0.5s\n[CV] END ......................max_depth=8, n_estimators=100; total time=   0.2s\n[CV] END ......................max_depth=8, n_estimators=120; total time=   0.3s\n[CV] END ......................max_depth=8, n_estimators=140; total time=   0.3s\n[CV] END ......................max_depth=8, n_estimators=140; total time=   0.3s\n[CV] END ......................max_depth=8, n_estimators=160; total time=   0.4s\n[CV] END ......................max_depth=8, n_estimators=180; total time=   0.4s\n[CV] END ......................max_depth=8, n_estimators=200; total time=   0.5s\n[CV] END ......................max_depth=9, n_estimators=100; total time=   0.2s\n[CV] END ......................max_depth=9, n_estimators=100; total time=   0.2s\n[CV] END ......................max_depth=9, n_estimators=120; total time=   0.3s\n[CV] END ......................max_depth=9, n_estimators=140; total time=   0.3s\n[CV] END ......................max_depth=9, n_estimators=160; total time=   0.4s\n[CV] END ......................max_depth=9, n_estimators=160; total time=   0.4s\n[CV] END ......................max_depth=9, n_estimators=180; total time=   0.4s\n[CV] END ......................max_depth=9, n_estimators=200; total time=   0.5s\n[CV] END .....................max_depth=10, n_estimators=100; total time=   0.2s\n[CV] END .....................max_depth=10, n_estimators=100; total time=   0.2s\n[CV] END .....................max_depth=10, n_estimators=120; total time=   0.3s\n[CV] END .....................max_depth=10, n_estimators=140; total time=   0.3s\n[CV] END .....................max_depth=10, n_estimators=160; total time=   0.4s\n[CV] END .....................max_depth=10, n_estimators=180; total time=   0.4s\n[CV] END .....................max_depth=10, n_estimators=180; total time=   0.4s\n[CV] END .....................max_depth=10, n_estimators=200; total time=   0.5s\n[CV] END .....................max_depth=11, n_estimators=100; total time=   0.2s\n[CV] END .....................max_depth=11, n_estimators=120; total time=   0.3s\n[CV] END .....................max_depth=11, n_estimators=140; total time=   0.3s\n[CV] END .....................max_depth=11, n_estimators=140; total time=   0.3s\n[CV] END .....................max_depth=11, n_estimators=160; total time=   0.4s\n[CV] END .....................max_depth=11, n_estimators=180; total time=   0.4s\n[CV] END .....................max_depth=11, n_estimators=200; total time=   0.5s\n[CV] END .....................max_depth=12, n_estimators=100; total time=   0.2s\n[CV] END .....................max_depth=12, n_estimators=100; total time=   0.2s\n[CV] END .....................max_depth=12, n_estimators=120; total time=   0.3s\n[CV] END .....................max_depth=12, n_estimators=140; total time=   0.3s\n[CV] END .....................max_depth=12, n_estimators=160; total time=   0.4s\n[CV] END .....................max_depth=12, n_estimators=160; total time=   0.4s\n[CV] END .....................max_depth=12, n_estimators=180; total time=   0.4s\n[CV] END .....................max_depth=12, n_estimators=200; total time=   0.5s\n[CV] END .....................max_depth=13, n_estimators=100; total time=   0.2s\n[CV] END .....................max_depth=13, n_estimators=100; total time=   0.2s\n[CV] END .....................max_depth=13, n_estimators=120; total time=   0.3s\n[CV] END .....................max_depth=13, n_estimators=140; total time=   0.3s\n[CV] END .....................max_depth=13, n_estimators=160; total time=   0.4s\n[CV] END .....................max_depth=13, n_estimators=180; total time=   0.4s\n[CV] END .....................max_depth=13, n_estimators=180; total time=   0.4s\n[CV] END .....................max_depth=13, n_estimators=200; total time=   0.5s\n[CV] END .....................max_depth=14, n_estimators=100; total time=   0.2s\n[CV] END .....................max_depth=14, n_estimators=120; total time=   0.3s\n[CV] END .....................max_depth=14, n_estimators=140; total time=   0.3s[CV] END ......................max_depth=1, n_estimators=100; total time=   0.2s\n[CV] END ......................max_depth=1, n_estimators=120; total time=   0.3s\n[CV] END ......................max_depth=1, n_estimators=140; total time=   0.3s\n[CV] END ......................max_depth=1, n_estimators=140; total time=   0.3s\n[CV] END ......................max_depth=1, n_estimators=160; total time=   0.4s\n[CV] END ......................max_depth=1, n_estimators=180; total time=   0.4s\n[CV] END ......................max_depth=1, n_estimators=200; total time=   0.5s\n[CV] END ......................max_depth=1, n_estimators=200; total time=   0.5s\n[CV] END ......................max_depth=2, n_estimators=100; total time=   0.2s\n[CV] END ......................max_depth=2, n_estimators=120; total time=   0.3s\n[CV] END ......................max_depth=2, n_estimators=140; total time=   0.3s\n[CV] END ......................max_depth=2, n_estimators=160; total time=   0.4s\n[CV] END ......................max_depth=2, n_estimators=180; total time=   0.4s\n[CV] END ......................max_depth=2, n_estimators=180; total time=   0.4s\n[CV] END ......................max_depth=2, n_estimators=200; total time=   0.5s\n[CV] END ......................max_depth=3, n_estimators=100; total time=   0.2s\n[CV] END ......................max_depth=3, n_estimators=120; total time=   0.3s\n[CV] END ......................max_depth=3, n_estimators=140; total time=   0.4s\n[CV] END ......................max_depth=3, n_estimators=140; total time=   0.4s\n[CV] END ......................max_depth=3, n_estimators=160; total time=   0.4s\n[CV] END ......................max_depth=3, n_estimators=180; total time=   0.4s\n[CV] END ......................max_depth=3, n_estimators=200; total time=   0.5s\n[CV] END ......................max_depth=4, n_estimators=100; total time=   0.2s\n[CV] END ......................max_depth=4, n_estimators=100; total time=   0.2s\n[CV] END ......................max_depth=4, n_estimators=120; total time=   0.3s\n[CV] END ......................max_depth=4, n_estimators=140; total time=   0.3s\n[CV] END ......................max_depth=4, n_estimators=160; total time=   0.4s\n[CV] END ......................max_depth=4, n_estimators=160; total time=   0.4s\n[CV] END ......................max_depth=4, n_estimators=180; total time=   0.4s\n[CV] END ......................max_depth=4, n_estimators=200; total time=   0.5s\n[CV] END ......................max_depth=5, n_estimators=100; total time=   0.2s\n[CV] END ......................max_depth=5, n_estimators=120; total time=   0.3s\n[CV] END ......................max_depth=5, n_estimators=120; total time=   0.3s\n[CV] END ......................max_depth=5, n_estimators=140; total time=   0.3s\n[CV] END ......................max_depth=5, n_estimators=160; total time=   0.4s\n[CV] END ......................max_depth=5, n_estimators=180; total time=   0.4s\n[CV] END ......................max_depth=5, n_estimators=200; total time=   0.5s\n[CV] END ......................max_depth=5, n_estimators=200; total time=   0.5s\n[CV] END ......................max_depth=6, n_estimators=100; total time=   0.2s\n[CV] END ......................max_depth=6, n_estimators=120; total time=   0.3s\n[CV] END ......................max_depth=6, n_estimators=140; total time=   0.3s\n[CV] END ......................max_depth=6, n_estimators=160; total time=   0.4s\n[CV] END ......................max_depth=6, n_estimators=180; total time=   0.4s\n[CV] END ......................max_depth=6, n_estimators=180; total time=   0.4s\n[CV] END ......................max_depth=6, n_estimators=200; total time=   0.5s\n[CV] END ......................max_depth=7, n_estimators=100; total time=   0.3s\n[CV] END ......................max_depth=7, n_estimators=120; total time=   0.3s\n[CV] END ......................max_depth=7, n_estimators=140; total time=   0.3s\n[CV] END ......................max_depth=7, n_estimators=140; total time=   0.3s\n[CV] END ......................max_depth=7, n_estimators=160; total time=   0.4s\n[CV] END ......................max_depth=7, n_estimators=180; total time=   0.4s\n[CV] END ......................max_depth=7, n_estimators=200; total time=   0.5s\n[CV] END ......................max_depth=8, n_estimators=100; total time=   0.2s\n[CV] END ......................max_depth=8, n_estimators=100; total time=   0.2s\n[CV] END ......................max_depth=8, n_estimators=120; total time=   0.3s\n[CV] END ......................max_depth=8, n_estimators=140; total time=   0.3s\n[CV] END ......................max_depth=8, n_estimators=160; total time=   0.4s\n[CV] END ......................max_depth=8, n_estimators=160; total time=   0.4s\n[CV] END ......................max_depth=8, n_estimators=180; total time=   0.4s\n[CV] END ......................max_depth=8, n_estimators=200; total time=   0.5s\n[CV] END ......................max_depth=9, n_estimators=100; total time=   0.2s\n[CV] END ......................max_depth=9, n_estimators=100; total time=   0.2s\n[CV] END ......................max_depth=9, n_estimators=120; total time=   0.3s\n[CV] END ......................max_depth=9, n_estimators=140; total time=   0.3s\n[CV] END ......................max_depth=9, n_estimators=160; total time=   0.4s\n[CV] END ......................max_depth=9, n_estimators=180; total time=   0.4s\n[CV] END ......................max_depth=9, n_estimators=180; total time=   0.4s\n[CV] END ......................max_depth=9, n_estimators=200; total time=   0.5s\n[CV] END .....................max_depth=10, n_estimators=100; total time=   0.2s\n[CV] END .....................max_depth=10, n_estimators=120; total time=   0.3s\n[CV] END .....................max_depth=10, n_estimators=140; total time=   0.3s\n[CV] END .....................max_depth=10, n_estimators=140; total time=   0.3s\n[CV] END .....................max_depth=10, n_estimators=160; total time=   0.4s\n[CV] END .....................max_depth=10, n_estimators=180; total time=   0.4s\n[CV] END .....................max_depth=10, n_estimators=200; total time=   0.5s\n[CV] END .....................max_depth=11, n_estimators=100; total time=   0.2s\n[CV] END .....................max_depth=11, n_estimators=100; total time=   0.2s\n[CV] END .....................max_depth=11, n_estimators=120; total time=   0.3s\n[CV] END .....................max_depth=11, n_estimators=140; total time=   0.3s\n[CV] END .....................max_depth=11, n_estimators=160; total time=   0.4s\n[CV] END .....................max_depth=11, n_estimators=160; total time=   0.4s\n[CV] END .....................max_depth=11, n_estimators=180; total time=   0.4s\n[CV] END .....................max_depth=11, n_estimators=200; total time=   0.5s\n[CV] END .....................max_depth=12, n_estimators=100; total time=   0.2s\n[CV] END .....................max_depth=12, n_estimators=100; total time=   0.2s\n[CV] END .....................max_depth=12, n_estimators=120; total time=   0.3s\n[CV] END .....................max_depth=12, n_estimators=140; total time=   0.3s\n[CV] END .....................max_depth=12, n_estimators=160; total time=   0.4s\n[CV] END .....................max_depth=12, n_estimators=180; total time=   0.4s\n[CV] END .....................max_depth=12, n_estimators=180; total time=   0.4s\n[CV] END .....................max_depth=12, n_estimators=200; total time=   0.5s\n[CV] END .....................max_depth=13, n_estimators=100; total time=   0.2s\n[CV] END .....................max_depth=13, n_estimators=120; total time=   0.3s\n[CV] END .....................max_depth=13, n_estimators=140; total time=   0.3s\n[CV] END .....................max_depth=13, n_estimators=140; total time=   0.3s\n[CV] END .....................max_depth=13, n_estimators=160; total time=   0.4s\n[CV] END .....................max_depth=13, n_estimators=180; total time=   0.4s\n[CV] END .....................max_depth=13, n_estimators=200; total time=   0.5s\n[CV] END .....................max_depth=14, n_estimators=100; total time=   0.2s\n[CV] END .....................max_depth=14, n_estimators=100; total time=   0.2s\n[CV] END .....................max_depth=14, n_estimators=120; total time=   0.3s\n[CV] END .....................max_depth=14, n_estimators=140; total time=   0.3s[CV] END ......................max_depth=1, n_estimators=100; total time=   0.2s\n[CV] END ......................max_depth=1, n_estimators=100; total time=   0.2s\n[CV] END ......................max_depth=1, n_estimators=120; total time=   0.3s\n[CV] END ......................max_depth=1, n_estimators=140; total time=   0.3s\n[CV] END ......................max_depth=1, n_estimators=160; total time=   0.4s\n[CV] END ......................max_depth=1, n_estimators=180; total time=   0.4s\n[CV] END ......................max_depth=1, n_estimators=180; total time=   0.5s\n[CV] END ......................max_depth=1, n_estimators=200; total time=   0.5s\n[CV] END ......................max_depth=2, n_estimators=120; total time=   0.3s\n[CV] END ......................max_depth=2, n_estimators=120; total time=   0.3s\n[CV] END ......................max_depth=2, n_estimators=140; total time=   0.3s\n[CV] END ......................max_depth=2, n_estimators=160; total time=   0.4s\n[CV] END ......................max_depth=2, n_estimators=180; total time=   0.4s\n[CV] END ......................max_depth=2, n_estimators=200; total time=   0.5s\n[CV] END ......................max_depth=2, n_estimators=200; total time=   0.5s\n[CV] END ......................max_depth=3, n_estimators=120; total time=   0.3s\n[CV] END ......................max_depth=3, n_estimators=120; total time=   0.3s\n[CV] END ......................max_depth=3, n_estimators=140; total time=   0.4s\n[CV] END ......................max_depth=3, n_estimators=160; total time=   0.4s\n[CV] END ......................max_depth=3, n_estimators=180; total time=   0.4s\n[CV] END ......................max_depth=3, n_estimators=200; total time=   0.5s\n[CV] END ......................max_depth=3, n_estimators=200; total time=   0.5s\n[CV] END ......................max_depth=4, n_estimators=100; total time=   0.2s\n[CV] END ......................max_depth=4, n_estimators=120; total time=   0.3s\n[CV] END ......................max_depth=4, n_estimators=140; total time=   0.3s\n[CV] END ......................max_depth=4, n_estimators=160; total time=   0.4s\n[CV] END ......................max_depth=4, n_estimators=180; total time=   0.4s\n[CV] END ......................max_depth=4, n_estimators=180; total time=   0.4s\n[CV] END ......................max_depth=4, n_estimators=200; total time=   0.5s\n[CV] END ......................max_depth=5, n_estimators=100; total time=   0.2s\n[CV] END ......................max_depth=5, n_estimators=120; total time=   0.3s\n[CV] END ......................max_depth=5, n_estimators=140; total time=   0.3s\n[CV] END ......................max_depth=5, n_estimators=140; total time=   0.3s\n[CV] END ......................max_depth=5, n_estimators=160; total time=   0.4s\n[CV] END ......................max_depth=5, n_estimators=180; total time=   0.4s\n[CV] END ......................max_depth=5, n_estimators=200; total time=   0.5s\n[CV] END ......................max_depth=6, n_estimators=100; total time=   0.2s\n[CV] END ......................max_depth=6, n_estimators=100; total time=   0.2s\n[CV] END ......................max_depth=6, n_estimators=120; total time=   0.3s\n[CV] END ......................max_depth=6, n_estimators=140; total time=   0.3s\n[CV] END ......................max_depth=6, n_estimators=160; total time=   0.4s\n[CV] END ......................max_depth=6, n_estimators=160; total time=   0.4s\n[CV] END ......................max_depth=6, n_estimators=180; total time=   0.4s\n[CV] END ......................max_depth=6, n_estimators=200; total time=   0.5s\n[CV] END ......................max_depth=7, n_estimators=100; total time=   0.3s\n[CV] END ......................max_depth=7, n_estimators=120; total time=   0.3s\n[CV] END ......................max_depth=7, n_estimators=120; total time=   0.3s\n[CV] END ......................max_depth=7, n_estimators=140; total time=   0.3s\n[CV] END ......................max_depth=7, n_estimators=160; total time=   0.4s\n[CV] END ......................max_depth=7, n_estimators=180; total time=   0.4s\n[CV] END ......................max_depth=7, n_estimators=200; total time=   0.5s\n[CV] END ......................max_depth=7, n_estimators=200; total time=   0.5s\n[CV] END ......................max_depth=8, n_estimators=100; total time=   0.2s\n[CV] END ......................max_depth=8, n_estimators=120; total time=   0.3s\n[CV] END ......................max_depth=8, n_estimators=140; total time=   0.3s\n[CV] END ......................max_depth=8, n_estimators=160; total time=   0.4s\n[CV] END ......................max_depth=8, n_estimators=180; total time=   0.4s\n[CV] END ......................max_depth=8, n_estimators=180; total time=   0.4s\n[CV] END ......................max_depth=8, n_estimators=200; total time=   0.5s\n[CV] END ......................max_depth=9, n_estimators=100; total time=   0.2s\n[CV] END ......................max_depth=9, n_estimators=120; total time=   0.3s\n[CV] END ......................max_depth=9, n_estimators=140; total time=   0.3s\n[CV] END ......................max_depth=9, n_estimators=140; total time=   0.3s\n[CV] END ......................max_depth=9, n_estimators=160; total time=   0.4s\n[CV] END ......................max_depth=9, n_estimators=180; total time=   0.4s\n[CV] END ......................max_depth=9, n_estimators=200; total time=   0.5s\n[CV] END .....................max_depth=10, n_estimators=100; total time=   0.2s\n[CV] END .....................max_depth=10, n_estimators=100; total time=   0.2s\n[CV] END .....................max_depth=10, n_estimators=120; total time=   0.3s\n[CV] END .....................max_depth=10, n_estimators=140; total time=   0.3s\n[CV] END .....................max_depth=10, n_estimators=160; total time=   0.4s\n[CV] END .....................max_depth=10, n_estimators=160; total time=   0.4s\n[CV] END .....................max_depth=10, n_estimators=180; total time=   0.4s\n[CV] END .....................max_depth=10, n_estimators=200; total time=   0.5s\n[CV] END .....................max_depth=11, n_estimators=100; total time=   0.2s\n[CV] END .....................max_depth=11, n_estimators=100; total time=   0.2s\n[CV] END .....................max_depth=11, n_estimators=120; total time=   0.3s\n[CV] END .....................max_depth=11, n_estimators=140; total time=   0.3s\n[CV] END .....................max_depth=11, n_estimators=160; total time=   0.4s\n[CV] END .....................max_depth=11, n_estimators=180; total time=   0.4s\n[CV] END .....................max_depth=11, n_estimators=180; total time=   0.4s\n[CV] END .....................max_depth=11, n_estimators=200; total time=   0.5s\n[CV] END .....................max_depth=12, n_estimators=100; total time=   0.2s\n[CV] END .....................max_depth=12, n_estimators=120; total time=   0.3s\n[CV] END .....................max_depth=12, n_estimators=140; total time=   0.3s\n[CV] END .....................max_depth=12, n_estimators=140; total time=   0.3s\n[CV] END .....................max_depth=12, n_estimators=160; total time=   0.4s\n[CV] END .....................max_depth=12, n_estimators=180; total time=   0.4s\n[CV] END .....................max_depth=12, n_estimators=200; total time=   0.5s\n[CV] END .....................max_depth=13, n_estimators=100; total time=   0.2s\n[CV] END .....................max_depth=13, n_estimators=100; total time=   0.2s\n[CV] END .....................max_depth=13, n_estimators=120; total time=   0.3s\n[CV] END .....................max_depth=13, n_estimators=140; total time=   0.3s\n[CV] END .....................max_depth=13, n_estimators=160; total time=   0.4s\n[CV] END .....................max_depth=13, n_estimators=160; total time=   0.4s\n[CV] END .....................max_depth=13, n_estimators=180; total time=   0.4s\n[CV] END .....................max_depth=13, n_estimators=200; total time=   0.5s\n[CV] END .....................max_depth=14, n_estimators=100; total time=   0.2s\n[CV] END .....................max_depth=14, n_estimators=120; total time=   0.3s\n[CV] END .....................max_depth=14, n_estimators=120; total time=   0.3s\n[CV] END .....................max_depth=14, n_estimators=140; total time=   0.3s\n[CV] END .....................max_depth=14, n_estimators=160; total time=   0.4s[CV] END ......................max_depth=1, n_estimators=100; total time=   0.2s\n[CV] END ......................max_depth=1, n_estimators=120; total time=   0.3s\n[CV] END ......................max_depth=1, n_estimators=120; total time=   0.3s\n[CV] END ......................max_depth=1, n_estimators=140; total time=   0.3s\n[CV] END ......................max_depth=1, n_estimators=160; total time=   0.4s\n[CV] END ......................max_depth=1, n_estimators=180; total time=   0.4s\n[CV] END ......................max_depth=1, n_estimators=200; total time=   0.6s\n[CV] END ......................max_depth=2, n_estimators=100; total time=   0.2s\n[CV] END ......................max_depth=2, n_estimators=100; total time=   0.2s\n[CV] END ......................max_depth=2, n_estimators=120; total time=   0.3s\n[CV] END ......................max_depth=2, n_estimators=140; total time=   0.3s\n[CV] END ......................max_depth=2, n_estimators=140; total time=   0.3s\n[CV] END ......................max_depth=2, n_estimators=160; total time=   0.4s\n[CV] END ......................max_depth=2, n_estimators=180; total time=   0.4s\n[CV] END ......................max_depth=2, n_estimators=200; total time=   0.5s\n[CV] END ......................max_depth=3, n_estimators=100; total time=   0.2s\n[CV] END ......................max_depth=3, n_estimators=100; total time=   0.2s\n[CV] END ......................max_depth=3, n_estimators=120; total time=   0.3s\n[CV] END ......................max_depth=3, n_estimators=140; total time=   0.4s\n[CV] END ......................max_depth=3, n_estimators=160; total time=   0.5s\n[CV] END ......................max_depth=3, n_estimators=160; total time=   0.4s\n[CV] END ......................max_depth=3, n_estimators=180; total time=   0.4s\n[CV] END ......................max_depth=3, n_estimators=200; total time=   0.5s\n[CV] END ......................max_depth=4, n_estimators=100; total time=   0.2s\n[CV] END ......................max_depth=4, n_estimators=120; total time=   0.3s\n[CV] END ......................max_depth=4, n_estimators=120; total time=   0.3s\n[CV] END ......................max_depth=4, n_estimators=140; total time=   0.3s\n[CV] END ......................max_depth=4, n_estimators=160; total time=   0.4s\n[CV] END ......................max_depth=4, n_estimators=180; total time=   0.4s\n[CV] END ......................max_depth=4, n_estimators=200; total time=   0.5s\n[CV] END ......................max_depth=4, n_estimators=200; total time=   0.5s\n[CV] END ......................max_depth=5, n_estimators=100; total time=   0.2s\n[CV] END ......................max_depth=5, n_estimators=120; total time=   0.3s\n[CV] END ......................max_depth=5, n_estimators=140; total time=   0.3s\n[CV] END ......................max_depth=5, n_estimators=160; total time=   0.4s\n[CV] END ......................max_depth=5, n_estimators=180; total time=   0.4s\n[CV] END ......................max_depth=5, n_estimators=180; total time=   0.4s\n[CV] END ......................max_depth=5, n_estimators=200; total time=   0.5s\n[CV] END ......................max_depth=6, n_estimators=100; total time=   0.2s\n[CV] END ......................max_depth=6, n_estimators=120; total time=   0.3s\n[CV] END ......................max_depth=6, n_estimators=140; total time=   0.3s\n[CV] END ......................max_depth=6, n_estimators=140; total time=   0.3s\n[CV] END ......................max_depth=6, n_estimators=160; total time=   0.4s\n[CV] END ......................max_depth=6, n_estimators=180; total time=   0.4s\n[CV] END ......................max_depth=6, n_estimators=200; total time=   0.5s\n[CV] END ......................max_depth=7, n_estimators=100; total time=   0.2s\n[CV] END ......................max_depth=7, n_estimators=100; total time=   0.3s\n[CV] END ......................max_depth=7, n_estimators=120; total time=   0.3s\n[CV] END ......................max_depth=7, n_estimators=140; total time=   0.3s\n[CV] END ......................max_depth=7, n_estimators=160; total time=   0.4s\n[CV] END ......................max_depth=7, n_estimators=160; total time=   0.4s\n[CV] END ......................max_depth=7, n_estimators=180; total time=   0.4s\n[CV] END ......................max_depth=7, n_estimators=200; total time=   0.5s\n[CV] END ......................max_depth=8, n_estimators=100; total time=   0.2s\n[CV] END ......................max_depth=8, n_estimators=120; total time=   0.3s\n[CV] END ......................max_depth=8, n_estimators=120; total time=   0.3s\n[CV] END ......................max_depth=8, n_estimators=140; total time=   0.3s\n[CV] END ......................max_depth=8, n_estimators=160; total time=   0.4s\n[CV] END ......................max_depth=8, n_estimators=180; total time=   0.4s\n[CV] END ......................max_depth=8, n_estimators=200; total time=   0.5s\n[CV] END ......................max_depth=8, n_estimators=200; total time=   0.5s\n[CV] END ......................max_depth=9, n_estimators=120; total time=   0.3s\n[CV] END ......................max_depth=9, n_estimators=120; total time=   0.3s\n[CV] END ......................max_depth=9, n_estimators=140; total time=   0.3s\n[CV] END ......................max_depth=9, n_estimators=160; total time=   0.4s\n[CV] END ......................max_depth=9, n_estimators=180; total time=   0.4s\n[CV] END ......................max_depth=9, n_estimators=200; total time=   0.5s\n[CV] END ......................max_depth=9, n_estimators=200; total time=   0.5s\n[CV] END .....................max_depth=10, n_estimators=120; total time=   0.3s\n[CV] END .....................max_depth=10, n_estimators=120; total time=   0.3s\n[CV] END .....................max_depth=10, n_estimators=140; total time=   0.3s\n[CV] END .....................max_depth=10, n_estimators=160; total time=   0.4s\n[CV] END .....................max_depth=10, n_estimators=180; total time=   0.4s\n[CV] END .....................max_depth=10, n_estimators=200; total time=   0.5s\n[CV] END .....................max_depth=10, n_estimators=200; total time=   0.5s\n[CV] END .....................max_depth=11, n_estimators=120; total time=   0.3s\n[CV] END .....................max_depth=11, n_estimators=120; total time=   0.3s\n[CV] END .....................max_depth=11, n_estimators=140; total time=   0.3s\n[CV] END .....................max_depth=11, n_estimators=160; total time=   0.4s\n[CV] END .....................max_depth=11, n_estimators=180; total time=   0.4s\n[CV] END .....................max_depth=11, n_estimators=200; total time=   0.5s\n[CV] END .....................max_depth=11, n_estimators=200; total time=   0.5s\n[CV] END .....................max_depth=12, n_estimators=120; total time=   0.3s\n[CV] END .....................max_depth=12, n_estimators=120; total time=   0.3s\n[CV] END .....................max_depth=12, n_estimators=140; total time=   0.3s\n[CV] END .....................max_depth=12, n_estimators=160; total time=   0.4s\n[CV] END .....................max_depth=12, n_estimators=180; total time=   0.4s\n[CV] END .....................max_depth=12, n_estimators=200; total time=   0.5s\n[CV] END .....................max_depth=12, n_estimators=200; total time=   0.5s\n[CV] END .....................max_depth=13, n_estimators=120; total time=   0.3s\n[CV] END .....................max_depth=13, n_estimators=120; total time=   0.3s\n[CV] END .....................max_depth=13, n_estimators=140; total time=   0.3s\n[CV] END .....................max_depth=13, n_estimators=160; total time=   0.4s\n[CV] END .....................max_depth=13, n_estimators=180; total time=   0.4s\n[CV] END .....................max_depth=13, n_estimators=200; total time=   0.5s\n[CV] END .....................max_depth=13, n_estimators=200; total time=   0.5s\n[CV] END .....................max_depth=14, n_estimators=100; total time=   0.2s\n[CV] END .....................max_depth=14, n_estimators=120; total time=   0.3s\n[CV] END .....................max_depth=14, n_estimators=140; total time=   0.3s\n[CV] END .....................max_depth=14, n_estimators=160; total time=   0.4s\n[CV] END .....................max_depth=14, n_estimators=180; total time=   0.5s\n[CV] END .....................max_depth=14, n_estimators=180; total time=   0.4s","output_type":"stream"},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"GridSearchCV(cv=5, estimator=RandomForestClassifier(), n_jobs=-1,\n             param_grid={'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,\n                                       13, 14, 15, 16, 17, 18, 19, 20],\n                         'n_estimators': [100, 120, 140, 160, 180, 200]},\n             verbose=2)","text/html":"<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=RandomForestClassifier(), n_jobs=-1,\n             param_grid={&#x27;max_depth&#x27;: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,\n                                       13, 14, 15, 16, 17, 18, 19, 20],\n                         &#x27;n_estimators&#x27;: [100, 120, 140, 160, 180, 200]},\n             verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=RandomForestClassifier(), n_jobs=-1,\n             param_grid={&#x27;max_depth&#x27;: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,\n                                       13, 14, 15, 16, 17, 18, 19, 20],\n                         &#x27;n_estimators&#x27;: [100, 120, 140, 160, 180, 200]},\n             verbose=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div></div></div></div></div></div>"},"metadata":{}}],"execution_count":21},{"id":"ab863cf1","cell_type":"code","source":"grid_search.best_params_","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T16:26:56.879916Z","iopub.execute_input":"2025-08-21T16:26:56.880227Z","iopub.status.idle":"2025-08-21T16:26:56.885571Z","shell.execute_reply.started":"2025-08-21T16:26:56.880199Z","shell.execute_reply":"2025-08-21T16:26:56.884924Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"{'max_depth': 6, 'n_estimators': 200}"},"metadata":{}}],"execution_count":22},{"id":"cf16a56a","cell_type":"code","source":"best_model = grid_search.best_estimator_\ny_pred = best_model.predict(X_val)\nprint(classification_report(y_val, y_pred))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T16:26:56.886470Z","iopub.execute_input":"2025-08-21T16:26:56.886726Z","iopub.status.idle":"2025-08-21T16:26:56.920655Z","shell.execute_reply.started":"2025-08-21T16:26:56.886707Z","shell.execute_reply":"2025-08-21T16:26:56.919911Z"}},"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           1       1.00      0.83      0.91        12\n           2       0.78      1.00      0.88         7\n\n    accuracy                           0.89        19\n   macro avg       0.89      0.92      0.89        19\nweighted avg       0.92      0.89      0.90        19\n\n","output_type":"stream"}],"execution_count":23},{"id":"25a4e720","cell_type":"code","source":"df_train.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T16:26:56.921398Z","iopub.execute_input":"2025-08-21T16:26:56.921686Z","iopub.status.idle":"2025-08-21T16:26:56.928721Z","shell.execute_reply.started":"2025-08-21T16:26:56.921660Z","shell.execute_reply":"2025-08-21T16:26:56.927890Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"Index(['file_1', 'file_2', 'label', 'cleaned_file_1', 'cleaned_file_2',\n       'text'],\n      dtype='object')"},"metadata":{}}],"execution_count":24},{"id":"be9dce15","cell_type":"code","source":"y_test_pred = best_model.predict(X_test)\nsubmission = pd.DataFrame({\n    'id': [i for i in range(0, len(y_test_pred))],\n    'real_text_id': y_test_pred\n})\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T16:26:56.929790Z","iopub.execute_input":"2025-08-21T16:26:56.930064Z","iopub.status.idle":"2025-08-21T16:26:56.983198Z","shell.execute_reply.started":"2025-08-21T16:26:56.930039Z","shell.execute_reply":"2025-08-21T16:26:56.982404Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"   id  real_text_id\n0   0             2\n1   1             2\n2   2             1\n3   3             2\n4   4             2","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>real_text_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":25},{"id":"206d953c","cell_type":"markdown","source":"## XGBoot","metadata":{}},{"id":"b40a1322","cell_type":"code","source":"!pip install xgboost","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T16:26:56.985614Z","iopub.execute_input":"2025-08-21T16:26:56.985859Z","iopub.status.idle":"2025-08-21T16:27:00.560079Z","shell.execute_reply.started":"2025-08-21T16:26:56.985840Z","shell.execute_reply":"2025-08-21T16:27:00.559249Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.0.3)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.26.4)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.15.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->xgboost) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->xgboost) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->xgboost) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->xgboost) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->xgboost) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->xgboost) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->xgboost) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->xgboost) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->xgboost) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->xgboost) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->xgboost) (2024.2.0)\n","output_type":"stream"}],"execution_count":26},{"id":"c0e7646e","cell_type":"code","source":"from xgboost import XGBClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import GridSearchCV\n\nbst = XGBClassifier()\nparam_grid = {\n    'n_estimators': [i for i in range(100, 201, 20)],\n    'max_depth': [2, 4, 6],\n    'learning_rate': [0.01, 0.1, 0.2],\n    'objective': ['binary:logistic']\n}\ngrid_search = GridSearchCV(n_jobs=-1, estimator=bst, param_grid=param_grid, cv=5, verbose=2)\n\n# fit model\ny_train_norm = y_train - 1  # Normalize labels to start from 0\ngrid_search.fit(X_train, y_train_norm)\n# make predictions\npreds = grid_search.predict(X_val) + 1  # Reverse normalization\nprint(classification_report(y_val, preds))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T16:27:00.561282Z","iopub.execute_input":"2025-08-21T16:27:00.561593Z","iopub.status.idle":"2025-08-21T16:27:06.449102Z","shell.execute_reply.started":"2025-08-21T16:27:00.561565Z","shell.execute_reply":"2025-08-21T16:27:06.448301Z"}},"outputs":[{"name":"stdout","text":"Fitting 5 folds for each of 54 candidates, totalling 270 fits\n\n[CV] END .....................max_depth=14, n_estimators=160; total time=   0.4s\n[CV] END .....................max_depth=14, n_estimators=160; total time=   0.4s\n[CV] END .....................max_depth=14, n_estimators=180; total time=   0.5s\n[CV] END .....................max_depth=14, n_estimators=200; total time=   0.5s\n[CV] END .....................max_depth=15, n_estimators=100; total time=   0.2s\n[CV] END .....................max_depth=15, n_estimators=120; total time=   0.3s\n[CV] END .....................max_depth=15, n_estimators=120; total time=   0.3s\n[CV] END .....................max_depth=15, n_estimators=140; total time=   0.3s\n[CV] END .....................max_depth=15, n_estimators=160; total time=   0.4s\n[CV] END .....................max_depth=15, n_estimators=180; total time=   0.4s\n[CV] END .....................max_depth=15, n_estimators=200; total time=   0.5s\n[CV] END .....................max_depth=15, n_estimators=200; total time=   0.5s\n[CV] END .....................max_depth=16, n_estimators=100; total time=   0.2s\n[CV] END .....................max_depth=16, n_estimators=120; total time=   0.3s\n[CV] END .....................max_depth=16, n_estimators=140; total time=   0.3s\n[CV] END .....................max_depth=16, n_estimators=160; total time=   0.4s\n[CV] END .....................max_depth=16, n_estimators=180; total time=   0.4s\n[CV] END .....................max_depth=16, n_estimators=180; total time=   0.4s\n[CV] END .....................max_depth=16, n_estimators=200; total time=   0.5s\n[CV] END .....................max_depth=17, n_estimators=100; total time=   0.2s\n[CV] END .....................max_depth=17, n_estimators=120; total time=   0.3s\n[CV] END .....................max_depth=17, n_estimators=140; total time=   0.3s\n[CV] END .....................max_depth=17, n_estimators=140; total time=   0.3s\n[CV] END .....................max_depth=17, n_estimators=160; total time=   0.4s\n[CV] END .....................max_depth=17, n_estimators=180; total time=   0.4s\n[CV] END .....................max_depth=17, n_estimators=200; total time=   0.5s\n[CV] END .....................max_depth=18, n_estimators=100; total time=   0.2s\n[CV] END .....................max_depth=18, n_estimators=100; total time=   0.2s\n[CV] END .....................max_depth=18, n_estimators=120; total time=   0.3s\n[CV] END .....................max_depth=18, n_estimators=140; total time=   0.3s\n[CV] END .....................max_depth=18, n_estimators=140; total time=   0.4s\n[CV] END .....................max_depth=18, n_estimators=160; total time=   0.4s\n[CV] END .....................max_depth=18, n_estimators=180; total time=   0.4s\n[CV] END .....................max_depth=18, n_estimators=200; total time=   0.5s\n[CV] END .....................max_depth=19, n_estimators=100; total time=   0.2s\n[CV] END .....................max_depth=19, n_estimators=100; total time=   0.2s\n[CV] END .....................max_depth=19, n_estimators=120; total time=   0.3s\n[CV] END .....................max_depth=19, n_estimators=140; total time=   0.3s\n[CV] END .....................max_depth=19, n_estimators=160; total time=   0.4s\n[CV] END .....................max_depth=19, n_estimators=160; total time=   0.4s\n[CV] END .....................max_depth=19, n_estimators=180; total time=   0.4s\n[CV] END .....................max_depth=19, n_estimators=200; total time=   0.5s\n[CV] END .....................max_depth=20, n_estimators=100; total time=   0.2s\n[CV] END .....................max_depth=20, n_estimators=120; total time=   0.3s\n[CV] END .....................max_depth=20, n_estimators=120; total time=   0.3s\n[CV] END .....................max_depth=20, n_estimators=140; total time=   0.3s\n[CV] END .....................max_depth=20, n_estimators=160; total time=   0.4s\n[CV] END .....................max_depth=20, n_estimators=180; total time=   0.4s\n[CV] END .....................max_depth=20, n_estimators=200; total time=   0.5s\n[CV] END .....................max_depth=20, n_estimators=200; total time=   0.4s\n[CV] END learning_rate=0.01, max_depth=2, n_estimators=100, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=2, n_estimators=120, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=2, n_estimators=140, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=2, n_estimators=140, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=2, n_estimators=160, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=2, n_estimators=180, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=2, n_estimators=200, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=4, n_estimators=100, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=4, n_estimators=100, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=4, n_estimators=120, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=4, n_estimators=120, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=4, n_estimators=160, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=4, n_estimators=160, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=4, n_estimators=180, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=4, n_estimators=200, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=6, n_estimators=100, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=6, n_estimators=100, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=6, n_estimators=140, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=6, n_estimators=140, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=6, n_estimators=160, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=6, n_estimators=160, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=6, n_estimators=200, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=6, n_estimators=200, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=2, n_estimators=120, objective=binary:logistic; total time=   0.0s\n[CV] END learning_rate=0.1, max_depth=2, n_estimators=120, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=2, n_estimators=140, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=2, n_estimators=140, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=2, n_estimators=180, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=2, n_estimators=180, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=2, n_estimators=200, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=2, n_estimators=200, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=4, n_estimators=120, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=4, n_estimators=120, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=4, n_estimators=160, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=4, n_estimators=160, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=4, n_estimators=180, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=4, n_estimators=200, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=6, n_estimators=100, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=6, n_estimators=100, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=6, n_estimators=120, objective=binary:logistic; total time=   0.1s\n[CV] END .....................max_depth=14, n_estimators=180; total time=   0.5s\n[CV] END .....................max_depth=14, n_estimators=200; total time=   0.5s\n[CV] END .....................max_depth=14, n_estimators=200; total time=   0.5s\n[CV] END .....................max_depth=15, n_estimators=100; total time=   0.2s\n[CV] END .....................max_depth=15, n_estimators=120; total time=   0.3s\n[CV] END .....................max_depth=15, n_estimators=140; total time=   0.3s\n[CV] END .....................max_depth=15, n_estimators=160; total time=   0.4s\n[CV] END .....................max_depth=15, n_estimators=180; total time=   0.4s\n[CV] END .....................max_depth=15, n_estimators=180; total time=   0.4s\n[CV] END .....................max_depth=15, n_estimators=200; total time=   0.5s\n[CV] END .....................max_depth=16, n_estimators=100; total time=   0.2s\n[CV] END .....................max_depth=16, n_estimators=120; total time=   0.3s\n[CV] END .....................max_depth=16, n_estimators=140; total time=   0.3s\n[CV] END .....................max_depth=16, n_estimators=140; total time=   0.3s\n[CV] END .....................max_depth=16, n_estimators=160; total time=   0.4s\n[CV] END .....................max_depth=16, n_estimators=180; total time=   0.4s\n[CV] END .....................max_depth=16, n_estimators=200; total time=   0.5s\n[CV] END .....................max_depth=17, n_estimators=100; total time=   0.2s\n[CV] END .....................max_depth=17, n_estimators=100; total time=   0.2s\n[CV] END .....................max_depth=17, n_estimators=120; total time=   0.3s\n[CV] END .....................max_depth=17, n_estimators=140; total time=   0.3s\n[CV] END .....................max_depth=17, n_estimators=160; total time=   0.4s\n[CV] END .....................max_depth=17, n_estimators=160; total time=   0.4s\n[CV] END .....................max_depth=17, n_estimators=180; total time=   0.4s\n[CV] END .....................max_depth=17, n_estimators=200; total time=   0.5s\n[CV] END .....................max_depth=18, n_estimators=100; total time=   0.2s\n[CV] END .....................max_depth=18, n_estimators=100; total time=   0.2s\n[CV] END .....................max_depth=18, n_estimators=120; total time=   0.3s\n[CV] END .....................max_depth=18, n_estimators=140; total time=   0.3s\n[CV] END .....................max_depth=18, n_estimators=160; total time=   0.4s\n[CV] END .....................max_depth=18, n_estimators=180; total time=   0.4s\n[CV] END .....................max_depth=18, n_estimators=180; total time=   0.4s\n[CV] END .....................max_depth=18, n_estimators=200; total time=   0.5s\n[CV] END .....................max_depth=19, n_estimators=100; total time=   0.2s\n[CV] END .....................max_depth=19, n_estimators=120; total time=   0.3s\n[CV] END .....................max_depth=19, n_estimators=140; total time=   0.3s\n[CV] END .....................max_depth=19, n_estimators=140; total time=   0.3s\n[CV] END .....................max_depth=19, n_estimators=160; total time=   0.4s\n[CV] END .....................max_depth=19, n_estimators=180; total time=   0.4s\n[CV] END .....................max_depth=19, n_estimators=200; total time=   0.5s\n[CV] END .....................max_depth=20, n_estimators=100; total time=   0.2s\n[CV] END .....................max_depth=20, n_estimators=100; total time=   0.2s\n[CV] END .....................max_depth=20, n_estimators=120; total time=   0.3s\n[CV] END .....................max_depth=20, n_estimators=140; total time=   0.3s\n[CV] END .....................max_depth=20, n_estimators=140; total time=   0.3s\n[CV] END .....................max_depth=20, n_estimators=160; total time=   0.4s\n[CV] END .....................max_depth=20, n_estimators=180; total time=   0.4s\n[CV] END .....................max_depth=20, n_estimators=200; total time=   0.5s\n[CV] END learning_rate=0.01, max_depth=2, n_estimators=100, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=2, n_estimators=100, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=2, n_estimators=120, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=2, n_estimators=140, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=2, n_estimators=160, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=2, n_estimators=180, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=2, n_estimators=200, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=4, n_estimators=100, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=4, n_estimators=100, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=4, n_estimators=120, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=4, n_estimators=120, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=4, n_estimators=140, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=4, n_estimators=160, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=4, n_estimators=180, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=4, n_estimators=180, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=6, n_estimators=100, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=6, n_estimators=100, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=6, n_estimators=120, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=6, n_estimators=120, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=6, n_estimators=160, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=6, n_estimators=160, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=6, n_estimators=180, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=6, n_estimators=200, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=2, n_estimators=100, objective=binary:logistic; total time=   0.0s\n[CV] END learning_rate=0.1, max_depth=2, n_estimators=120, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=2, n_estimators=140, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=2, n_estimators=140, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=2, n_estimators=160, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=2, n_estimators=160, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=2, n_estimators=200, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=2, n_estimators=200, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=4, n_estimators=120, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=4, n_estimators=120, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=4, n_estimators=140, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=4, n_estimators=160, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=4, n_estimators=180, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=4, n_estimators=180, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=6, n_estimators=100, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=6, n_estimators=100, objective=binary:logistic; total time=   0.0s\n[CV] END learning_rate=0.1, max_depth=6, n_estimators=120, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=6, n_estimators=120, objective=binary:logistic; total time=   0.1s\n[CV] END .....................max_depth=14, n_estimators=140; total time=   0.3s\n[CV] END .....................max_depth=14, n_estimators=160; total time=   0.4s\n[CV] END .....................max_depth=14, n_estimators=180; total time=   0.5s\n[CV] END .....................max_depth=14, n_estimators=200; total time=   0.5s\n[CV] END .....................max_depth=15, n_estimators=100; total time=   0.2s\n[CV] END .....................max_depth=15, n_estimators=100; total time=   0.2s\n[CV] END .....................max_depth=15, n_estimators=120; total time=   0.3s\n[CV] END .....................max_depth=15, n_estimators=140; total time=   0.3s\n[CV] END .....................max_depth=15, n_estimators=160; total time=   0.4s\n[CV] END .....................max_depth=15, n_estimators=160; total time=   0.4s\n[CV] END .....................max_depth=15, n_estimators=180; total time=   0.4s\n[CV] END .....................max_depth=15, n_estimators=200; total time=   0.5s\n[CV] END .....................max_depth=16, n_estimators=100; total time=   0.2s\n[CV] END .....................max_depth=16, n_estimators=120; total time=   0.3s\n[CV] END .....................max_depth=16, n_estimators=120; total time=   0.3s\n[CV] END .....................max_depth=16, n_estimators=140; total time=   0.3s\n[CV] END .....................max_depth=16, n_estimators=160; total time=   0.4s\n[CV] END .....................max_depth=16, n_estimators=180; total time=   0.4s\n[CV] END .....................max_depth=16, n_estimators=200; total time=   0.5s\n[CV] END .....................max_depth=16, n_estimators=200; total time=   0.5s\n[CV] END .....................max_depth=17, n_estimators=100; total time=   0.2s\n[CV] END .....................max_depth=17, n_estimators=120; total time=   0.3s\n[CV] END .....................max_depth=17, n_estimators=140; total time=   0.3s\n[CV] END .....................max_depth=17, n_estimators=160; total time=   0.4s\n[CV] END .....................max_depth=17, n_estimators=180; total time=   0.4s\n[CV] END .....................max_depth=17, n_estimators=180; total time=   0.4s\n[CV] END .....................max_depth=17, n_estimators=200; total time=   0.5s\n[CV] END .....................max_depth=18, n_estimators=100; total time=   0.2s\n[CV] END .....................max_depth=18, n_estimators=120; total time=   0.3s\n[CV] END .....................max_depth=18, n_estimators=140; total time=   0.3s\n[CV] END .....................max_depth=18, n_estimators=160; total time=   0.4s\n[CV] END .....................max_depth=18, n_estimators=160; total time=   0.4s\n[CV] END .....................max_depth=18, n_estimators=180; total time=   0.4s\n[CV] END .....................max_depth=18, n_estimators=200; total time=   0.5s\n[CV] END .....................max_depth=19, n_estimators=100; total time=   0.2s\n[CV] END .....................max_depth=19, n_estimators=100; total time=   0.2s\n[CV] END .....................max_depth=19, n_estimators=120; total time=   0.3s\n[CV] END .....................max_depth=19, n_estimators=140; total time=   0.3s\n[CV] END .....................max_depth=19, n_estimators=160; total time=   0.4s\n[CV] END .....................max_depth=19, n_estimators=180; total time=   0.4s\n[CV] END .....................max_depth=19, n_estimators=180; total time=   0.4s\n[CV] END .....................max_depth=19, n_estimators=200; total time=   0.5s\n[CV] END .....................max_depth=20, n_estimators=100; total time=   0.2s\n[CV] END .....................max_depth=20, n_estimators=120; total time=   0.3s\n[CV] END .....................max_depth=20, n_estimators=140; total time=   0.3s\n[CV] END .....................max_depth=20, n_estimators=160; total time=   0.4s\n[CV] END .....................max_depth=20, n_estimators=160; total time=   0.4s\n[CV] END .....................max_depth=20, n_estimators=180; total time=   0.4s\n[CV] END .....................max_depth=20, n_estimators=200; total time=   0.4s\n[CV] END learning_rate=0.01, max_depth=2, n_estimators=100, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=2, n_estimators=120, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=2, n_estimators=120, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=2, n_estimators=140, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=2, n_estimators=160, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=2, n_estimators=180, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=2, n_estimators=180, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=2, n_estimators=200, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=2, n_estimators=200, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=4, n_estimators=140, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=4, n_estimators=140, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=4, n_estimators=160, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=4, n_estimators=160, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=4, n_estimators=200, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=4, n_estimators=200, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=6, n_estimators=100, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=6, n_estimators=120, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=6, n_estimators=140, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=6, n_estimators=140, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=6, n_estimators=180, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=6, n_estimators=180, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=6, n_estimators=200, objective=binary:logistic; total time=   0.2s\n[CV] END learning_rate=0.01, max_depth=6, n_estimators=200, objective=binary:logistic; total time=   0.2s\n[CV] END learning_rate=0.1, max_depth=2, n_estimators=140, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=2, n_estimators=160, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=2, n_estimators=180, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=2, n_estimators=180, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=4, n_estimators=100, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=4, n_estimators=100, objective=binary:logistic; total time=   0.0s\n[CV] END learning_rate=0.1, max_depth=4, n_estimators=100, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=4, n_estimators=120, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=4, n_estimators=140, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=4, n_estimators=140, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=4, n_estimators=180, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=4, n_estimators=180, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=4, n_estimators=200, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=4, n_estimators=200, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=6, n_estimators=140, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=6, n_estimators=140, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=6, n_estimators=160, objective=binary:logistic; total time=   0.1s\n[CV] END .....................max_depth=14, n_estimators=200; total time=   0.5s\n[CV] END .....................max_depth=15, n_estimators=100; total time=   0.2s\n[CV] END .....................max_depth=15, n_estimators=120; total time=   0.3s\n[CV] END .....................max_depth=15, n_estimators=140; total time=   0.3s\n[CV] END .....................max_depth=15, n_estimators=140; total time=   0.3s\n[CV] END .....................max_depth=15, n_estimators=160; total time=   0.4s\n[CV] END .....................max_depth=15, n_estimators=180; total time=   0.4s\n[CV] END .....................max_depth=15, n_estimators=200; total time=   0.5s\n[CV] END .....................max_depth=16, n_estimators=100; total time=   0.2s\n[CV] END .....................max_depth=16, n_estimators=100; total time=   0.2s\n[CV] END .....................max_depth=16, n_estimators=120; total time=   0.3s\n[CV] END .....................max_depth=16, n_estimators=140; total time=   0.3s\n[CV] END .....................max_depth=16, n_estimators=160; total time=   0.4s\n[CV] END .....................max_depth=16, n_estimators=160; total time=   0.4s\n[CV] END .....................max_depth=16, n_estimators=180; total time=   0.4s\n[CV] END .....................max_depth=16, n_estimators=200; total time=   0.5s\n[CV] END .....................max_depth=17, n_estimators=100; total time=   0.2s\n[CV] END .....................max_depth=17, n_estimators=120; total time=   0.3s\n[CV] END .....................max_depth=17, n_estimators=120; total time=   0.3s\n[CV] END .....................max_depth=17, n_estimators=140; total time=   0.3s\n[CV] END .....................max_depth=17, n_estimators=160; total time=   0.4s\n[CV] END .....................max_depth=17, n_estimators=180; total time=   0.4s\n[CV] END .....................max_depth=17, n_estimators=200; total time=   0.5s\n[CV] END .....................max_depth=17, n_estimators=200; total time=   0.5s\n[CV] END .....................max_depth=18, n_estimators=120; total time=   0.3s\n[CV] END .....................max_depth=18, n_estimators=120; total time=   0.3s\n[CV] END .....................max_depth=18, n_estimators=140; total time=   0.3s\n[CV] END .....................max_depth=18, n_estimators=160; total time=   0.4s\n[CV] END .....................max_depth=18, n_estimators=180; total time=   0.4s\n[CV] END .....................max_depth=18, n_estimators=200; total time=   0.5s\n[CV] END .....................max_depth=18, n_estimators=200; total time=   0.5s\n[CV] END .....................max_depth=19, n_estimators=120; total time=   0.3s\n[CV] END .....................max_depth=19, n_estimators=120; total time=   0.3s\n[CV] END .....................max_depth=19, n_estimators=140; total time=   0.3s\n[CV] END .....................max_depth=19, n_estimators=160; total time=   0.4s\n[CV] END .....................max_depth=19, n_estimators=180; total time=   0.4s\n[CV] END .....................max_depth=19, n_estimators=200; total time=   0.5s\n[CV] END .....................max_depth=19, n_estimators=200; total time=   0.5s\n[CV] END .....................max_depth=20, n_estimators=100; total time=   0.2s\n[CV] END .....................max_depth=20, n_estimators=120; total time=   0.3s\n[CV] END .....................max_depth=20, n_estimators=140; total time=   0.3s\n[CV] END .....................max_depth=20, n_estimators=160; total time=   0.4s\n[CV] END .....................max_depth=20, n_estimators=180; total time=   0.4s\n[CV] END .....................max_depth=20, n_estimators=180; total time=   0.4s\n[CV] END .....................max_depth=20, n_estimators=200; total time=   0.4s\n[CV] END learning_rate=0.01, max_depth=2, n_estimators=100, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=2, n_estimators=120, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=2, n_estimators=140, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=2, n_estimators=160, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=2, n_estimators=160, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=2, n_estimators=180, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=2, n_estimators=200, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=4, n_estimators=100, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=4, n_estimators=120, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=4, n_estimators=140, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=4, n_estimators=140, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=4, n_estimators=180, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=4, n_estimators=180, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=4, n_estimators=200, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=4, n_estimators=200, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=6, n_estimators=120, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=6, n_estimators=120, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=6, n_estimators=140, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=6, n_estimators=160, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=6, n_estimators=180, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.01, max_depth=6, n_estimators=180, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=2, n_estimators=100, objective=binary:logistic; total time=   0.0s\n[CV] END learning_rate=0.1, max_depth=2, n_estimators=100, objective=binary:logistic; total time=   0.0s\n[CV] END learning_rate=0.1, max_depth=2, n_estimators=100, objective=binary:logistic; total time=   0.0s\n[CV] END learning_rate=0.1, max_depth=2, n_estimators=100, objective=binary:logistic; total time=   0.0s\n[CV] END learning_rate=0.1, max_depth=2, n_estimators=120, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=2, n_estimators=120, objective=binary:logistic; total time=   0.0s\n[CV] END learning_rate=0.1, max_depth=2, n_estimators=160, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=2, n_estimators=160, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=2, n_estimators=180, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=2, n_estimators=200, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=4, n_estimators=100, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=4, n_estimators=100, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=4, n_estimators=140, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=4, n_estimators=140, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=4, n_estimators=160, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=4, n_estimators=160, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=4, n_estimators=200, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=4, n_estimators=200, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=6, n_estimators=100, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=6, n_estimators=120, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=6, n_estimators=140, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=6, n_estimators=140, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=6, n_estimators=180, objective=binary:logistic; total time=   0.1s              precision    recall  f1-score   support\n\n           1       1.00      0.83      0.91        12\n           2       0.78      1.00      0.88         7\n\n    accuracy                           0.89        19\n   macro avg       0.89      0.92      0.89        19\nweighted avg       0.92      0.89      0.90        19\n\n","output_type":"stream"}],"execution_count":27},{"id":"de99e397","cell_type":"code","source":"grid_search.best_params_","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T16:27:06.450251Z","iopub.execute_input":"2025-08-21T16:27:06.450584Z","iopub.status.idle":"2025-08-21T16:27:06.456470Z","shell.execute_reply.started":"2025-08-21T16:27:06.450558Z","shell.execute_reply":"2025-08-21T16:27:06.455701Z"}},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"{'learning_rate': 0.01,\n 'max_depth': 2,\n 'n_estimators': 120,\n 'objective': 'binary:logistic'}"},"metadata":{}}],"execution_count":28},{"id":"eeaaeb17","cell_type":"markdown","source":"## SVM","metadata":{}},{"id":"b400eacf","cell_type":"code","source":"from sklearn.svm import SVC\nsvc = SVC()\nparam_grid = {\n    'C': [0.1, 1, 10, 100],\n    'gamma': [0.01, 0.1, 1],\n    'kernel': ['linear', 'rbf', 'sigmoid', 'poly']\n}\ngrid_search = GridSearchCV(n_jobs=-1, estimator=svc, param_grid=param_grid, cv=5, verbose=2)\ngrid_search.fit(X_train, y_train_norm)\ny_pred = grid_search.predict(X_val) + 1  # Reverse normalization\n\nprint(classification_report(y_val, y_pred))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T16:27:06.457352Z","iopub.execute_input":"2025-08-21T16:27:06.458094Z","iopub.status.idle":"2025-08-21T16:27:06.781213Z","shell.execute_reply.started":"2025-08-21T16:27:06.458068Z","shell.execute_reply":"2025-08-21T16:27:06.780428Z"}},"outputs":[{"name":"stdout","text":"Fitting 5 folds for each of 48 candidates, totalling 240 fits\n              precision    recall  f1-score   support\n\n           1       1.00      0.75      0.86        12\n           2       0.70      1.00      0.82         7\n\n    accuracy                           0.84        19\n   macro avg       0.85      0.88      0.84        19\nweighted avg       0.89      0.84      0.84        19\n\n","output_type":"stream"}],"execution_count":29},{"id":"9e7acfec","cell_type":"code","source":"grid_search.best_params_","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T16:27:06.782131Z","iopub.execute_input":"2025-08-21T16:27:06.782626Z","iopub.status.idle":"2025-08-21T16:27:06.787665Z","shell.execute_reply.started":"2025-08-21T16:27:06.782599Z","shell.execute_reply":"2025-08-21T16:27:06.787024Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"{'C': 0.1, 'gamma': 0.01, 'kernel': 'linear'}"},"metadata":{}}],"execution_count":30},{"id":"48c3d798","cell_type":"markdown","source":"## Logistỉc Regression","metadata":{}},{"id":"fb805102","cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlgt = LogisticRegression()\nlgt.fit(X_train, y_train_norm)\ny_pred = lgt.predict(X_val) + 1  # Reverse normalization\n\nprint(classification_report(y_val, y_pred))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T16:27:06.788712Z","iopub.execute_input":"2025-08-21T16:27:06.788950Z","iopub.status.idle":"2025-08-21T16:27:06.856386Z","shell.execute_reply.started":"2025-08-21T16:27:06.788926Z","shell.execute_reply":"2025-08-21T16:27:06.854851Z"}},"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           1       1.00      0.83      0.91        12\n           2       0.78      1.00      0.88         7\n\n    accuracy                           0.89        19\n   macro avg       0.89      0.92      0.89        19\nweighted avg       0.92      0.89      0.90        19\n\n","output_type":"stream"}],"execution_count":31},{"id":"ca70146a","cell_type":"code","source":"y_pred_test = lgt.predict(X_test) + 1  # Reverse normalization\n\nsubmission = pd.DataFrame({\n    'id': [i for i in range(0, len(y_pred_test))],\n    'real_text_id': y_pred_test\n})\nsubmission.to_csv('submission_logistic_regression.csv', index=False)\nlen(y_pred_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T16:27:06.857014Z","iopub.execute_input":"2025-08-21T16:27:06.857283Z","iopub.status.idle":"2025-08-21T16:27:06.879905Z","shell.execute_reply.started":"2025-08-21T16:27:06.857261Z","shell.execute_reply":"2025-08-21T16:27:06.878438Z"}},"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"1068"},"metadata":{}},{"name":"stdout","text":"\n[CV] END learning_rate=0.1, max_depth=6, n_estimators=120, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=6, n_estimators=160, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=6, n_estimators=160, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=6, n_estimators=180, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=6, n_estimators=200, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.2, max_depth=2, n_estimators=100, objective=binary:logistic; total time=   0.0s\n[CV] END learning_rate=0.2, max_depth=2, n_estimators=100, objective=binary:logistic; total time=   0.0s\n[CV] END learning_rate=0.2, max_depth=2, n_estimators=120, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.2, max_depth=2, n_estimators=120, objective=binary:logistic; total time=   0.0s\n[CV] END learning_rate=0.2, max_depth=2, n_estimators=160, objective=binary:logistic; total time=   0.0s\n[CV] END learning_rate=0.2, max_depth=2, n_estimators=160, objective=binary:logistic; total time=   0.0s\n[CV] END learning_rate=0.2, max_depth=2, n_estimators=180, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.2, max_depth=2, n_estimators=200, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.2, max_depth=4, n_estimators=100, objective=binary:logistic; total time=   0.0s\n[CV] END learning_rate=0.2, max_depth=4, n_estimators=120, objective=binary:logistic; total time=   0.0s\n[CV] END learning_rate=0.2, max_depth=4, n_estimators=140, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.2, max_depth=4, n_estimators=140, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.2, max_depth=4, n_estimators=180, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.2, max_depth=4, n_estimators=180, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.2, max_depth=4, n_estimators=200, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.2, max_depth=4, n_estimators=200, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.2, max_depth=6, n_estimators=100, objective=binary:logistic; total time=   0.0s\n[CV] END learning_rate=0.2, max_depth=6, n_estimators=120, objective=binary:logistic; total time=   0.0s\n[CV] END learning_rate=0.2, max_depth=6, n_estimators=120, objective=binary:logistic; total time=   0.0s\n[CV] END learning_rate=0.2, max_depth=6, n_estimators=120, objective=binary:logistic; total time=   0.0s\n[CV] END learning_rate=0.2, max_depth=6, n_estimators=180, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.2, max_depth=6, n_estimators=180, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.2, max_depth=6, n_estimators=200, objective=binary:logistic; total time=   0.1s\n[CV] END ...................C=0.1, gamma=0.01, kernel=linear; total time=   0.0s\n[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=   0.0s\n[CV] END ..................C=0.1, gamma=0.01, kernel=sigmoid; total time=   0.0s\n[CV] END ..................C=0.1, gamma=0.01, kernel=sigmoid; total time=   0.0s\n[CV] END ....................C=0.1, gamma=0.1, kernel=linear; total time=   0.0s\n[CV] END ....................C=0.1, gamma=0.1, kernel=linear; total time=   0.0s\n[CV] END ...................C=0.1, gamma=0.1, kernel=sigmoid; total time=   0.0s\n[CV] END ...................C=0.1, gamma=0.1, kernel=sigmoid; total time=   0.0s\n[CV] END ...................C=0.1, gamma=0.1, kernel=sigmoid; total time=   0.0s\n[CV] END ......................C=0.1, gamma=0.1, kernel=poly; total time=   0.0s\n[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   0.0s\n[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   0.0s\n[CV] END .....................C=0.1, gamma=1, kernel=sigmoid; total time=   0.0s\n[CV] END .....................C=0.1, gamma=1, kernel=sigmoid; total time=   0.0s\n[CV] END .....................C=1, gamma=0.01, kernel=linear; total time=   0.0s\n[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   0.0s\n[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   0.0s\n[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   0.0s\n[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   0.0s\n[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   0.0s\n[CV] END ....................C=1, gamma=0.01, kernel=sigmoid; total time=   0.0s\n[CV] END ....................C=1, gamma=0.01, kernel=sigmoid; total time=   0.0s\n[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   0.0s\n[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   0.0s\n[CV] END .....................C=1, gamma=0.1, kernel=sigmoid; total time=   0.0s\n[CV] END .....................C=1, gamma=0.1, kernel=sigmoid; total time=   0.0s\n[CV] END .....................C=1, gamma=0.1, kernel=sigmoid; total time=   0.0s\n[CV] END .....................C=1, gamma=0.1, kernel=sigmoid; total time=   0.0s\n[CV] END .....................C=1, gamma=0.1, kernel=sigmoid; total time=   0.0s\n[CV] END ........................C=1, gamma=0.1, kernel=poly; total time=   0.0s\n[CV] END ....................C=10, gamma=0.01, kernel=linear; total time=   0.0s\n[CV] END ....................C=10, gamma=0.01, kernel=linear; total time=   0.0s\n[CV] END ....................C=10, gamma=0.01, kernel=linear; total time=   0.0s\n[CV] END ....................C=10, gamma=0.01, kernel=linear; total time=   0.0s\n[CV] END ....................C=10, gamma=0.01, kernel=linear; total time=   0.0s\n[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   0.0s\n[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   0.0s\n[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   0.0s\n[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   0.0s\n[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   0.0s\n[CV] END ...................C=10, gamma=0.01, kernel=sigmoid; total time=   0.0s\n[CV] END ...................C=10, gamma=0.01, kernel=sigmoid; total time=   0.0s\n[CV] END ...................C=10, gamma=0.01, kernel=sigmoid; total time=   0.0s\n[CV] END ...................C=10, gamma=0.01, kernel=sigmoid; total time=   0.0s\n[CV] END ...................C=10, gamma=0.01, kernel=sigmoid; total time=   0.0s\n[CV] END ......................C=10, gamma=0.01, kernel=poly; total time=   0.0s\n[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time=   0.0s\n[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time=   0.0s\n[CV] END ..................C=100, gamma=0.01, kernel=sigmoid; total time=   0.0s\n[CV] END .....................C=100, gamma=0.01, kernel=poly; total time=   0.0s\n[CV] END ....................C=100, gamma=0.1, kernel=linear; total time=   0.0s\n[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time=   0.0s\n[CV] END ...................C=100, gamma=0.1, kernel=sigmoid; total time=   0.0s\n[CV] END ......................C=100, gamma=0.1, kernel=poly; total time=   0.0s\n[CV] END ......................C=100, gamma=1, kernel=linear; total time=   0.0s\n[CV] END ......................C=100, gamma=1, kernel=linear; total time=   0.0s\n[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   0.0s\n[CV] END .....................C=100, gamma=1, kernel=sigmoid; total time=   0.0s\n[CV] END ........................C=100, gamma=1, kernel=poly; total time=   0.0s\n\n[CV] END learning_rate=0.1, max_depth=6, n_estimators=160, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=6, n_estimators=200, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=6, n_estimators=200, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.2, max_depth=2, n_estimators=100, objective=binary:logistic; total time=   0.0s\n[CV] END learning_rate=0.2, max_depth=2, n_estimators=120, objective=binary:logistic; total time=   0.0s\n[CV] END learning_rate=0.2, max_depth=2, n_estimators=140, objective=binary:logistic; total time=   0.0s\n[CV] END learning_rate=0.2, max_depth=2, n_estimators=140, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.2, max_depth=2, n_estimators=180, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.2, max_depth=2, n_estimators=180, objective=binary:logistic; total time=   0.0s\n[CV] END learning_rate=0.2, max_depth=2, n_estimators=200, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.2, max_depth=2, n_estimators=200, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.2, max_depth=4, n_estimators=120, objective=binary:logistic; total time=   0.0s\n[CV] END learning_rate=0.2, max_depth=4, n_estimators=120, objective=binary:logistic; total time=   0.0s\n[CV] END learning_rate=0.2, max_depth=4, n_estimators=140, objective=binary:logistic; total time=   0.0s\n[CV] END learning_rate=0.2, max_depth=4, n_estimators=160, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.2, max_depth=4, n_estimators=180, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.2, max_depth=4, n_estimators=180, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.2, max_depth=6, n_estimators=100, objective=binary:logistic; total time=   0.0s\n[CV] END learning_rate=0.2, max_depth=6, n_estimators=100, objective=binary:logistic; total time=   0.0s\n[CV] END learning_rate=0.2, max_depth=6, n_estimators=140, objective=binary:logistic; total time=   0.0s\n[CV] END learning_rate=0.2, max_depth=6, n_estimators=140, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.2, max_depth=6, n_estimators=140, objective=binary:logistic; total time=   0.0s\n[CV] END learning_rate=0.2, max_depth=6, n_estimators=160, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.2, max_depth=6, n_estimators=180, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.2, max_depth=6, n_estimators=200, objective=binary:logistic; total time=   0.1s\n[CV] END ...................C=0.1, gamma=0.01, kernel=linear; total time=   0.0s\n[CV] END ...................C=0.1, gamma=0.01, kernel=linear; total time=   0.0s\n[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=   0.0s\n[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=   0.0s\n[CV] END .....................C=0.1, gamma=0.01, kernel=poly; total time=   0.0s\n[CV] END .....................C=0.1, gamma=0.01, kernel=poly; total time=   0.0s\n[CV] END ....................C=0.1, gamma=0.1, kernel=linear; total time=   0.0s\n[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   0.0s\n[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   0.0s\n[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   0.0s\n[CV] END ......................C=0.1, gamma=1, kernel=linear; total time=   0.0s\n[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   0.0s\n[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   0.0s\n[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   0.0s\n[CV] END ........................C=0.1, gamma=1, kernel=poly; total time=   0.0s\n[CV] END ........................C=0.1, gamma=1, kernel=poly; total time=   0.0s\n[CV] END ........................C=0.1, gamma=1, kernel=poly; total time=   0.0s\n[CV] END ........................C=0.1, gamma=1, kernel=poly; total time=   0.0s\n[CV] END .....................C=1, gamma=0.01, kernel=linear; total time=   0.0s\n[CV] END .....................C=1, gamma=0.01, kernel=linear; total time=   0.0s\n[CV] END .....................C=1, gamma=0.01, kernel=linear; total time=   0.0s\n[CV] END .....................C=1, gamma=0.01, kernel=linear; total time=   0.0s\n[CV] END ........................C=1, gamma=1, kernel=linear; total time=   0.0s\n[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   0.0s\n[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   0.0s\n[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   0.0s\n[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   0.0s\n[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   0.0s\n[CV] END .......................C=1, gamma=1, kernel=sigmoid; total time=   0.0s\n[CV] END .......................C=1, gamma=1, kernel=sigmoid; total time=   0.0s\n[CV] END ......................C=10, gamma=0.01, kernel=poly; total time=   0.0s\n[CV] END ......................C=10, gamma=0.01, kernel=poly; total time=   0.0s\n[CV] END ......................C=10, gamma=0.01, kernel=poly; total time=   0.0s\n[CV] END ......................C=10, gamma=0.01, kernel=poly; total time=   0.0s\n[CV] END .....................C=10, gamma=0.1, kernel=linear; total time=   0.0s\n[CV] END .....................C=10, gamma=0.1, kernel=linear; total time=   0.0s\n[CV] END .....................C=10, gamma=0.1, kernel=linear; total time=   0.0s\n[CV] END .....................C=10, gamma=0.1, kernel=linear; total time=   0.0s\n[CV] END .....................C=10, gamma=0.1, kernel=linear; total time=   0.0s\n[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=   0.0s\n[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=   0.0s\n[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=   0.0s\n[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=   0.0s\n[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=   0.0s\n[CV] END ....................C=10, gamma=0.1, kernel=sigmoid; total time=   0.0s\n[CV] END ....................C=10, gamma=0.1, kernel=sigmoid; total time=   0.0s\n[CV] END ...................C=100, gamma=0.01, kernel=linear; total time=   0.0s\n[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time=   0.0s\n[CV] END ..................C=100, gamma=0.01, kernel=sigmoid; total time=   0.0s\n[CV] END ..................C=100, gamma=0.01, kernel=sigmoid; total time=   0.0s\n[CV] END .....................C=100, gamma=0.01, kernel=poly; total time=   0.0s\n[CV] END ....................C=100, gamma=0.1, kernel=linear; total time=   0.0s\n[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time=   0.0s\n[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time=   0.0s\n[CV] END ...................C=100, gamma=0.1, kernel=sigmoid; total time=   0.0s\n[CV] END ...................C=100, gamma=0.1, kernel=sigmoid; total time=   0.0s\n[CV] END ......................C=100, gamma=0.1, kernel=poly; total time=   0.0s\n[CV] END ......................C=100, gamma=1, kernel=linear; total time=   0.0s\n[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   0.0s\n[CV] END .....................C=100, gamma=1, kernel=sigmoid; total time=   0.0s\n[CV] END ........................C=100, gamma=1, kernel=poly; total time=   0.0s\n\n[CV] END learning_rate=0.1, max_depth=6, n_estimators=180, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=6, n_estimators=200, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=6, n_estimators=200, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.2, max_depth=2, n_estimators=140, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.2, max_depth=2, n_estimators=140, objective=binary:logistic; total time=   0.0s\n[CV] END learning_rate=0.2, max_depth=2, n_estimators=160, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.2, max_depth=2, n_estimators=160, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.2, max_depth=2, n_estimators=200, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.2, max_depth=2, n_estimators=200, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.2, max_depth=4, n_estimators=100, objective=binary:logistic; total time=   0.0s\n[CV] END learning_rate=0.2, max_depth=4, n_estimators=100, objective=binary:logistic; total time=   0.0s\n[CV] END learning_rate=0.2, max_depth=4, n_estimators=140, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.2, max_depth=4, n_estimators=140, objective=binary:logistic; total time=   0.0s\n[CV] END learning_rate=0.2, max_depth=4, n_estimators=160, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.2, max_depth=4, n_estimators=160, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.2, max_depth=4, n_estimators=200, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.2, max_depth=4, n_estimators=200, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.2, max_depth=6, n_estimators=120, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.2, max_depth=6, n_estimators=120, objective=binary:logistic; total time=   0.0s\n[CV] END learning_rate=0.2, max_depth=6, n_estimators=140, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.2, max_depth=6, n_estimators=140, objective=binary:logistic; total time=   0.0s\n[CV] END learning_rate=0.2, max_depth=6, n_estimators=180, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.2, max_depth=6, n_estimators=180, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.2, max_depth=6, n_estimators=200, objective=binary:logistic; total time=   0.0s\n[CV] END ...................C=0.1, gamma=0.01, kernel=linear; total time=   0.0s\n[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=   0.0s\n[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=   0.0s\n[CV] END ..................C=0.1, gamma=0.01, kernel=sigmoid; total time=   0.0s\n[CV] END .....................C=0.1, gamma=0.01, kernel=poly; total time=   0.0s\n[CV] END ....................C=0.1, gamma=0.1, kernel=linear; total time=   0.0s\n[CV] END ....................C=0.1, gamma=0.1, kernel=linear; total time=   0.0s\n[CV] END ......................C=0.1, gamma=0.1, kernel=poly; total time=   0.0s\n[CV] END ......................C=0.1, gamma=0.1, kernel=poly; total time=   0.0s\n[CV] END ......................C=0.1, gamma=0.1, kernel=poly; total time=   0.0s\n[CV] END ......................C=0.1, gamma=0.1, kernel=poly; total time=   0.0s\n[CV] END .....................C=0.1, gamma=1, kernel=sigmoid; total time=   0.0s\n[CV] END .....................C=0.1, gamma=1, kernel=sigmoid; total time=   0.0s\n[CV] END .....................C=0.1, gamma=1, kernel=sigmoid; total time=   0.0s\n[CV] END ........................C=0.1, gamma=1, kernel=poly; total time=   0.0s\n[CV] END ....................C=1, gamma=0.01, kernel=sigmoid; total time=   0.0s\n[CV] END ....................C=1, gamma=0.01, kernel=sigmoid; total time=   0.0s\n[CV] END ....................C=1, gamma=0.01, kernel=sigmoid; total time=   0.0s\n[CV] END .......................C=1, gamma=0.01, kernel=poly; total time=   0.0s\n[CV] END .......................C=1, gamma=0.01, kernel=poly; total time=   0.0s\n[CV] END .......................C=1, gamma=0.01, kernel=poly; total time=   0.0s\n[CV] END .......................C=1, gamma=0.01, kernel=poly; total time=   0.0s\n[CV] END .......................C=1, gamma=0.01, kernel=poly; total time=   0.0s\n[CV] END ........................C=1, gamma=0.1, kernel=poly; total time=   0.0s\n[CV] END ........................C=1, gamma=0.1, kernel=poly; total time=   0.0s\n[CV] END ........................C=1, gamma=0.1, kernel=poly; total time=   0.0s\n[CV] END ........................C=1, gamma=0.1, kernel=poly; total time=   0.0s\n[CV] END ........................C=1, gamma=1, kernel=linear; total time=   0.0s\n[CV] END ........................C=1, gamma=1, kernel=linear; total time=   0.0s\n[CV] END ........................C=1, gamma=1, kernel=linear; total time=   0.0s\n[CV] END ........................C=1, gamma=1, kernel=linear; total time=   0.0s\n[CV] END ....................C=10, gamma=0.1, kernel=sigmoid; total time=   0.0s\n[CV] END ....................C=10, gamma=0.1, kernel=sigmoid; total time=   0.0s\n[CV] END ....................C=10, gamma=0.1, kernel=sigmoid; total time=   0.0s\n[CV] END .......................C=10, gamma=0.1, kernel=poly; total time=   0.0s\n[CV] END .......................C=10, gamma=0.1, kernel=poly; total time=   0.0s\n[CV] END .......................C=10, gamma=0.1, kernel=poly; total time=   0.0s\n[CV] END .......................C=10, gamma=0.1, kernel=poly; total time=   0.0s\n[CV] END .......................C=10, gamma=0.1, kernel=poly; total time=   0.0s\n[CV] END .......................C=10, gamma=1, kernel=linear; total time=   0.0s\n[CV] END .......................C=10, gamma=1, kernel=linear; total time=   0.0s\n[CV] END .......................C=10, gamma=1, kernel=linear; total time=   0.0s\n[CV] END .......................C=10, gamma=1, kernel=linear; total time=   0.0s\n[CV] END .......................C=10, gamma=1, kernel=linear; total time=   0.0s\n[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=   0.0s\n[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=   0.0s\n[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=   0.0s\n[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time=   0.0s\n[CV] END ..................C=100, gamma=0.01, kernel=sigmoid; total time=   0.0s\n[CV] END .....................C=100, gamma=0.01, kernel=poly; total time=   0.0s\n[CV] END .....................C=100, gamma=0.01, kernel=poly; total time=   0.0s\n[CV] END ....................C=100, gamma=0.1, kernel=linear; total time=   0.0s\n[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time=   0.0s\n[CV] END ...................C=100, gamma=0.1, kernel=sigmoid; total time=   0.0s\n[CV] END ......................C=100, gamma=0.1, kernel=poly; total time=   0.0s\n[CV] END ......................C=100, gamma=0.1, kernel=poly; total time=   0.0s\n[CV] END ......................C=100, gamma=1, kernel=linear; total time=   0.0s\n[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   0.0s\n[CV] END .....................C=100, gamma=1, kernel=sigmoid; total time=   0.0s\n[CV] END ........................C=100, gamma=1, kernel=poly; total time=   0.0s\n[CV] END ........................C=100, gamma=1, kernel=poly; total time=   0.0s\n\n[CV] END learning_rate=0.1, max_depth=6, n_estimators=140, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=6, n_estimators=160, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=6, n_estimators=180, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.1, max_depth=6, n_estimators=180, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.2, max_depth=2, n_estimators=100, objective=binary:logistic; total time=   0.0s\n[CV] END learning_rate=0.2, max_depth=2, n_estimators=100, objective=binary:logistic; total time=   0.0s\n[CV] END learning_rate=0.2, max_depth=2, n_estimators=120, objective=binary:logistic; total time=   0.0s\n[CV] END learning_rate=0.2, max_depth=2, n_estimators=120, objective=binary:logistic; total time=   0.0s\n[CV] END learning_rate=0.2, max_depth=2, n_estimators=140, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.2, max_depth=2, n_estimators=160, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.2, max_depth=2, n_estimators=180, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.2, max_depth=2, n_estimators=180, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.2, max_depth=4, n_estimators=100, objective=binary:logistic; total time=   0.0s\n[CV] END learning_rate=0.2, max_depth=4, n_estimators=100, objective=binary:logistic; total time=   0.0s\n[CV] END learning_rate=0.2, max_depth=4, n_estimators=120, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.2, max_depth=4, n_estimators=120, objective=binary:logistic; total time=   0.0s\n[CV] END learning_rate=0.2, max_depth=4, n_estimators=160, objective=binary:logistic; total time=   0.0s\n[CV] END learning_rate=0.2, max_depth=4, n_estimators=160, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.2, max_depth=4, n_estimators=180, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.2, max_depth=4, n_estimators=200, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.2, max_depth=6, n_estimators=100, objective=binary:logistic; total time=   0.0s\n[CV] END learning_rate=0.2, max_depth=6, n_estimators=100, objective=binary:logistic; total time=   0.0s\n[CV] END learning_rate=0.2, max_depth=6, n_estimators=160, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.2, max_depth=6, n_estimators=160, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.2, max_depth=6, n_estimators=160, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.2, max_depth=6, n_estimators=160, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.2, max_depth=6, n_estimators=200, objective=binary:logistic; total time=   0.1s\n[CV] END learning_rate=0.2, max_depth=6, n_estimators=200, objective=binary:logistic; total time=   0.1s\n[CV] END ...................C=0.1, gamma=0.01, kernel=linear; total time=   0.0s\n[CV] END ..................C=0.1, gamma=0.01, kernel=sigmoid; total time=   0.0s\n[CV] END ..................C=0.1, gamma=0.01, kernel=sigmoid; total time=   0.0s\n[CV] END .....................C=0.1, gamma=0.01, kernel=poly; total time=   0.0s\n[CV] END .....................C=0.1, gamma=0.01, kernel=poly; total time=   0.0s\n[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   0.0s\n[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   0.0s\n[CV] END ...................C=0.1, gamma=0.1, kernel=sigmoid; total time=   0.0s\n[CV] END ...................C=0.1, gamma=0.1, kernel=sigmoid; total time=   0.0s\n[CV] END ......................C=0.1, gamma=1, kernel=linear; total time=   0.0s\n[CV] END ......................C=0.1, gamma=1, kernel=linear; total time=   0.0s\n[CV] END ......................C=0.1, gamma=1, kernel=linear; total time=   0.0s\n[CV] END ......................C=0.1, gamma=1, kernel=linear; total time=   0.0s\n[CV] END ......................C=1, gamma=0.1, kernel=linear; total time=   0.0s\n[CV] END ......................C=1, gamma=0.1, kernel=linear; total time=   0.0s\n[CV] END ......................C=1, gamma=0.1, kernel=linear; total time=   0.0s\n[CV] END ......................C=1, gamma=0.1, kernel=linear; total time=   0.0s\n[CV] END ......................C=1, gamma=0.1, kernel=linear; total time=   0.0s\n[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   0.0s\n[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   0.0s\n[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   0.0s\n[CV] END .......................C=1, gamma=1, kernel=sigmoid; total time=   0.0s\n[CV] END .......................C=1, gamma=1, kernel=sigmoid; total time=   0.0s\n[CV] END .......................C=1, gamma=1, kernel=sigmoid; total time=   0.0s\n[CV] END ..........................C=1, gamma=1, kernel=poly; total time=   0.0s\n[CV] END ..........................C=1, gamma=1, kernel=poly; total time=   0.0s\n[CV] END ..........................C=1, gamma=1, kernel=poly; total time=   0.0s\n[CV] END ..........................C=1, gamma=1, kernel=poly; total time=   0.0s\n[CV] END ..........................C=1, gamma=1, kernel=poly; total time=   0.0s\n[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=   0.0s\n[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=   0.0s\n[CV] END ......................C=10, gamma=1, kernel=sigmoid; total time=   0.0s\n[CV] END ......................C=10, gamma=1, kernel=sigmoid; total time=   0.0s\n[CV] END ......................C=10, gamma=1, kernel=sigmoid; total time=   0.0s\n[CV] END ......................C=10, gamma=1, kernel=sigmoid; total time=   0.0s\n[CV] END ......................C=10, gamma=1, kernel=sigmoid; total time=   0.0s\n[CV] END .........................C=10, gamma=1, kernel=poly; total time=   0.0s\n[CV] END .........................C=10, gamma=1, kernel=poly; total time=   0.0s\n[CV] END .........................C=10, gamma=1, kernel=poly; total time=   0.0s\n[CV] END .........................C=10, gamma=1, kernel=poly; total time=   0.0s\n[CV] END .........................C=10, gamma=1, kernel=poly; total time=   0.0s\n[CV] END ...................C=100, gamma=0.01, kernel=linear; total time=   0.0s\n[CV] END ...................C=100, gamma=0.01, kernel=linear; total time=   0.0s\n[CV] END ...................C=100, gamma=0.01, kernel=linear; total time=   0.0s\n[CV] END ...................C=100, gamma=0.01, kernel=linear; total time=   0.0s\n[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time=   0.0s\n[CV] END ..................C=100, gamma=0.01, kernel=sigmoid; total time=   0.0s\n[CV] END .....................C=100, gamma=0.01, kernel=poly; total time=   0.0s\n[CV] END ....................C=100, gamma=0.1, kernel=linear; total time=   0.0s\n[CV] END ....................C=100, gamma=0.1, kernel=linear; total time=   0.0s\n[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time=   0.0s\n[CV] END ...................C=100, gamma=0.1, kernel=sigmoid; total time=   0.0s\n[CV] END ......................C=100, gamma=0.1, kernel=poly; total time=   0.0s\n[CV] END ......................C=100, gamma=1, kernel=linear; total time=   0.0s\n[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   0.0s\n[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   0.0s\n[CV] END .....................C=100, gamma=1, kernel=sigmoid; total time=   0.0s\n[CV] END .....................C=100, gamma=1, kernel=sigmoid; total time=   0.0s\n[CV] END ........................C=100, gamma=1, kernel=poly; total time=   0.0s\n","output_type":"stream"}],"execution_count":32}]}