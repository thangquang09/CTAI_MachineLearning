data:
  dataset_folder: data
  train_dataset: train.csv
  val_dataset: dev.csv
  test_dataset: test.csv
  use_id: False # nếu set là True thì có sử dụng thông tin id, ngược lại thì không

tokenizer:
  padding: max_length
  max_length: 512      # Tăng cho NLI task
  truncation: True
  return_attention_mask: True

text_embedding:
  type: pretrained #có 3 loại, pretrained, tf_idf, count_vec
  add_new_token: False
  text_encoder: microsoft/deberta-v3-base  # Best for NLI
  freeze: False
  use_lora: True       # Enable LoRA để tiết kiệm memory
  lora_target_modules: ["query_proj", "value_proj", "key_proj"]  # DeBERTa modules
  lora_r: 16           # Tăng rank
  lora_alpha: 32       # Tăng alpha
  lora_dropout: 0.1
  d_features: 768
  d_model: 512
  dropout: 0.1         # Giảm dropout

attention:
  layers: 4
  heads: 16
  d_model: 512
  d_key: 64
  d_value: 64
  d_ff: 2048
  d_feature: 2048
  dropout: 0.2
  use_aoa: False

multi_encoder:
  type: co # có 3 loại, co (co-attention), cross (cross-attention), guide (guide-attention)
  d_model: 512
  layers: 4

uni_encoder:
  d_model: 512
  layers: 3            # Giảm để tránh overfitting

model:
  type_model: cnn
  intermediate_dims: 512
  hidden_dim: 128      # Tăng từ 32 → 128
  dropout: 0.1         # Giảm dropout

train:
  output_dir: checkpoint
  seed: 12345
  num_train_epochs: 50  # Giảm epochs
  patience: 7          # Tăng patience
  learning_rate: 1.0e-5 # Giảm LR cho DeBERTa
  weight_decay: 0.01
  metric_for_best_model: f1
  per_device_train_batch_size: 16  # Giảm batch size
  per_device_valid_batch_size: 16

inference:
  test_dataset: /content/data/test.csv
  batch_size: 64