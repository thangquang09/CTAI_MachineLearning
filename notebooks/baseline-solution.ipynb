{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2-uxElrLnwNy",
    "papermill": {
     "duration": 0.005901,
     "end_time": "2025-06-22T19:14:09.769223",
     "exception": false,
     "start_time": "2025-06-22T19:14:09.763322",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Baseline Solution: Fake or Real - The Impostor Hunt in Texts üîç\n",
    "\n",
    "---\n",
    "\n",
    "Here we provide the baseline solution for the *Fake or Real: The Impostor Hunt in Texts* challenge!\n",
    "In this notebook, we walk you through two **simple, interpretable, and ML-free approaches** to tackle the problem of detecting fake texts.\n",
    "\n",
    "### üí° The overview of first approach:\n",
    "\n",
    "We use the `langdetect` library to analyze each text by identifying the presence of **English vs. non-English words**. Here's the idea:\n",
    "\n",
    "1. **Detect Language**: We break the text into words and determine the language of each.\n",
    "2. **Calculate Proportion**: We then compute the percentage of English words in the entire text.\n",
    "3. **Assign Label**: The text which gets higher percentage of English words is classified as **Real** and its number is saved to the results list.\n",
    "\n",
    "---\n",
    "\n",
    "### üì¶ Getting Started: Install & Import Required Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T07:35:52.058518Z",
     "iopub.status.busy": "2025-08-15T07:35:52.058198Z",
     "iopub.status.idle": "2025-08-15T07:36:00.759271Z",
     "shell.execute_reply": "2025-08-15T07:36:00.758134Z",
     "shell.execute_reply.started": "2025-08-15T07:35:52.058497Z"
    },
    "id": "LCyK5ruYWdAp",
    "outputId": "6e27c50a-b46e-49c3-d7cb-1a7c25ad6af3",
    "papermill": {
     "duration": 8.467807,
     "end_time": "2025-06-22T19:14:18.241966",
     "exception": false,
     "start_time": "2025-06-22T19:14:09.774159",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm-:--:--\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: six in /home/thangquang/anaconda3/lib/python3.13/site-packages (from langdetect) (1.17.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "\u001b[33m  DEPRECATION: Building 'langdetect' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'langdetect'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for langdetect (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993222 sha256=6f6486e61971787709e7f5c4bbd55cee1297e11f810d3775007df2a0e86ed9e7\n",
      "  Stored in directory: /home/thangquang/.cache/pip/wheels/eb/87/25/2dddf1c94e1786054e25022ec5530bfed52bad86d882999c48\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.9\n"
     ]
    }
   ],
   "source": [
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T07:57:10.327551Z",
     "iopub.status.busy": "2025-08-15T07:57:10.327043Z",
     "iopub.status.idle": "2025-08-15T07:57:10.333853Z",
     "shell.execute_reply": "2025-08-15T07:57:10.332870Z",
     "shell.execute_reply.started": "2025-08-15T07:57:10.327514Z"
    },
    "id": "ImkKXISLXN6V",
    "papermill": {
     "duration": 4.005574,
     "end_time": "2025-06-22T19:14:22.253245",
     "exception": false,
     "start_time": "2025-06-22T19:14:18.247671",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of directories: 95\n",
      "Number of directories: 1068\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import string\n",
    "import unicodedata\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from langdetect import DetectorFactory, detect\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "DetectorFactory.seed = 42\n",
    "\n",
    "\n",
    "def read_texts_from_dir(dir_path):\n",
    "    \"\"\"\n",
    "    Reads the texts from a given directory and saves them in the pd.DataFrame with columns ['id', 'file_1', 'file_2'].\n",
    "\n",
    "    Params:\n",
    "      dir_path (str): path to the directory with data\n",
    "    \"\"\"\n",
    "    # Count number of directories in the provided path\n",
    "    dir_count = sum(\n",
    "        os.path.isdir(os.path.join(root, d))\n",
    "        for root, dirs, _ in os.walk(dir_path)\n",
    "        for d in dirs\n",
    "    )\n",
    "    data = [0 for _ in range(dir_count)]\n",
    "    print(f\"Number of directories: {dir_count}\")\n",
    "\n",
    "    # For each directory, read both file_1.txt and file_2.txt and save results to the list\n",
    "    i = 0\n",
    "    for folder_name in sorted(os.listdir(dir_path)):\n",
    "        folder_path = os.path.join(dir_path, folder_name)\n",
    "        if os.path.isdir(folder_path):\n",
    "            try:\n",
    "                with open(\n",
    "                    os.path.join(folder_path, \"file_1.txt\"), \"r\", encoding=\"utf-8\"\n",
    "                ) as f1:\n",
    "                    text1 = f1.read().strip()\n",
    "                with open(\n",
    "                    os.path.join(folder_path, \"file_2.txt\"), \"r\", encoding=\"utf-8\"\n",
    "                ) as f2:\n",
    "                    text2 = f2.read().strip()\n",
    "                index = int(folder_name[-4:])\n",
    "                data[i] = (index, text1, text2)\n",
    "                i += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading directory {folder_name}: {e}\")\n",
    "\n",
    "    # Change list with results into pandas DataFrame\n",
    "    df = pd.DataFrame(data, columns=[\"id\", \"file_1\", \"file_2\"]).set_index(\"id\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# =================== LOAD DATA ================================\n",
    "\n",
    "# Use the above function to load both train and test data\n",
    "# train_path=\"/kaggle/input/fake-or-real-the-impostor-hunt/data/train\"\n",
    "# df_train=read_texts_from_dir(train_path)\n",
    "# test_path=\"/kaggle/input/fake-or-real-the-impostor-hunt/data/test\"\n",
    "# df_test=read_texts_from_dir(test_path)\n",
    "\n",
    "\n",
    "# Use the above function to load both train and test data\n",
    "train_path = \"/home/thangquang/CODE/CTAI_MachineLearning/data/fake-or-real-the-impostor-hunt/data/train\"\n",
    "df_train = read_texts_from_dir(train_path)\n",
    "test_path = \"/home/thangquang/CODE/CTAI_MachineLearning/data/fake-or-real-the-impostor-hunt/data/test\"\n",
    "df_test = read_texts_from_dir(test_path)\n",
    "\n",
    "# Load ground truth for train data\n",
    "df_train_gt = pd.read_csv(\n",
    "    \"/home/thangquang/CODE/CTAI_MachineLearning/data/fake-or-real-the-impostor-hunt/data/train.csv\"\n",
    ")\n",
    "# df_train_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing train ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e56961fef274d6c84454c15f7bfe2b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Cleaning text:   0%|          | 0/95 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1b81b95a21744198b3bb1fbc836bd6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Cleaning text:   0%|          | 0/95 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing test  ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "541139393d474e2eb7b15ffcd7ee9e70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Cleaning text:   0%|          | 0/1068 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83380fb9b3b1419cb9a491b7be228a3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Cleaning text:   0%|          | 0/1068 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing & building features ...\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "[CV] END .......clf__C=0.1, clf__penalty=l2, select__k=15000; total time=   0.1s\n",
      "[CV] END .......clf__C=0.1, clf__penalty=l1, select__k=15000; total time=   0.1s\n",
      "[CV] END .......clf__C=0.1, clf__penalty=l1, select__k=15000; total time=   0.5s\n",
      "[CV] END .......clf__C=0.1, clf__penalty=l2, select__k=15000; total time=   0.1s\n",
      "[CV] END .......clf__C=0.1, clf__penalty=l1, select__k=25000; total time=   0.5s\n",
      "[CV] END .......clf__C=0.1, clf__penalty=l2, select__k=15000; total time=   0.1s\n",
      "[CV] END .......clf__C=0.1, clf__penalty=l2, select__k=15000; total time=   0.1s\n",
      "[CV] END .......clf__C=0.1, clf__penalty=l2, select__k=20000; total time=   0.1s\n",
      "[CV] END .......clf__C=0.1, clf__penalty=l2, select__k=15000; total time=   0.1s\n",
      "[CV] END .......clf__C=0.1, clf__penalty=l2, select__k=20000; total time=   0.1s\n",
      "[CV] END .......clf__C=0.1, clf__penalty=l2, select__k=20000; total time=   0.1s\n",
      "[CV] END .......clf__C=0.1, clf__penalty=l1, select__k=15000; total time=   0.8s\n",
      "[CV] END .......clf__C=0.1, clf__penalty=l2, select__k=20000; total time=   0.1s\n",
      "[CV] END .......clf__C=0.1, clf__penalty=l2, select__k=20000; total time=   0.1s\n",
      "[CV] END .......clf__C=0.1, clf__penalty=l1, select__k=15000; total time=   0.9s\n",
      "[CV] END .......clf__C=0.1, clf__penalty=l2, select__k=25000; total time=   0.1s\n",
      "[CV] END .......clf__C=0.1, clf__penalty=l2, select__k=25000; total time=   0.2s\n",
      "[CV] END .......clf__C=0.1, clf__penalty=l2, select__k=25000; total time=   0.2s\n",
      "[CV] END .......clf__C=0.1, clf__penalty=l2, select__k=25000; total time=   0.2s\n",
      "[CV] END .......clf__C=0.1, clf__penalty=l2, select__k=25000; total time=   0.2s\n",
      "[CV] END .......clf__C=0.1, clf__penalty=l1, select__k=20000; total time=   1.2s\n",
      "[CV] END .......clf__C=0.1, clf__penalty=l1, select__k=20000; total time=   1.2s\n",
      "[CV] END .......clf__C=0.5, clf__penalty=l1, select__k=15000; total time=   0.2s\n",
      "[CV] END .......clf__C=0.1, clf__penalty=l1, select__k=20000; total time=   1.3s\n",
      "[CV] END .......clf__C=0.1, clf__penalty=l1, select__k=15000; total time=   1.4s\n",
      "[CV] END .......clf__C=0.5, clf__penalty=l1, select__k=20000; total time=   0.5s\n",
      "[CV] END .......clf__C=0.5, clf__penalty=l1, select__k=15000; total time=   0.7s\n",
      "[CV] END .......clf__C=0.1, clf__penalty=l1, select__k=25000; total time=   2.1s\n",
      "[CV] END .......clf__C=0.1, clf__penalty=l1, select__k=20000; total time=   2.1s\n",
      "[CV] END .......clf__C=0.5, clf__penalty=l1, select__k=25000; total time=   0.5s\n",
      "[CV] END .......clf__C=0.5, clf__penalty=l2, select__k=15000; total time=   0.1s\n",
      "[CV] END .......clf__C=0.5, clf__penalty=l2, select__k=15000; total time=   0.1s\n",
      "[CV] END .......clf__C=0.5, clf__penalty=l2, select__k=15000; total time=   0.1s\n",
      "[CV] END .......clf__C=0.5, clf__penalty=l1, select__k=15000; total time=   1.8s\n",
      "[CV] END .......clf__C=0.1, clf__penalty=l1, select__k=25000; total time=   2.8s\n",
      "[CV] END .......clf__C=0.5, clf__penalty=l2, select__k=15000; total time=   0.2s\n",
      "[CV] END .......clf__C=0.5, clf__penalty=l1, select__k=20000; total time=   1.2s\n",
      "[CV] END .......clf__C=0.5, clf__penalty=l2, select__k=15000; total time=   0.1s\n",
      "[CV] END .......clf__C=0.5, clf__penalty=l2, select__k=20000; total time=   0.1s\n",
      "[CV] END .......clf__C=0.5, clf__penalty=l2, select__k=20000; total time=   0.1s\n",
      "[CV] END .......clf__C=0.5, clf__penalty=l2, select__k=20000; total time=   0.2s\n",
      "[CV] END .......clf__C=0.5, clf__penalty=l2, select__k=20000; total time=   0.2s\n",
      "[CV] END .......clf__C=0.5, clf__penalty=l2, select__k=25000; total time=   0.1s\n",
      "[CV] END .......clf__C=0.5, clf__penalty=l1, select__k=15000; total time=   2.2s\n",
      "[CV] END .......clf__C=0.1, clf__penalty=l1, select__k=20000; total time=   3.4s\n",
      "[CV] END .......clf__C=0.5, clf__penalty=l2, select__k=20000; total time=   0.4s\n",
      "[CV] END .......clf__C=0.1, clf__penalty=l1, select__k=25000; total time=   3.4s\n",
      "[CV] END .......clf__C=0.5, clf__penalty=l2, select__k=25000; total time=   0.2s\n",
      "[CV] END .......clf__C=0.5, clf__penalty=l2, select__k=25000; total time=   0.3s\n",
      "[CV] END .......clf__C=0.5, clf__penalty=l2, select__k=25000; total time=   0.3s\n",
      "[CV] END .......clf__C=0.5, clf__penalty=l2, select__k=25000; total time=   0.3s\n",
      "[CV] END .........clf__C=1, clf__penalty=l1, select__k=15000; total time=   0.2s\n",
      "[CV] END .........clf__C=1, clf__penalty=l1, select__k=15000; total time=   0.4s\n",
      "[CV] END .......clf__C=0.5, clf__penalty=l1, select__k=15000; total time=   2.9s\n",
      "[CV] END .........clf__C=1, clf__penalty=l1, select__k=20000; total time=   0.3s\n",
      "[CV] END .......clf__C=0.5, clf__penalty=l1, select__k=25000; total time=   1.7s\n",
      "[CV] END .........clf__C=1, clf__penalty=l1, select__k=25000; total time=   0.7s\n",
      "[CV] END .......clf__C=0.5, clf__penalty=l1, select__k=20000; total time=   3.6s\n",
      "[CV] END .........clf__C=1, clf__penalty=l1, select__k=15000; total time=   1.4s\n",
      "[CV] END .........clf__C=1, clf__penalty=l1, select__k=20000; total time=   1.1s\n",
      "[CV] END .........clf__C=1, clf__penalty=l2, select__k=15000; total time=   0.1s\n",
      "[CV] END .........clf__C=1, clf__penalty=l2, select__k=15000; total time=   0.1s\n",
      "[CV] END .........clf__C=1, clf__penalty=l2, select__k=15000; total time=   0.1s\n",
      "[CV] END .........clf__C=1, clf__penalty=l2, select__k=15000; total time=   0.1s\n",
      "[CV] END .........clf__C=1, clf__penalty=l2, select__k=15000; total time=   0.1s\n",
      "[CV] END .........clf__C=1, clf__penalty=l2, select__k=20000; total time=   0.1s\n",
      "[CV] END .........clf__C=1, clf__penalty=l1, select__k=25000; total time=   0.9s\n",
      "[CV] END .........clf__C=1, clf__penalty=l2, select__k=20000; total time=   0.2s\n",
      "[CV] END .........clf__C=1, clf__penalty=l2, select__k=20000; total time=   0.2s\n",
      "[CV] END .......clf__C=0.5, clf__penalty=l1, select__k=20000; total time=   4.7s\n",
      "[CV] END .........clf__C=1, clf__penalty=l2, select__k=20000; total time=   0.3s\n",
      "[CV] END .........clf__C=1, clf__penalty=l2, select__k=20000; total time=   0.2s\n",
      "[CV] END .........clf__C=1, clf__penalty=l1, select__k=15000; total time=   2.8s\n",
      "[CV] END .........clf__C=1, clf__penalty=l2, select__k=25000; total time=   0.1s\n",
      "[CV] END .........clf__C=1, clf__penalty=l1, select__k=20000; total time=   2.7s\n",
      "[CV] END .........clf__C=1, clf__penalty=l1, select__k=15000; total time=   2.9s\n",
      "[CV] END .........clf__C=1, clf__penalty=l2, select__k=25000; total time=   0.3s\n",
      "[CV] END .........clf__C=1, clf__penalty=l2, select__k=25000; total time=   0.3s\n",
      "[CV] END .........clf__C=2, clf__penalty=l1, select__k=15000; total time=   0.2s\n",
      "[CV] END .......clf__C=0.5, clf__penalty=l1, select__k=25000; total time=   5.0s\n",
      "[CV] END .........clf__C=1, clf__penalty=l2, select__k=25000; total time=   0.5s\n",
      "[CV] END .........clf__C=1, clf__penalty=l2, select__k=25000; total time=   0.5s\n",
      "[CV] END .......clf__C=0.1, clf__penalty=l1, select__k=25000; total time=   6.9s\n",
      "[CV] END .........clf__C=1, clf__penalty=l1, select__k=25000; total time=   2.4s\n",
      "[CV] END .........clf__C=2, clf__penalty=l1, select__k=20000; total time=   0.7s\n",
      "[CV] END .........clf__C=2, clf__penalty=l1, select__k=15000; total time=   0.8s\n",
      "[CV] END .........clf__C=1, clf__penalty=l1, select__k=20000; total time=   3.9s\n",
      "[CV] END .........clf__C=2, clf__penalty=l1, select__k=20000; total time=   1.4s\n",
      "[CV] END .........clf__C=2, clf__penalty=l1, select__k=25000; total time=   1.3s\n",
      "[CV] END .........clf__C=2, clf__penalty=l1, select__k=15000; total time=   2.2s\n",
      "[CV] END .........clf__C=2, clf__penalty=l2, select__k=15000; total time=   0.1s\n",
      "[CV] END .........clf__C=2, clf__penalty=l2, select__k=15000; total time=   0.1s\n",
      "[CV] END .........clf__C=2, clf__penalty=l1, select__k=15000; total time=   2.5s\n",
      "[CV] END .........clf__C=2, clf__penalty=l2, select__k=15000; total time=   0.1s\n",
      "[CV] END .........clf__C=2, clf__penalty=l2, select__k=15000; total time=   0.1s\n",
      "[CV] END .......clf__C=0.5, clf__penalty=l1, select__k=20000; total time=   7.8s\n",
      "[CV] END .........clf__C=2, clf__penalty=l2, select__k=15000; total time=   0.1s\n",
      "[CV] END .........clf__C=2, clf__penalty=l2, select__k=20000; total time=   0.1s\n",
      "[CV] END .........clf__C=2, clf__penalty=l2, select__k=20000; total time=   0.2s\n",
      "[CV] END .........clf__C=2, clf__penalty=l2, select__k=20000; total time=   0.2s\n",
      "[CV] END .........clf__C=2, clf__penalty=l2, select__k=20000; total time=   0.3s\n",
      "[CV] END .........clf__C=2, clf__penalty=l2, select__k=20000; total time=   0.2s\n",
      "[CV] END .........clf__C=2, clf__penalty=l2, select__k=25000; total time=   0.2s\n",
      "[CV] END .........clf__C=2, clf__penalty=l2, select__k=25000; total time=   0.3s\n",
      "[CV] END .........clf__C=2, clf__penalty=l1, select__k=25000; total time=   1.5s\n",
      "[CV] END .........clf__C=2, clf__penalty=l2, select__k=25000; total time=   0.5s\n",
      "[CV] END .......clf__C=0.5, clf__penalty=l1, select__k=25000; total time=   8.0s\n",
      "[CV] END .........clf__C=2, clf__penalty=l2, select__k=25000; total time=   0.2s\n",
      "[CV] END .........clf__C=2, clf__penalty=l2, select__k=25000; total time=   0.7s\n",
      "[CV] END .........clf__C=1, clf__penalty=l1, select__k=25000; total time=   6.2s\n",
      "[CV] END .........clf__C=5, clf__penalty=l1, select__k=15000; total time=   0.3s\n",
      "[CV] END .........clf__C=2, clf__penalty=l1, select__k=15000; total time=   4.4s\n",
      "[CV] END .........clf__C=5, clf__penalty=l1, select__k=15000; total time=   0.8s\n",
      "[CV] END .........clf__C=5, clf__penalty=l1, select__k=20000; total time=   0.9s\n",
      "[CV] END .........clf__C=2, clf__penalty=l1, select__k=20000; total time=   5.1s\n",
      "[CV] END .........clf__C=2, clf__penalty=l1, select__k=20000; total time=   5.6s\n",
      "[CV] END .........clf__C=5, clf__penalty=l1, select__k=20000; total time=   1.3s\n",
      "[CV] END .........clf__C=1, clf__penalty=l1, select__k=20000; total time=  10.4s\n",
      "[CV] END .........clf__C=5, clf__penalty=l1, select__k=15000; total time=   4.0s\n",
      "[CV] END .......clf__C=0.5, clf__penalty=l1, select__k=25000; total time=  12.5s\n",
      "[CV] END .........clf__C=5, clf__penalty=l1, select__k=25000; total time=   2.0s\n",
      "[CV] END .........clf__C=5, clf__penalty=l2, select__k=15000; total time=   0.1s\n",
      "[CV] END .........clf__C=5, clf__penalty=l2, select__k=15000; total time=   0.1s\n",
      "[CV] END .........clf__C=5, clf__penalty=l2, select__k=15000; total time=   0.1s\n",
      "[CV] END .........clf__C=5, clf__penalty=l2, select__k=15000; total time=   0.1s\n",
      "[CV] END .........clf__C=5, clf__penalty=l2, select__k=15000; total time=   0.1s\n",
      "[CV] END .........clf__C=5, clf__penalty=l2, select__k=20000; total time=   0.1s\n",
      "[CV] END .........clf__C=5, clf__penalty=l2, select__k=20000; total time=   0.2s\n",
      "[CV] END .........clf__C=5, clf__penalty=l1, select__k=15000; total time=   4.8s\n",
      "[CV] END .........clf__C=5, clf__penalty=l2, select__k=20000; total time=   0.3s\n",
      "[CV] END .........clf__C=5, clf__penalty=l2, select__k=20000; total time=   0.2s\n",
      "[CV] END .........clf__C=5, clf__penalty=l1, select__k=15000; total time=   4.9s\n",
      "[CV] END .........clf__C=5, clf__penalty=l2, select__k=20000; total time=   0.2s\n",
      "[CV] END .........clf__C=5, clf__penalty=l2, select__k=25000; total time=   0.2s\n",
      "[CV] END .........clf__C=5, clf__penalty=l2, select__k=25000; total time=   0.3s\n",
      "[CV] END .........clf__C=5, clf__penalty=l2, select__k=25000; total time=   0.3s\n",
      "[CV] END .........clf__C=5, clf__penalty=l2, select__k=25000; total time=   0.3s\n",
      "[CV] END .........clf__C=5, clf__penalty=l2, select__k=25000; total time=   0.2s\n",
      "[CV] END .........clf__C=2, clf__penalty=l1, select__k=25000; total time=   7.4s\n",
      "[CV] END .........clf__C=2, clf__penalty=l1, select__k=25000; total time=   8.7s\n",
      "[CV] END .........clf__C=2, clf__penalty=l1, select__k=20000; total time=   9.3s\n",
      "[CV] END .........clf__C=5, clf__penalty=l1, select__k=20000; total time=   5.7s\n",
      "[CV] END .........clf__C=5, clf__penalty=l1, select__k=20000; total time=   6.5s\n",
      "[CV] END .........clf__C=5, clf__penalty=l1, select__k=25000; total time=   2.9s\n",
      "[CV] END .........clf__C=5, clf__penalty=l1, select__k=25000; total time=   4.8s\n",
      "[CV] END .........clf__C=5, clf__penalty=l1, select__k=25000; total time=   3.2s\n",
      "[CV] END .........clf__C=1, clf__penalty=l1, select__k=25000; total time=  12.7s\n",
      "[CV] END .........clf__C=5, clf__penalty=l1, select__k=20000; total time=   6.8s\n",
      "[CV] END .........clf__C=2, clf__penalty=l1, select__k=25000; total time=  10.2s\n",
      "[CV] END .........clf__C=5, clf__penalty=l1, select__k=25000; total time=   6.1s\n",
      "Best CV accuracy: 0.7263\n",
      "Best params    : {'clf__C': 5, 'clf__penalty': 'l1', 'select__k': 15000}\n",
      "Training on full data ...\n",
      "Predicting on test ...\n",
      "‚úÖ  Submission saved to /home/thangquang/CODE/CTAI_MachineLearning/notebooks/submission.csv\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------- 1. IMPORTS ------------------------------------\n",
    "import os, re, gc, warnings, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# ML & utils\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# ----------------------- 2. TEXT PRE-PROCESSING ------------------------------\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words  = set(stopwords.words(\"english\"))\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"L√†m s·∫°ch + chu·∫©n ho√° m·ªôt c√¢u vƒÉn.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    text = re.sub(r\"http\\S+\", \" \", text)          # xo√° URL\n",
    "    text = re.sub(r\"\\d+\", \" NUM \", text)          # thay s·ªë = token NUM\n",
    "    text = re.sub(r\"[^\\w\\s]\", \" \", text)          # b·ªè punctuation\n",
    "    text = text.lower()\n",
    "    tokens = [lemmatizer.lemmatize(tok)\n",
    "              for tok in word_tokenize(text)\n",
    "              if tok.isalpha() and tok not in stop_words and len(tok) > 2]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def preprocess_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"√Åp d·ª•ng clean_text cho 2 c·ªôt file_1 v√† file_2 + t·∫°o c·ªôt combined.\"\"\"\n",
    "    tqdm.pandas(desc=\"Cleaning text\")\n",
    "    df[\"file_1\"] = df[\"file_1\"].progress_apply(clean_text)\n",
    "    df[\"file_2\"] = df[\"file_2\"].progress_apply(clean_text)\n",
    "    df[\"combined\"] = df[\"file_1\"] + \" [SEP] \" + df[\"file_2\"]\n",
    "    return df\n",
    "\n",
    "# ----------------------- 3. STATISTICAL FEATURES -----------------------------\n",
    "def statistical_features(df: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"Sinh ƒë·∫∑c tr∆∞ng th·ªëng k√™/ƒë·∫øm ƒë∆°n gi·∫£n ·ªü d·∫°ng dense numpy array.\"\"\"\n",
    "    len_1   = df[\"file_1\"].str.len()\n",
    "    len_2   = df[\"file_2\"].str.len()\n",
    "    words_1 = df[\"file_1\"].str.split().apply(len)\n",
    "    words_2 = df[\"file_2\"].str.split().apply(len)\n",
    "\n",
    "    features = pd.DataFrame({\n",
    "        \"len_diff\"       : (len_1 - len_2).abs(),\n",
    "        \"word_diff\"      : (words_1 - words_2).abs(),\n",
    "        \"len_ratio\"      : (len_1 + 1) / (len_2 + 1),\n",
    "        \"words_ratio\"    : (words_1 + 1) / (words_2 + 1),\n",
    "        \"avg_word_len_1\" : df[\"file_1\"].apply(lambda x: np.mean([len(w) for w in x.split()]) if x else 0),\n",
    "        \"avg_word_len_2\" : df[\"file_2\"].apply(lambda x: np.mean([len(w) for w in x.split()]) if x else 0),\n",
    "    })\n",
    "    return features.values.astype(np.float32)\n",
    "\n",
    "# ----------------------- 4. VECTORIZERS --------------------------------------\n",
    "word_vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1, 3),\n",
    "    analyzer=\"word\",\n",
    "    max_features=20_000,\n",
    "    sublinear_tf=True,\n",
    ")\n",
    "\n",
    "char_vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(3, 6),\n",
    "    analyzer=\"char_wb\",\n",
    "    max_features=15_000,\n",
    "    sublinear_tf=True,\n",
    ")\n",
    "\n",
    "# ----------------------- 5. MODEL + PIPELINE ---------------------------------\n",
    "def build_feature_matrix(df: pd.DataFrame, fit: bool = False):\n",
    "    \"\"\"Tr·∫£ v·ªÅ sparse matrix = [word_tfidf | char_tfidf | stats_dense].\"\"\"\n",
    "    # TF-IDF\n",
    "    if fit:\n",
    "        X_word = word_vectorizer.fit_transform(df[\"combined\"])\n",
    "        X_char = char_vectorizer.fit_transform(df[\"combined\"])\n",
    "    else:\n",
    "        X_word = word_vectorizer.transform(df[\"combined\"])\n",
    "        X_char = char_vectorizer.transform(df[\"combined\"])\n",
    "\n",
    "    # Statistical (dense) -> convert to sparse for hstack\n",
    "    X_stats = csr_matrix(statistical_features(df))\n",
    "\n",
    "    return hstack([X_word, X_char, X_stats]).tocsr()\n",
    "\n",
    "def train_and_evaluate(X, y):\n",
    "    \"\"\"Cross-validate + grid search LogisticRegression, tr·∫£ v·ªÅ best model.\"\"\"\n",
    "    # Feature selection b√™n trong pipeline ƒë·ªÉ kh√¥ng r√≤ r·ªâ d·ªØ li·ªáu\n",
    "    clf = Pipeline([\n",
    "        (\"select\", SelectKBest(chi2, k=20_000)),\n",
    "        (\"clf\", LogisticRegression(max_iter=5_000, solver=\"liblinear\", n_jobs=-1))\n",
    "    ])\n",
    "\n",
    "    param_grid = {\n",
    "        \"select__k\"   : [15_000, 20_000, 25_000],\n",
    "        \"clf__C\"      : [0.1, 0.5, 1, 2, 5],\n",
    "        \"clf__penalty\": [\"l1\", \"l2\"]\n",
    "    }\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    grid = GridSearchCV(\n",
    "        estimator=clf,\n",
    "        param_grid=param_grid,\n",
    "        scoring=\"accuracy\",\n",
    "        cv=skf,\n",
    "        n_jobs=-1,\n",
    "        verbose=2\n",
    "    )\n",
    "    grid.fit(X, y)\n",
    "    print(f\"Best CV accuracy: {grid.best_score_:.4f}\")\n",
    "    print(f\"Best params    : {grid.best_params_}\")\n",
    "    return grid.best_estimator_\n",
    "\n",
    "# ----------------------- 6. MAIN EXECUTION -----------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # --- 6.1 Pre-process ------------------------------------------------------\n",
    "    print(\"Pre-processing train ...\")\n",
    "    df_train = preprocess_dataframe(df_train)\n",
    "    print(\"Pre-processing test  ...\")\n",
    "    df_test  = preprocess_dataframe(df_test)\n",
    "\n",
    "    # --- 6.2 Build feature matrices -----------------------------------------\n",
    "    print(\"Vectorizing & building features ...\")\n",
    "    X_train = build_feature_matrix(df_train, fit=True)\n",
    "    X_test  = build_feature_matrix(df_test,  fit=False)\n",
    "    y_train = df_train_gt[\"real_text_id\"].values  # 0 ho·∫∑c 1\n",
    "\n",
    "    # --- 6.3 Train + CV ------------------------------------------------------\n",
    "    model = train_and_evaluate(X_train, y_train)\n",
    "\n",
    "    # --- 6.4 Validate hold-out set (optional) --------------------------------\n",
    "    # N·∫øu mu·ªën m·ªôt t·∫≠p hold-out thay v√¨ CV:\n",
    "    # X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    #     X_train, y_train, test_size=0.2,\n",
    "    #     random_state=SEED, stratify=y_train\n",
    "    # )\n",
    "    # model.fit(X_tr, y_tr)\n",
    "    # val_pred = model.predict(X_val)\n",
    "    # print(\"Hold-out accuracy:\", accuracy_score(y_val, val_pred))\n",
    "\n",
    "    # --- 6.5 Retrain on full data & infer test -------------------------------\n",
    "    print(\"Training on full data ...\")\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Predicting on test ...\")\n",
    "    test_pred = model.predict(X_test)\n",
    "\n",
    "    # --- 6.6 Build submission -------------------------------------------------\n",
    "    submission = pd.DataFrame({\n",
    "        \"id\": df_test.index,\n",
    "        \"real_text_id\": test_pred.astype(int)\n",
    "    }).sort_values(\"id\")\n",
    "\n",
    "    save_path = Path(\"submission.csv\")\n",
    "    submission.to_csv(save_path, index=False)\n",
    "    print(f\"‚úÖ  Submission saved to {save_path.resolve()}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 12964783,
     "sourceId": 105874,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "ctai-machinelearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 340.921147,
   "end_time": "2025-06-22T19:19:45.451603",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-22T19:14:04.530456",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
