data:
  dataset_folder: data
  train_dataset: train.csv
  val_dataset: dev.csv
  test_dataset: test.csv
  use_id: False

tokenizer:
  padding: max_length
  max_length: 1024     # Tăng context length cho long documents
  truncation: True
  return_attention_mask: True

text_embedding:
  type: pretrained
  add_new_token: False
  text_encoder: microsoft/deberta-v3-large  # Large model!
  freeze: False
  use_lora: True       # Enable LoRA để handle large model efficiently
  lora_target_modules: ["query_proj", "key_proj", "value_proj"]  # DeBERTa modules
  lora_r: 16           # Tăng rank cho large model
  lora_alpha: 32       # Tăng alpha
  lora_dropout: 0.1
  d_features: 1024     # DeBERTa-large hidden size
  d_model: 1024        # Keep original dimension
  dropout: 0.1

model:
  type_model: pretrained_nli
  dropout: 0.1

train:
  output_dir: checkpoint
  seed: 12345
  num_train_epochs: 10   # Ít epochs hơn vì large model
  patience: 2           # Patience thấp hơn
  learning_rate: 5.0e-6 # LR thấp hơn cho large model
  weight_decay: 0.01
  metric_for_best_model: f1
  per_device_train_batch_size: 2   # Batch size nhỏ cho 1024 tokens + large model
  per_device_valid_batch_size: 2
  gradient_accumulation_steps: 8    # Compensate for small batch size

inference:
  test_dataset: data/test.csv
  batch_size: 4         # Small batch cho inference
